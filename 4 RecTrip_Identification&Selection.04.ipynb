{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "415bab13-0e39-444b-b30e-4e04ade2c89b",
   "metadata": {},
   "source": [
    "# This notebook was used to develop and then carry out the non-machine-learning steps in the recreational trip identification process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f72c1-547b-4ee5-86e7-946f1389bc98",
   "metadata": {},
   "source": [
    "## Key inputs: \n",
    "* Indicators file with columns indicating the predicted vessel class: **Indicators_OurTable.Predictions.csv**\n",
    "* Data from **Cuebiq's Device table**\n",
    "\n",
    "## Key outputs:\n",
    "* Final data set with features for identified recreational fishing trips including indication if the trip is fully-tracked (called interrupted here) and the sample weight for the trip: **DisappearanceIndicators.csv**\n",
    "* File with all of the stops and trawls within a given trip **Stop_Trawls_Indicators.csv** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14737f22-889e-4ede-99c1-4d4b4d86b188",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spectus Platform Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1860e869-8e8e-4725-8ed7-4f1b126ee80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install modules\n",
    "!pip install tqdm\n",
    "!pip install statsmodels\n",
    "\n",
    "# # Suppress all warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime\n",
    "import pytz  \n",
    "import warnings\n",
    "import numpy as np\n",
    "import math\n",
    "local_timezone = pytz.timezone('US/Central')\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "import datetime as dt\n",
    "import geopandas as gpd\n",
    "import heapq\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "import statsmodels\n",
    "\n",
    "from scipy import interpolate\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "from zipfile import ZipFile\n",
    "from shapely.geometry import Point  # Import the Point class from shapely.geometry\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "class OperationCancelled(Exception):\n",
    "    pass\n",
    "\n",
    "local_timezone = pytz.timezone('US/Central')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365345d-7fa7-463c-b9ea-dc678c083b72",
   "metadata": {},
   "source": [
    "### SQL Prelminatries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1e42a-0977-4d36-859b-2995aa32ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL engine\n",
    "from trino.dbapi import connect \n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "class TrinoEngine():\n",
    "    def __init__(self):\n",
    "        conn = connect(\n",
    "            host=\"localhost\",\n",
    "            port=9090,\n",
    "            catalog=\"cuebiq\"\n",
    "        )\n",
    "        self.cur = conn.cursor()\n",
    "        self.engine = create_engine(\"trino://localhost:9090/cuebiq/\")\n",
    "    \n",
    "    def execute_statement(self, query:str) -> list:\n",
    "        \"\"\"\n",
    "        Create and drop statements.\n",
    "        \"\"\"\n",
    "        self.cur.execute(query)\n",
    "        return self.cur.fetchall()\n",
    "    \n",
    "    def read_sql(self, query:str) -> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Select and insert into operations.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "sql_engine = TrinoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de54c893-5be9-4f42-97ea-00600002b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates_sequence(\n",
    "    date_start, \n",
    "    date_end, \n",
    "    date_format\n",
    "):\n",
    "    return [\n",
    "        (datetime.strptime(date_start, date_format) + timedelta(days=x)).strftime(date_format)\n",
    "        for x in range (\n",
    "        0,\n",
    "        (datetime.strptime(date_end, date_format) - datetime.strptime(date_start, date_format) + timedelta(days=1)).days\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975610ff-5b18-403a-be1e-7863b404a4d7",
   "metadata": {},
   "source": [
    "### Date parameter for request from Spectus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d866e1be-49b3-4fc6-808b-b7caaea54208",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format = \"%Y%m%d\"\n",
    "\n",
    "first_date = \"20190101\"\n",
    "last_date_to_compute = \"20220422\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ed972-4170-45e6-b067-4fe0c2a32aa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions and other objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20824737-0054-4145-a179-e3dbdf20948f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Gulf wkt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d7a47-4e77-4a1a-9265-2f06e5346c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gulf=  'POLYGON ((-97.1461459379788 25.957168007570218, -96.289339604698 24.85679572942162, -92.46567751398405 24.93226258740249, -86.46809930536585 26.63685607835417, -87.4518747436496 30.2870218479717, -87.56970870420537 30.265986917324028, -87.68688306481553 30.240398581969316, -87.80866540589953 30.223904790285445, -87.86330288865585 30.223335989499247, -87.95809562982397 30.227886303681714, -88.02919018569986 30.21821663439117, -88.04103927834595 30.236417575256624, -87.96007047859824 30.256320996255198, -87.9264980494346 30.259732606445226, -87.87712683007626 30.243810745100816, -87.84223783506322 30.25688960618315, -87.78825863523141 30.25802681616385, -87.76653529871386 30.268829654214812, -87.75929418654121 30.287589653053473, -87.77443469381102 30.308051015443525, -87.80142429372691 30.321121316921534, -87.83236359119138 30.34441622544655, -87.84487096676237 30.372249210920785, -87.87910145959437 30.384175375165654, -87.9185984350809 30.414267849534085, -87.92386469847926 30.462509996885288, -87.94032177159842 30.47215556160502, -87.93505550820048 30.49938493987453, -87.91991500093027 30.529441859299524, -87.9172818692315 30.552120171744733, -87.91991500093027 30.575926698627413, -87.92452298140375 30.609925886488114, -87.92978924480211 30.64051495391115, -87.9475628837711 30.650709160976575, -87.96928622028864 30.651841784313817, -87.99298440558087 30.665432229337696, -88.01141632747473 30.659203512573512, -88.0423556249392 30.638249428634225, -88.05091330296123 30.624088693607646, -88.04696360541267 30.61559125795084, -88.05354643466042 30.590094476474817, -88.07790290287716 30.565157802896906, -88.07592805410286 30.539080789195197, -88.09370063563347 30.456267467088836, -88.09896689903184 30.398370320370773, -88.10620801120405 30.353504810736837, -88.15887064518647 30.323393627447288, -88.19178479142525 30.31032362908067, -88.20429216699586 30.330780246575657, -88.19705105482359 30.353504810736837, -88.28855238136764 30.381335210261483, -88.2957934935403 30.364865114151982, -88.31817511298274 30.363161152750493, -88.32014996175704 30.38190309511809, -88.3517475421464 30.390988804016374, -88.39058623470804 30.37224860380745, -88.39256108348229 30.342143188113525, -88.43732432236718 30.34725608152752, -88.44259058576552 30.331348424972347, -88.48077099540268 30.308618718161995, -88.5018360489957 30.312596797505776, -88.5288256489116 30.337030027619335, -88.55384040005282 30.331348424972347, -88.58807111214138 30.335893733464147, -88.60584475111035 30.368840908630105, -88.65784982716818 30.350096668434418, -88.70377316315728 30.333940318201655, -88.76345157602148 30.340304903230376, -88.81215810808295 30.366323547786152, -88.96059706293589 30.3823315816322, -89.10671665911954 30.334299618169936, -89.33169445006897 30.2782325348214, -89.41287200350473 30.192067121425936, -89.44534302487904 30.08977311890679, -89.359526754104 30.069703042406346, -89.20876844058175 30.176028017449696, -89.16006190852028 30.16199165952355, -89.16006190852028 30.08977311890679, -89.13918768049385 30.055651566395866, -89.16238126718959 29.921058052351142, -89.21108779925103 29.7782302647352, -89.24587817929464 29.66946405486304, -89.31545893938255 29.693644509171918, -89.27371048332964 29.629150377050877, -89.30386214603394 29.59688781400496, -89.3850396994697 29.61503677626405, -89.46157853556547 29.532331728567527, -89.3827203407997 29.439460132099583, -89.18979122539224 29.427128425449155, -89.19596940111164 29.394836842780364, -88.95749181835598 29.20950068771296, -89.04027937299146 29.04327521681688, -89.10329676532581 29.036793464759455, -89.13913018449634 28.94816874403513, -89.19473376596763 28.98276303815456, -89.26022242858967 29.027070073382717, -89.29358457747252 29.016265231135534, -89.4010848349841 28.903827915763785, -89.44186079473003 28.94816874403513, -89.39243538897739 29.025989640029877, -89.44680333530505 29.0864764893534, -89.45298151102406 29.14692783767063, -89.42209063242889 29.17066685855066, -89.376372132108 29.156639918576943, -89.36525141581359 29.180376694261838, -89.34671688865649 29.20195081707395, -89.37390086182032 29.206265096965808, -89.40232047012778 29.203029404070804, -89.4356826190106 29.20195081707395, -89.46780913274982 29.179297868997324, -89.49499310591362 29.193321712569713, -89.54441851166625 29.2278337721458, -89.63214860687673 29.254788228681946, -89.68404528291667 29.282813330704627, -89.80143062157865 29.305443379957012, -89.85579856790665 29.300055727855437, -89.9076952439466 29.27526886673428, -89.98924716343802 29.21489311194422, -90.05473582606008 29.165272108427317, -90.1572935429963 29.102672297767754, -90.26232567572231 29.06703684192044, -90.2833314731671 29.10914859304988, -90.26479694600961 29.13181701694839, -90.28827401374208 29.15232223739727, -90.29939473003648 29.19547774319095, -90.269739486585 29.23538042243976, -90.29568782460515 29.2310673685935, -90.30927981118687 29.24724038352842, -90.33893505463838 29.26341084252479, -90.393303000966 29.251552755878066, -90.40565935240406 29.30220951059583, -90.44767094729364 29.334530475168393, -90.50203889362164 29.32698983267116, -90.54899302908619 29.3032870410304, -90.57123446167503 29.309751984787326, -90.56629192109963 29.291433580776584, -90.59100462397578 29.241849662499888, -90.72198194921984 29.166349767170132, -90.74669465209598 29.108069019686837, -90.77634989554745 29.09943202507891, -90.84801673388853 29.115625795370804, -90.88755705849043 29.13721352427197, -90.89126396392177 29.16742872854931, -90.91474103165422 29.217048688862008, -90.92956865337999 29.198713674444775, -90.93080428852403 29.16742872854931, -90.97652278884493 29.160954790180483, -91.02965510002853 29.163112814994165, -91.08772995178788 29.17713887061521, -91.15445424955357 29.210577881865433, -91.21005783102524 29.21165637807694, -91.28790284508534 29.240771484217632, -91.33609261569393 29.292511224930067, -91.36327658885774 29.33237606278429, -91.36698404442399 29.461560546417502, -91.37316222014341 29.483075264842185, -91.42505889618336 29.473394206552683, -91.48189811279867 29.486302078781378, -91.50784645081843 29.51963980456361, -91.52885224826323 29.514263494469418, -91.5424442348453 29.538992155608895, -91.5424442348453 29.565864276975645, -91.56097876200242 29.62710598028727, -91.58692710002259 29.62603188493061, -91.6351168706312 29.61636451146687, -91.65118012750057 29.642142112800144, -91.63758814091887 29.656102225576717, -91.64129504635021 29.723727639327834, -91.71296188469128 29.718362205888425, -91.76980110130658 29.702264184791886, -91.83034722335326 29.685090118027063, -91.85505992622939 29.662544701481323, -91.845174845079 29.634624327137942, -91.79451380418274 29.609919080630874, -91.75744474986853 29.590580315799386, -91.69689862782151 29.57123784364269, -91.71914006041031 29.53254178333445, -91.74879530386181 29.517489314800784, -91.77103673645024 29.48199963736188, -91.8019276150454 29.470166981654714, -91.8525886559417 29.47662132872088, -91.94649692687157 29.517489314800784, -92.01816376521265 29.556191136499905, -92.081181157547 29.575536491098305, -92.14666982016863 29.57338719024669, -92.2282217396605 29.543292175193457, -92.2620745477254 29.513938983582918, -92.40184398133182 29.53587031357371, -92.53015690398729 29.55580375974114, -92.65388722226176 29.58171136156176, -92.76845233177524 29.613588513778055, -92.90134785881101 29.66735835342932, -92.99758255080252 29.705179169035475, -93.10985635812567 29.739006777997886, -93.2015084457365 29.758900043967557, -93.27024751144474 29.76088915343348, -93.3389865771526 29.740996282222838, -93.43980387352478 29.750943211286994, -93.61623414217547 29.740996282222838, -93.76058618016258 29.715129649330578, -93.8293252458708 29.663376387168597, -93.95534686633557 29.6852752506843, -94.10199020651301 29.65541198179467, -94.53962892485461 29.488014011072636, -94.71376789131534 29.402214927729204, -94.72293005770678 29.324334651639106, -94.9360211614021 29.206402439154715, -95.13765575414574 29.060298314788838, -95.37824248412433 28.86583930097359, -95.42635983011992 28.85781253647636, -95.88003908572986 28.63281185841163, -96.17332576608472 28.508049911739334, -96.3657951500674 28.385156078353774, -96.42995161139497 28.326681104162276, -96.5811775559527 28.237899002445175, -96.8538425165949 28.05404703841444, -97.01423366991399 27.871905659410203, -97.18378864850824 27.632633899635536, -97.28460594488008 27.443685488256804, -97.34647110401767 27.286998971872194, -97.37396673030096 27.111736817761667, -97.37167542811062 26.942327753778045, -97.30062980709232 26.653886203530476, -97.27907391837897 26.589650805967054, -97.227579295341 26.438555761793396, -97.19764056101677 26.3183955849306, -97.16770182669217 26.152971688301193, -97.15093613547081 26.036819954392655, -97.1461459379788 25.957168007570218))'\n",
    "Gulf_wkt= 'POLYGON ((-97.1461459379788 25.957168007570218, -96.289339604698 24.85679572942162, -92.46567751398405 24.93226258740249, -86.46809930536585 26.63685607835417, -87.4518747436496 30.2870218479717, -87.56970870420537 30.265986917324028, -87.68688306481553 30.240398581969316, -87.80866540589953 30.223904790285445, -87.86330288865585 30.223335989499247, -87.95809562982397 30.227886303681714, -88.02919018569986 30.21821663439117, -88.04103927834595 30.236417575256624, -87.96007047859824 30.256320996255198, -87.9264980494346 30.259732606445226, -87.87712683007626 30.243810745100816, -87.84223783506322 30.25688960618315, -87.78825863523141 30.25802681616385, -87.76653529871386 30.268829654214812, -87.75929418654121 30.287589653053473, -87.77443469381102 30.308051015443525, -87.80142429372691 30.321121316921534, -87.83236359119138 30.34441622544655, -87.84487096676237 30.372249210920785, -87.87910145959437 30.384175375165654, -87.9185984350809 30.414267849534085, -87.92386469847926 30.462509996885288, -87.94032177159842 30.47215556160502, -87.93505550820048 30.49938493987453, -87.91991500093027 30.529441859299524, -87.9172818692315 30.552120171744733, -87.91991500093027 30.575926698627413, -87.92452298140375 30.609925886488114, -87.92978924480211 30.64051495391115, -87.9475628837711 30.650709160976575, -87.96928622028864 30.651841784313817, -87.99298440558087 30.665432229337696, -88.01141632747473 30.659203512573512, -88.0423556249392 30.638249428634225, -88.05091330296123 30.624088693607646, -88.04696360541267 30.61559125795084, -88.05354643466042 30.590094476474817, -88.07790290287716 30.565157802896906, -88.07592805410286 30.539080789195197, -88.09370063563347 30.456267467088836, -88.09896689903184 30.398370320370773, -88.10620801120405 30.353504810736837, -88.15887064518647 30.323393627447288, -88.19178479142525 30.31032362908067, -88.20429216699586 30.330780246575657, -88.19705105482359 30.353504810736837, -88.28855238136764 30.381335210261483, -88.2957934935403 30.364865114151982, -88.31817511298274 30.363161152750493, -88.32014996175704 30.38190309511809, -88.3517475421464 30.390988804016374, -88.39058623470804 30.37224860380745, -88.39256108348229 30.342143188113525, -88.43732432236718 30.34725608152752, -88.44259058576552 30.331348424972347, -88.48077099540268 30.308618718161995, -88.5018360489957 30.312596797505776, -88.5288256489116 30.337030027619335, -88.55384040005282 30.331348424972347, -88.58807111214138 30.335893733464147, -88.60584475111035 30.368840908630105, -88.65784982716818 30.350096668434418, -88.70377316315728 30.333940318201655, -88.76345157602148 30.340304903230376, -88.81215810808295 30.366323547786152, -88.96059706293589 30.3823315816322, -89.10671665911954 30.334299618169936, -89.33169445006897 30.2782325348214, -89.41287200350473 30.192067121425936, -89.44534302487904 30.08977311890679, -89.359526754104 30.069703042406346, -89.20876844058175 30.176028017449696, -89.16006190852028 30.16199165952355, -89.16006190852028 30.08977311890679, -89.13918768049385 30.055651566395866, -89.16238126718959 29.921058052351142, -89.21108779925103 29.7782302647352, -89.24587817929464 29.66946405486304, -89.31545893938255 29.693644509171918, -89.27371048332964 29.629150377050877, -89.30386214603394 29.59688781400496, -89.3850396994697 29.61503677626405, -89.46157853556547 29.532331728567527, -89.3827203407997 29.439460132099583, -89.18979122539224 29.427128425449155, -89.19596940111164 29.394836842780364, -88.95749181835598 29.20950068771296, -89.04027937299146 29.04327521681688, -89.10329676532581 29.036793464759455, -89.13913018449634 28.94816874403513, -89.19473376596763 28.98276303815456, -89.26022242858967 29.027070073382717, -89.29358457747252 29.016265231135534, -89.4010848349841 28.903827915763785, -89.44186079473003 28.94816874403513, -89.39243538897739 29.025989640029877, -89.44680333530505 29.0864764893534, -89.45298151102406 29.14692783767063, -89.42209063242889 29.17066685855066, -89.376372132108 29.156639918576943, -89.36525141581359 29.180376694261838, -89.34671688865649 29.20195081707395, -89.37390086182032 29.206265096965808, -89.40232047012778 29.203029404070804, -89.4356826190106 29.20195081707395, -89.46780913274982 29.179297868997324, -89.49499310591362 29.193321712569713, -89.54441851166625 29.2278337721458, -89.63214860687673 29.254788228681946, -89.68404528291667 29.282813330704627, -89.80143062157865 29.305443379957012, -89.85579856790665 29.300055727855437, -89.9076952439466 29.27526886673428, -89.98924716343802 29.21489311194422, -90.05473582606008 29.165272108427317, -90.1572935429963 29.102672297767754, -90.26232567572231 29.06703684192044, -90.2833314731671 29.10914859304988, -90.26479694600961 29.13181701694839, -90.28827401374208 29.15232223739727, -90.29939473003648 29.19547774319095, -90.269739486585 29.23538042243976, -90.29568782460515 29.2310673685935, -90.30927981118687 29.24724038352842, -90.33893505463838 29.26341084252479, -90.393303000966 29.251552755878066, -90.40565935240406 29.30220951059583, -90.44767094729364 29.334530475168393, -90.50203889362164 29.32698983267116, -90.54899302908619 29.3032870410304, -90.57123446167503 29.309751984787326, -90.56629192109963 29.291433580776584, -90.59100462397578 29.241849662499888, -90.72198194921984 29.166349767170132, -90.74669465209598 29.108069019686837, -90.77634989554745 29.09943202507891, -90.84801673388853 29.115625795370804, -90.88755705849043 29.13721352427197, -90.89126396392177 29.16742872854931, -90.91474103165422 29.217048688862008, -90.92956865337999 29.198713674444775, -90.93080428852403 29.16742872854931, -90.97652278884493 29.160954790180483, -91.02965510002853 29.163112814994165, -91.08772995178788 29.17713887061521, -91.15445424955357 29.210577881865433, -91.21005783102524 29.21165637807694, -91.28790284508534 29.240771484217632, -91.33609261569393 29.292511224930067, -91.36327658885774 29.33237606278429, -91.36698404442399 29.461560546417502, -91.37316222014341 29.483075264842185, -91.42505889618336 29.473394206552683, -91.48189811279867 29.486302078781378, -91.50784645081843 29.51963980456361, -91.52885224826323 29.514263494469418, -91.5424442348453 29.538992155608895, -91.5424442348453 29.565864276975645, -91.56097876200242 29.62710598028727, -91.58692710002259 29.62603188493061, -91.6351168706312 29.61636451146687, -91.65118012750057 29.642142112800144, -91.63758814091887 29.656102225576717, -91.64129504635021 29.723727639327834, -91.71296188469128 29.718362205888425, -91.76980110130658 29.702264184791886, -91.83034722335326 29.685090118027063, -91.85505992622939 29.662544701481323, -91.845174845079 29.634624327137942, -91.79451380418274 29.609919080630874, -91.75744474986853 29.590580315799386, -91.69689862782151 29.57123784364269, -91.71914006041031 29.53254178333445, -91.74879530386181 29.517489314800784, -91.77103673645024 29.48199963736188, -91.8019276150454 29.470166981654714, -91.8525886559417 29.47662132872088, -91.94649692687157 29.517489314800784, -92.01816376521265 29.556191136499905, -92.081181157547 29.575536491098305, -92.14666982016863 29.57338719024669, -92.2282217396605 29.543292175193457, -92.2620745477254 29.513938983582918, -92.40184398133182 29.53587031357371, -92.53015690398729 29.55580375974114, -92.65388722226176 29.58171136156176, -92.76845233177524 29.613588513778055, -92.90134785881101 29.66735835342932, -92.99758255080252 29.705179169035475, -93.10985635812567 29.739006777997886, -93.2015084457365 29.758900043967557, -93.27024751144474 29.76088915343348, -93.3389865771526 29.740996282222838, -93.43980387352478 29.750943211286994, -93.61623414217547 29.740996282222838, -93.76058618016258 29.715129649330578, -93.8293252458708 29.663376387168597, -93.95534686633557 29.6852752506843, -94.10199020651301 29.65541198179467, -94.53962892485461 29.488014011072636, -94.71376789131534 29.402214927729204, -94.72293005770678 29.324334651639106, -94.9360211614021 29.206402439154715, -95.13765575414574 29.060298314788838, -95.37824248412433 28.86583930097359, -95.42635983011992 28.85781253647636, -95.88003908572986 28.63281185841163, -96.17332576608472 28.508049911739334, -96.3657951500674 28.385156078353774, -96.42995161139497 28.326681104162276, -96.5811775559527 28.237899002445175, -96.8538425165949 28.05404703841444, -97.01423366991399 27.871905659410203, -97.18378864850824 27.632633899635536, -97.28460594488008 27.443685488256804, -97.34647110401767 27.286998971872194, -97.37396673030096 27.111736817761667, -97.37167542811062 26.942327753778045, -97.30062980709232 26.653886203530476, -97.27907391837897 26.589650805967054, -97.227579295341 26.438555761793396, -97.19764056101677 26.3183955849306, -97.16770182669217 26.152971688301193, -97.15093613547081 26.036819954392655, -97.1461459379788 25.957168007570218))'\n",
    "Gulf_with_coast_Coarse = 'POLYGON ((-84.84686264156812 25.68340767152216, -87.43248750342957 30.94498325396067, -95.80214228446721 30.0359530662285, -98.3915463041922 26.769804868308682, -95.4673270575147 24.286120603664912, -84.84686264156812 25.68340767152216))'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d912b6e-eefd-46ed-8637-feb84ea50eec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Islands with lots of pings on it that should be excluded from feature calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa5ad1-8582-4817-8f67-4e72d20a7af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AlabamaIsland= 'POLYGON ((-88.11218406491841 30.28270197801872, -88.11748790022997 30.284420403489747, -88.1289123735651 30.296676657854306, -88.13525930319557 30.31441063898764, -88.12276411652998 30.344306892409193, -88.1261384293325 30.346573465262267, -88.14068302824425 30.330248934489845, -88.14841474252134 30.326463909723657, -88.14033683208271 30.309429488292594, -88.12233483298868 30.27266101723977, -88.13929844491092 30.259205656184136, -88.17437965596034 30.254122039939332, -88.2182311697725 30.24923753336725, -88.30108761392364 30.233386081193586, -88.32532134524112 30.23258845903686, -88.34528532389731 30.2314917180055, -88.40194641363063 30.215039068387483, -88.40817794454074 30.217232905188055, -88.42087180380169 30.212147117858834, -88.43252707457806 30.211449048099738, -88.44764430697126 30.20376995391028, -88.4625307419233 30.204368346411314, -88.48353330906463 30.209653988836138, -88.51157452826871 30.22032416074346, -88.51088213594514 30.2246118384801, -88.53211550052762 30.228699915260023, -88.57377443864924 30.22780254711091, -88.68005685360352 30.245947643114775, -88.7160612544177 30.256215137818614, -88.73648716752392 30.24843987369816, -88.7441034830807 30.249536425624242, -88.76568304382471 30.244851072739735, -88.77803070692467 30.247742061555087, -88.78576242120177 30.24166091740736, -88.86804184612036 30.248938361474544, -88.8717346051779 30.25133081048743, -88.89908410195018 30.234083937046506, -88.91223955609405 30.225409510211975, -88.9413202941934 30.222318343593045, -88.98551800416709 30.215836746861996, -88.9950960979732 30.208656633512476, -89.06366907456174 30.218130299450735, -89.05593736028412 30.237573354839526, -89.06678484001655 30.251729468053952, -89.08051728776313 30.235080870280683, -89.13729363724818 30.23378483291677, -89.1603733813597 30.22989637955409, -89.12047958599241 30.22005260021551, -89.12145473065867 30.211288948999467, -89.09844131653294 30.20977208396677, -89.10448721346415 30.20286162516733, -89.08849484093591 30.203030178716162, -89.0746477866739 30.219378500914573, -88.99020129980069 30.207581121374503, -88.97615921660534 30.20437870267233, -88.95626626541215 30.205221454538503, -88.93988353780959 30.21752474053794, -88.91335960288517 30.224434169581016, -88.90282804048856 30.221737864773814, -88.89112630449237 30.229152525120753, -88.8747438740979 30.238588556974634, -88.86908803503293 30.248529110671157, -88.78542036054029 30.240947271347025, -88.77430371134409 30.238082926154163, -88.75870139668251 30.23572399096632, -88.732957577491 30.23825141935825, -88.69941263547238 30.232690225631984, -88.64343933162394 30.22308510875915, -88.59780256123877 30.21954614500025, -88.57654440751222 30.227466506513878, -88.53129735506896 30.228477021928455, -88.52973712360316 30.21988264595238, -88.52349619773817 30.217523274113134, -88.51160410515517 30.21952563990618, -88.50750849755646 30.207053988309582, -88.46518721903666 30.192389349352695, -88.41935541971861 30.19660354973145, -88.40141275785743 30.214301221513225, -88.34641459868976 30.230142150145852, -88.30409253525568 30.224580980238727, -88.22452073048152 30.239072330381035, -88.17205794743222 30.243958466852533, -88.12642117704708 30.227108695087196, -88.07200810466486 30.2368819144896, -88.07493353866359 30.258783876570888, -88.11218406491841 30.28270197801872))'\n",
    "\n",
    "GalvestonIsland = 'POLYGON ((-94.79810501429436 29.285782165982226, -94.85587801828437 29.256474439597326, -94.96386123262589 29.194262486622222, -94.98840851114379 29.17802706499387, -94.9832015126703 29.172398185737194, -94.95751050670931 29.185942592884174, -94.93155434112559 29.200608310191058, -94.90207396798999 29.21719558256349, -94.87503715080425 29.23384831840025, -94.84904021158258 29.24836617943305, -94.82392719885846 29.263585041171126, -94.80006095983339 29.275328868980722, -94.79198685607093 29.280744467360933, -94.78065199926338 29.29067598019759, -94.7849675640115 29.294349034860943, -94.79810501429436 29.285782165982226))'\n",
    "\n",
    "BiscayneTX = 'POLYGON ((-94.72630885515686 29.39417826702656, -94.71635868628177 29.38842018852729, -94.70841340220476 29.39890094161126, -94.69809195840361 29.409315928554435, -94.6909634792623 29.41675455187773, -94.6995770582621 29.419341771414878, -94.72630885515686 29.39417826702656))'\n",
    "\n",
    "# Function to extract coordinates from polygon string\n",
    "def extract_coordinates(polygon):\n",
    "    coordinates = polygon.split('((')[1].split('))')[0].split(', ')\n",
    "    return [(float(coord.split()[0]), float(coord.split()[1])) for coord in coordinates]\n",
    "\n",
    "# Extract coordinates for each polygon\n",
    "alabama_coordinates = extract_coordinates(AlabamaIsland)\n",
    "galveston_coordinates = extract_coordinates(GalvestonIsland)\n",
    "biscayne_coordinates = extract_coordinates(BiscayneTX)\n",
    "\n",
    "# Find the largest and smallest latitudes and longitudes\n",
    "all_coordinates = alabama_coordinates + galveston_coordinates + biscayne_coordinates\n",
    "\n",
    "min_islands_latitude = min(coord[1] for coord in all_coordinates)\n",
    "max_islands_latitude = max(coord[1] for coord in all_coordinates)\n",
    "min_islands_longitude = min(coord[0] for coord in all_coordinates)\n",
    "max_islands_longitude = max(coord[0] for coord in all_coordinates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e09075d-d8e9-427b-969e-52ffcf7952bc",
   "metadata": {},
   "source": [
    "### Function that can be used to create a new variable indicating if the lat & lng are within a polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38d183-99d3-4e1b-afb7-8f5613f2d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function that can be used to create a new variable indicating if the lat & lng are within a polygon \"\"\"\n",
    "def is_inside_any_polygon(lat, lng, polygons):\n",
    "    #  Function that can be used to creat a TF new variable indiccating if a point is within a polygon\n",
    "    point = Point(lng, lat)\n",
    "    for polygon in polygons:\n",
    "        if polygon.contains(point):\n",
    "            return True\n",
    "    return False\n",
    "    \"\"\"\n",
    "    Usage suppose that \n",
    "            1. the polygon you want to use is defined by My_wkt \n",
    "            2. df contains lat, lng is df\n",
    "            3. new variable is \"is_in_wkt\"\n",
    "    polygons = [wkt.loads(My_wkt)]  # Create a list with a single polygon\n",
    "    df['is_in_wkt'] = df.apply(lambda row: is_inside_any_polygon(row['lat'], row['lng'], polygons), axis=1)\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad3568-59fe-47da-b294-daeda036d295",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Function that excludes rows that are within the islands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab419de-7818-45f6-bd25-ef514841dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon, Point  # Import the Point class\n",
    "\n",
    "# AlabamaIsland reformatting\n",
    "coordinates_str = AlabamaIsland.replace('POLYGON ((', '').replace('))', '')  # Remove the 'POLYGON ((' and '))'\n",
    "coordinates_list = [tuple(map(float, point.split())) for point in coordinates_str.split(',')]  # Split and convert to tuples\n",
    "# Create a Polygon object using the list of coordinates\n",
    "alabama_island_polygon = Polygon(coordinates_list)\n",
    "\n",
    "# GalvestonIsland reformatting\n",
    "coordinates_str = GalvestonIsland.replace('POLYGON ((', '').replace('))', '')  # Remove the 'POLYGON ((' and '))'\n",
    "coordinates_list = [tuple(map(float, point.split())) for point in coordinates_str.split(',')]  # Split and convert to tuples\n",
    "# Create a Polygon object using the list of coordinates\n",
    "galveston_island_polygon = Polygon(coordinates_list)\n",
    "\n",
    "# BiscayneTX reformatting\n",
    "coordinates_str = BiscayneTX.replace('POLYGON ((', '').replace('))', '')  # Remove the 'POLYGON ((' and '))'\n",
    "coordinates_list = [tuple(map(float, point.split())) for point in coordinates_str.split(',')]  # Split and convert to tuples\n",
    "# Create a Polygon object using the list of coordinates\n",
    "biscayne_tx_polygon = Polygon(coordinates_list)\n",
    "\n",
    "# Create a list of polygons\n",
    "island_polygons = [alabama_island_polygon, galveston_island_polygon, biscayne_tx_polygon]\n",
    "\n",
    "## , min_islands_latitude, max_islands_latitude, min_islands_longitude, max_islands_longitude\n",
    "def is_point_outside_all_islands(data_frame):\n",
    "    # Filter out points not in the Gulf but outside the islands rectangle\n",
    "    data_frame['in_Gulf_not_Islands_rectangle'] = (data_frame['lat'] < min_islands_latitude) | \\\n",
    "                                                   (data_frame['lat'] > max_islands_latitude) | \\\n",
    "                                                   (data_frame['lng'] < min_islands_longitude) | \\\n",
    "                                                   (data_frame['lng'] > max_islands_longitude)\n",
    "    filtered_data = data_frame[data_frame['in_Gulf_not_Islands_rectangle'] == False]\n",
    "    \n",
    "    # Apply the function is_point_outside_all_islands to the filtered data\n",
    "    filtered_data['in_Gulf_not_Islands'] = filtered_data.apply(is_point_outside_all_islands_row, axis=1)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "def is_point_outside_all_islands_row(row):\n",
    "    lat = row['lat']\n",
    "    lng = row['lng']\n",
    "    point = Point(lng, lat)\n",
    "    return all(not polygon.contains(point) for polygon in island_polygons)\n",
    "\n",
    "########################### ON Islands\n",
    "def is_point_on_islands(data_frame):\n",
    "    # Filter out points not in the Gulf but outside the islands rectangle\n",
    "    data_frame['in_Gulf_not_Islands_rectangle'] = (data_frame['lat'] < min_islands_latitude) | \\\n",
    "                                                   (data_frame['lat'] > max_islands_latitude) | \\\n",
    "                                                   (data_frame['lng'] < min_islands_longitude) | \\\n",
    "                                                   (data_frame['lng'] > max_islands_longitude)\n",
    "    filtered_data = data_frame[data_frame['in_Gulf_not_Islands_rectangle'] == False]\n",
    "    \n",
    "    # Apply the function is_point_outside_all_islands to the filtered data\n",
    "    filtered_data['in_Gulf_not_Islands'] = filtered_data.apply(is_point_on_islands_row, axis=1)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "def is_point_on_islands_row(row):\n",
    "    lat = row['lat']\n",
    "    lng = row['lng']\n",
    "    point = Point(lng, lat)\n",
    "    return all(polygon.contains(point) for polygon in island_polygons)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d79c38-b840-4354-adb7-3ecce9f1f017",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Function that finds out if a point is inside the Gulf wkt and not in the islands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5ca640-2eb0-479a-8d67-9a4f6e71f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon, Point  \n",
    "\n",
    "# AlabamaIsland reformatting\n",
    "coordinates_str = AlabamaIsland.replace('POLYGON ((', '').replace('))', '')  \n",
    "coordinates_list = [tuple(map(float, point.split())) for point in coordinates_str.split(',')]  \n",
    "alabama_island_polygon = Polygon(coordinates_list)\n",
    "\n",
    "# GalvestonIsland reformatting\n",
    "coordinates_str = GalvestonIsland.replace('POLYGON ((', '').replace('))', '')  \n",
    "coordinates_list = [tuple(map(float, point.split())) for point in coordinates_str.split(',')]  \n",
    "galveston_island_polygon = Polygon(coordinates_list)\n",
    "\n",
    "# BiscayneTX reformatting\n",
    "coordinates_str = BiscayneTX.replace('POLYGON ((', '').replace('))', '')  \n",
    "coordinates_list = [tuple(map(float, point.split())) for point in coordinates_str.split(',')]  \n",
    "biscayne_tx_polygon = Polygon(coordinates_list)\n",
    "\n",
    "# Create a list of polygons\n",
    "island_polygons = [alabama_island_polygon, galveston_island_polygon, biscayne_tx_polygon]\n",
    "\n",
    "# Gulf reformatting\n",
    "coordinates_str = Gulf.replace('POLYGON ((', '').replace('))', '')  \n",
    "coordinates_list = [tuple(map(float, point.split())) for point in coordinates_str.split(',')]  \n",
    "gulf_polygon = Polygon(coordinates_list)\n",
    "\n",
    "# Gulf with coast\n",
    "coordinates_str = Gulf_with_coast_Coarse.replace('POLYGON ((', '').replace('))', '')  \n",
    "coordinates_list = [tuple(map(float, point.split())) for point in coordinates_str.split(',')]  \n",
    "gulf_with_coast_polygon = Polygon(coordinates_list)\n",
    "\n",
    "# Create a function to check if a point is inside the Gulf and not inside any island polygon\n",
    "# def from shapely.geometry import Polygon, Point  # Import the Point class\n",
    "\n",
    "# AlabamaIsland reformatting\n",
    "coordinates_str = AlabamaIsland.replace('POLYGON ((', '').replace('))', '')  # Remove the 'POLYGON ((' and '))'\n",
    "coordinates_list = [tuple(map(float, point.split())) for point in coordinates_str.split(',')]  # Split and convert to tuples\n",
    "# Create a Polygon object using the list of coordinates\n",
    "alabama_island_polygon = Polygon(coordinates_list)\n",
    "\n",
    "# GalvestonIsland reformatting\n",
    "coordinates_str = GalvestonIsland.replace('POLYGON ((', '').replace('))', '')  # Remove the 'POLYGON ((' and '))'\n",
    "coordinates_list = [tuple(map(float, point.split())) for point in coordinates_str.split(',')]  # Split and convert to tuples\n",
    "# Create a Polygon object using the list of coordinates\n",
    "galveston_island_polygon = Polygon(coordinates_list)\n",
    "\n",
    "# BiscayneTX reformatting\n",
    "coordinates_str = BiscayneTX.replace('POLYGON ((', '').replace('))', '')  # Remove the 'POLYGON ((' and '))'\n",
    "coordinates_list = [tuple(map(float, point.split())) for point in coordinates_str.split(',')]  # Split and convert to tuples\n",
    "# Create a Polygon object using the list of coordinates\n",
    "biscayne_tx_polygon = Polygon(coordinates_list)\n",
    "\n",
    "# Create a list of polygons\n",
    "island_polygons = [alabama_island_polygon, galveston_island_polygon, biscayne_tx_polygon]\n",
    "\n",
    "# Create a function to check if a point is outside all polygons\n",
    "def is_point_in_Gulf_not_Islands(row):\n",
    "    lat = row['lat']\n",
    "    lng = row['lng']\n",
    "    point = Point(lng, lat)\n",
    "    \n",
    "    # Check if the point is inside the Gulf polygon\n",
    "    if gulf_polygon.contains(point):\n",
    "        # Check if the point is not inside any island polygon\n",
    "        if all(not polygon.contains(point) for polygon in island_polygons):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def is_point_in_Gulf_Coast_or_Islands(row):\n",
    "    lat = row['lat']\n",
    "    lng = row['lng']\n",
    "    point = Point(lng, lat)\n",
    "    \n",
    "    # Check if the point is inside the Gulf polygon\n",
    "    if gulf_with_coast_polygon.contains(point):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294c39f-908e-4dc1-87db-41bd5857988f",
   "metadata": {},
   "source": [
    "### Function that eliminates pings that are almost certainly erroneous (speed > 60 m.p.h.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a522f-790f-45c3-8e09-0acd8dda7209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pingspeed(pings_df):\n",
    "    pings_df['time_diff_minutes_from_previous'] = pings_df[\"event_timestamp\"].diff()/60.0\n",
    "    pings_df['time_diff_minutes_from_previous'].fillna(value=0, inplace=True)\n",
    "\n",
    "    pings_df['time_diff_minutes_to_next'] = pings_df[\"event_timestamp\"].diff(-1)/60.0\n",
    "    pings_df['time_diff_minutes_to_next'].fillna(value=99999, inplace=True)\n",
    "\n",
    "    \n",
    "    pings_df_shifted_down = pings_df.shift(1)\n",
    "    pings_df['dist_fwd'] = haversine(pings_df['lat'], pings_df['lng'], pings_df_shifted_down['lat'], pings_df_shifted_down['lng'])\n",
    "    pings_df['ping_speed_fwd'] = abs(pings_df['dist_fwd']/(0.00001+pings_df['time_diff_minutes_from_previous']))\n",
    "    pings_df['ping_speed_fwd'].fillna(value=0, inplace=True)\n",
    "\n",
    "    # Calculate speed moving backward e.g., first row is the speed to the next ping\n",
    "    pings_df_shifted_up = pings_df.shift(-1)\n",
    "    pings_df['dist_bkwd'] = haversine(pings_df['lat'], pings_df['lng'], pings_df_shifted_up['lat'], pings_df_shifted_up['lng'])\n",
    "    pings_df['ping_speed_bkwd'] = abs(pings_df['dist_bkwd']/(0.00001+pings_df['time_diff_minutes_to_next']))\n",
    "    pings_df['ping_speed_bkwd'].fillna(value=0, inplace=True)\n",
    "\n",
    "    pings_df['Avg_ping_speed'] = (pings_df['ping_speed_fwd'] + pings_df['ping_speed_bkwd']) / 2\n",
    "    \n",
    "    # columns_to_drop = ['ping_speed_bkwd', 'ping_speed_fwd', 'dist_bkwd', 'dist_fwd', 'time_diff_minutes_from_previous', 'time_diff_minutes_to_next']\n",
    "    # pings_df.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    return pings_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd4544-2130-4370-b313-cfb730c6eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2, to_radians=True, earth_radius=6371):\n",
    "    if to_radians:\n",
    "        lat1, lon1, lat2, lon2 = np.radians([lat1, lon1, lat2, lon2])\n",
    "\n",
    "    a = np.sin((lat2-lat1)/2.0)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin((lon2-lon1)/2.0)**2\n",
    "\n",
    "    return earth_radius * 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def EliminateErrantPings(pings_df):\n",
    "    # Calculate speed moving forward e.g., row 0 is time since previous trip\n",
    "    # create (or recreate) the time difference variables\n",
    "    pings_df['time_diff_minutes_from_previous'] = pings_df[\"event_timestamp\"].diff()/60.0\n",
    "    pings_df['time_diff_minutes_from_previous'].fillna(value=0, inplace=True)\n",
    "\n",
    "    pings_df['time_diff_minutes_to_next'] = pings_df[\"event_timestamp\"].diff(-1)/60.0\n",
    "    pings_df['time_diff_minutes_to_next'].fillna(value=99999, inplace=True)\n",
    "\n",
    "    \n",
    "    pings_df_shifted_down = pings_df.shift(1)\n",
    "    pings_df['dist_fwd'] = haversine(pings_df['lat'], pings_df['lng'], pings_df_shifted_down['lat'], pings_df_shifted_down['lng'])\n",
    "    pings_df['ping_speed_fwd'] = abs(pings_df['dist_fwd']/(0.00001+pings_df['time_diff_minutes_from_previous']))\n",
    "    pings_df['ping_speed_fwd'].fillna(value=0, inplace=True)\n",
    "\n",
    "    # Calculate speed moving backward e.g., first row is the speed to the next ping\n",
    "    pings_df_shifted_up = pings_df.shift(-1)\n",
    "    pings_df['dist_bkwd'] = haversine(pings_df['lat'], pings_df['lng'], pings_df_shifted_up['lat'], pings_df_shifted_up['lng'])\n",
    "    pings_df['ping_speed_bkwd'] = abs(pings_df['dist_bkwd']/(0.00001+pings_df['time_diff_minutes_to_next']))\n",
    "    pings_df['ping_speed_bkwd'].fillna(value=0, inplace=True)\n",
    "\n",
    "    pings_df['Avg_ping_speed'] = (pings_df['ping_speed_fwd'] + pings_df['ping_speed_bkwd']) / 2\n",
    "    pings_df['row_index'] = pings_df.reset_index().index\n",
    "\n",
    "\n",
    "    # Step 2: Check if the maximum of Avg_ping_speed > 2.0\n",
    "    iIteration=0\n",
    "    while len(pings_df) > 2 and pings_df['Avg_ping_speed'].max() > 1.6:  # 1.6 km/hr = 60 mp.h\n",
    "        iIteration=iIteration+1\n",
    "\n",
    "        max_index = pings_df['Avg_ping_speed'].idxmax()\n",
    "    \n",
    "        # Step 4: Recalculate ping_speed_fwd for the row after the row that was dropped\n",
    "        if max_index + 1 < len(pings_df) and max_index - 1 >= 0:\n",
    "            lat_after = pings_df.iloc[max_index+1]['lat']\n",
    "            lon_after = pings_df.iloc[max_index+1]['lng']\n",
    "            lat_before = pings_df.iloc[max_index - 1]['lat']\n",
    "            lon_before = pings_df.iloc[max_index - 1]['lng']\n",
    "            distance = haversine(lat_before, lon_before, lat_after, lon_after)\n",
    "            time_diff = pings_df.iloc[max_index+1]['event_timestamp']-pings_df.iloc[max_index-1]['event_timestamp']\n",
    "            new_speed = distance /time_diff\n",
    "            \n",
    "            # Calculate new fwd speed for the row before\n",
    "            index_before = max_index - 1\n",
    "            index_after = max_index + 1\n",
    "            \n",
    "            # Update the value using .loc[] or .iloc[] with a single call\n",
    "            pings_df.loc[index_before, 'ping_speed_fwd'] = new_speed\n",
    "            pings_df.loc[index_after, 'ping_speed_bkwd'] = new_speed\n",
    "    \n",
    "        pings_df = pings_df.drop(max_index)\n",
    "        pings_df['Avg_ping_speed'] = (pings_df['ping_speed_fwd'] + pings_df['ping_speed_bkwd']) / 2\n",
    "\n",
    "        # Reset index (I don't know if this is really necessary)\n",
    "        pings_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return pings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed16ac-012f-4e17-ae4b-478445f780aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EliminateErrantPingsSpeed(pings_df, mph_limit):\n",
    "    km_per_min_limit = mph_limit*(0.0268224)\n",
    "    # Calculate speed moving forward e.g., row 0 is time since previous trip\n",
    "    pings_df = pings_df.copy()\n",
    "    pings_df.sort_values(by='event_timestamp', inplace=True)\n",
    "    pings_df = pings_df.drop_duplicates()\n",
    "\n",
    "    # create (or recreate) the time difference variables\n",
    "    pings_df['time_diff_minutes_from_previous'] = abs(pings_df[\"event_timestamp\"].diff()/60.0)\n",
    "    pings_df['time_diff_minutes_from_previous'].fillna(value=0, inplace=True)\n",
    "\n",
    "    pings_df['time_diff_minutes_to_next'] = abs(pings_df[\"event_timestamp\"].diff(-1)/60.0)\n",
    "    pings_df['time_diff_minutes_to_next'].fillna(value=0, inplace=True)\n",
    "\n",
    "    \n",
    "    pings_df_shifted_down = pings_df.shift(1)\n",
    "    pings_df['dist_fwd'] = haversine(pings_df['lat'], pings_df['lng'], pings_df_shifted_down['lat'], pings_df_shifted_down['lng'])\n",
    "    pings_df['ping_speed_fwd'] = abs(pings_df['dist_fwd']/(0.00001+pings_df['time_diff_minutes_from_previous']))\n",
    "    pings_df['ping_speed_fwd'].fillna(value=0, inplace=True)\n",
    "\n",
    "    # Calculate speed moving backward e.g., first row is the speed to the next ping\n",
    "    pings_df_shifted_up = pings_df.shift(-1)\n",
    "    pings_df['dist_bkwd'] = haversine(pings_df['lat'], pings_df['lng'], pings_df_shifted_up['lat'], pings_df_shifted_up['lng'])\n",
    "    pings_df['ping_speed_bkwd'] = abs(pings_df['dist_bkwd']/(0.00001+pings_df['time_diff_minutes_to_next']))\n",
    "    pings_df['ping_speed_bkwd'].fillna(value=0, inplace=True)\n",
    "\n",
    "    pings_df['Avg_ping_speed'] = (pings_df['ping_speed_fwd'] + pings_df['ping_speed_bkwd']) / 2\n",
    "    pings_df['row_index'] = pings_df.reset_index().index\n",
    "\n",
    "    # Step 2: Check if the maximum of ping_speed > km_per_min_limit\n",
    "    iIteration=0\n",
    "    while len(pings_df) > 2 and pings_df['Avg_ping_speed'].max() > km_per_min_limit:\n",
    "        iIteration=iIteration+1\n",
    "\n",
    "        max_index = pings_df['Avg_ping_speed'].idxmax()\n",
    "    \n",
    "        # Step 4: Recalculate ping_speed_fwd for the row after the row that was dropped\n",
    "        if max_index + 1 < len(pings_df) and max_index - 1 >= 0:\n",
    "            lat_after = pings_df.iloc[max_index+1]['lat']\n",
    "            lon_after = pings_df.iloc[max_index+1]['lng']\n",
    "            lat_before = pings_df.iloc[max_index - 1]['lat']\n",
    "            lon_before = pings_df.iloc[max_index - 1]['lng']\n",
    "            distance = haversine(lat_before, lon_before, lat_after, lon_after)\n",
    "            time_diff = pings_df.iloc[max_index+1]['event_timestamp']-pings_df.iloc[max_index-1]['event_timestamp']\n",
    "            new_speed = 0\n",
    "            if distance*time_diff > 0:\n",
    "                new_speed = distance /time_diff\n",
    "            \n",
    "            # Calculate new fwd speed for the row before\n",
    "            index_before = max_index - 1\n",
    "            index_after = max_index + 1\n",
    "            \n",
    "            # Update the value using .loc[] or .iloc[] with a single call\n",
    "            pings_df.loc[index_before, 'ping_speed_fwd'] = new_speed\n",
    "            pings_df.loc[index_after, 'ping_speed_bkwd'] = new_speed\n",
    "\n",
    "            pings_df = pings_df[pings_df['event_timestamp'].notna() & (pings_df['event_timestamp'] != '')]\n",
    "\n",
    "        \n",
    "        ################ Debugging ###############\n",
    "        pings_df = pings_df.drop(max_index)\n",
    "        pings_df['Avg_ping_speed'] = (pings_df['ping_speed_fwd'] + pings_df['ping_speed_bkwd']) / 2\n",
    "\n",
    "        # Reset index (I don't know if this is really necessary)\n",
    "        pings_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return pings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a34ceff-0695-40cb-a305-9cdae9f777f3",
   "metadata": {},
   "source": [
    "### Function to create CDF of speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666f652-12d3-4f77-9358-58b426411839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def CumulativeSpeedsGraph(pings_df):\n",
    "\n",
    "    pings_df = pings_df.sort_values(by=['event_timestamp'], ascending=[True])\n",
    "    pings_df = pings_df.drop_duplicates(subset=['event_timestamp'])\n",
    "    pings_df['hours_since_start'] = (pings_df['event_timestamp'] - pings_df['event_timestamp'].min())/(60*60)\n",
    "\n",
    "    # Calculate speed moving forward e.g., row 0 is time since previous trip\n",
    "    # create (or recreate) the time difference variables\n",
    "    pings_df['time_diff_minutes_from_previous'] = pings_df[\"event_timestamp\"].diff()/60.0\n",
    "    pings_df['time_diff_minutes_from_previous'].fillna(value=0, inplace=True)\n",
    "\n",
    "    pings_df['time_diff_minutes_to_next'] = pings_df[\"event_timestamp\"].diff(-1)/60.0\n",
    "    pings_df['time_diff_minutes_to_next'].fillna(value=99999, inplace=True)\n",
    "\n",
    "    \n",
    "    pings_df_shifted_down = pings_df.shift(1)\n",
    "    pings_df['dist_fwd'] = haversine(pings_df['lat'], pings_df['lng'], pings_df_shifted_down['lat'], pings_df_shifted_down['lng'])\n",
    "    pings_df['dist_fwd'].fillna(value=0, inplace=True)\n",
    "    pings_df['ping_speed_fwd'] = abs(pings_df['dist_fwd']/(0.00001+pings_df['time_diff_minutes_from_previous']))\n",
    "    pings_df['ping_speed_fwd'].fillna(value=0, inplace=True)\n",
    "\n",
    "    pings_df['lat_origin'] = pings_df['lat'].iloc[0]\n",
    "    pings_df['lng_origin'] = pings_df['lng'].iloc[0]\n",
    "    pings_df['dist_from_origin'] =  haversine(pings_df['lat'], pings_df['lng'], pings_df['lat_origin'], pings_df['lng_origin'])\n",
    "\n",
    "    # Calculate speed moving backward e.g., first row is the speed to the next ping\n",
    "    pings_df_shifted_up = pings_df.shift(-1)\n",
    "    pings_df['dist_bkwd'] = haversine(pings_df['lat'], pings_df['lng'], pings_df_shifted_up['lat'], pings_df_shifted_up['lng'])\n",
    "    pings_df['dist_bkwd'].fillna(value=0, inplace=True)\n",
    "    pings_df['ping_speed_bkwd'] = abs(pings_df['dist_bkwd']/(0.00001+pings_df['time_diff_minutes_to_next']))\n",
    "    pings_df['ping_speed_bkwd'].fillna(value=0, inplace=True)\n",
    "\n",
    "    pings_df['Avg_ping_speed'] = (pings_df['ping_speed_fwd'] + pings_df['ping_speed_bkwd']) / 2\n",
    "    pings_df['row_index'] = pings_df.reset_index().index\n",
    "\n",
    "    # print(\"Timestamp | Time Difference | Distance | Speed\")\n",
    "    # for timestamp, timediff, dist, speed in zip(pings_df['event_timestamp'].head(50), \n",
    "    #                                             pings_df['time_diff_minutes_from_previous'].head(50), \n",
    "    #                                             pings_df['dist_fwd'].head(50), \n",
    "    #                                             pings_df['ping_speed_fwd'].head(50)):\n",
    "    #     print(f\"{timestamp} |          {timediff:.5f} |      {dist:.2f} |   {speed:.2f}\")\n",
    "\n",
    "    # Step 2: Check if the maximum of Avg_ping_speed > 2.0\n",
    "    iIteration=0\n",
    "    \n",
    "    while len(pings_df) > 2 and pings_df[['ping_speed_fwd', 'ping_speed_bkwd']].max().max() > 1.60934:  # 1.6 km/hr = 60 mp.h\n",
    "    # while len(pings_df) > 2 and pings_df['Avg_ping_speed'].max() > 1.6:  # 1.6 km/hr = 60 mp.h\n",
    "        iIteration=iIteration+1\n",
    "\n",
    "        max_index = pings_df['Avg_ping_speed'].idxmax()\n",
    "    \n",
    "        # Step 4: Recalculate ping_speed_fwd for the row after the row that was dropped\n",
    "        if max_index + 1 < len(pings_df) and max_index - 1 >= 0:\n",
    "            lat_after = pings_df.iloc[max_index+1]['lat']\n",
    "            lon_after = pings_df.iloc[max_index+1]['lng']\n",
    "            lat_before = pings_df.iloc[max_index - 1]['lat']\n",
    "            lon_before = pings_df.iloc[max_index - 1]['lng']\n",
    "            distance = haversine(lat_before, lon_before, lat_after, lon_after)\n",
    "            time_diff = pings_df.iloc[max_index+1]['event_timestamp']-pings_df.iloc[max_index-1]['event_timestamp']\n",
    "            new_speed = 0\n",
    "            if distance*time_diff > 0:\n",
    "                new_speed = distance /time_diff\n",
    "            \n",
    "            # Calculate new fwd speed for the row before\n",
    "            index_before = max_index - 1\n",
    "            index_after = max_index + 1\n",
    "            \n",
    "            # Update the value using .loc[] or .iloc[] with a single call\n",
    "            pings_df.loc[index_before, 'ping_speed_fwd'] = new_speed\n",
    "            pings_df.loc[index_after, 'ping_speed_bkwd'] = new_speed\n",
    "    \n",
    "        pings_df = pings_df.drop(max_index)\n",
    "        pings_df['Avg_ping_speed'] = (pings_df['ping_speed_fwd'] + pings_df['ping_speed_bkwd']) / 2\n",
    "\n",
    "        # Reset index (I don't know if this is really necessary)\n",
    "        pings_df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    # print(\"Timestamp | Time Difference | Distance | Speed\")\n",
    "    # for timestamp, timediff, dist, speed in zip(pings_df['event_timestamp'].head(50), \n",
    "    #                                             pings_df['time_diff_minutes_from_previous'].head(50), \n",
    "    #                                             pings_df['dist_fwd'].head(50), \n",
    "    #                                             pings_df['ping_speed_fwd'].head(50)):\n",
    "    #     print(f\"{timestamp} |          {timediff:.2f} |      {dist:.2f} |   {speed:.2f}\")\n",
    "    ####################################################################\n",
    "    # Now create the cumulative speeds graph using forward speed\n",
    "    # convert speed fwd to MPH\n",
    "    \n",
    "    ##############################################################################\n",
    "    ###   Cumulative Speed Graph \n",
    "    ##############################################################################\n",
    "    #### Create graph with cumulative time at speeed\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "    fig_columns_df = pd.DataFrame()\n",
    "    fig_columns_df['ping_speed_fwd'] = pings_df['ping_speed_fwd']*37.2823\n",
    "    fig_columns_df['time_diff_minutes_from_previous'] = pings_df['time_diff_minutes_from_previous']\n",
    "    fig_columns_df = fig_columns_df.sort_values(by=['ping_speed_fwd'], ascending=[True])\n",
    "    fig_columns_df['cumulative_time_at_speed_lt'] = fig_columns_df['time_diff_minutes_from_previous'].cumsum()\n",
    "    fig_columns_df['cumulative_time_at_speed_lt'].fillna(value=0, inplace=True)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the data\n",
    "    ax.plot(fig_columns_df['ping_speed_fwd'], fig_columns_df['cumulative_time_at_speed_lt'])\n",
    "\n",
    "    # Set the labels for the axes\n",
    "    ax.set_xlabel('ping_speed_fwd')\n",
    "    ax.set_ylabel('cumulative_time_at_speed_lt')\n",
    "\n",
    "    # Set major ticks at 1 unit intervals on the x-axis\n",
    "    ax.set_xlim(left=0)\n",
    "    GridStep = (fig_columns_df['ping_speed_fwd'].max())/20\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(5))\n",
    "\n",
    "    # Enable grid lines\n",
    "    ax.grid(True)    \n",
    "    # Show the plot\n",
    "    # plt.show()\n",
    "    ##############################################################################\n",
    "    ### End of cumulative speed graph \"\"\"\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "#     #### Create graph with time on horizontal axis speed and distance \n",
    "#     # Step 1: Create a figure and a set of subplots\n",
    "    fig_speeds, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.plot(pings_df['hours_since_start'], 37.2823*pings_df['ping_speed_fwd'], marker='o', linestyle='None', label='Ping Speed Forward', color='tab:blue')\n",
    "    ax1.plot(pings_df['hours_since_start'], 37.2823*pings_df['ping_speed_bkwd'], marker='o', linestyle='None', label='Ping Speed Backward', color='tab:orange')\n",
    "    # ax1.plot(pings_df['hours_since_start'], 37.2823*pings_df['ping_speed_fwd'], label='Ping Speed Forward', color='tab:blue')\n",
    "    # ax1.plot(pings_df['hours_since_start'], 37.2823*pings_df['ping_speed_bkwd'], label='Ping Speed Backward', color='tab:orange')\n",
    "    ax1.set_xlabel('Hours Since Start')\n",
    "    ax1.set_ylabel('Ping Speed (fwd/bkwd)')\n",
    "    ax1.tick_params(axis='y')\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    # Step 3: Create a secondary y-axis for dist_from_origin\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pings_df['hours_since_start'], pings_df['dist_from_origin'], label='Distance from Origin', color='tab:green')\n",
    "    ax2.set_ylabel('Distance from Origin')\n",
    "    ax2.tick_params(axis='y')\n",
    "\n",
    "    # Step 4: Combine legends\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines + lines2, labels + labels2, loc='upper center')\n",
    "\n",
    "    # Step 5: Display the plot\n",
    "    plt.title('Ping Speeds and Distance from Origin over Time')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    return fig_columns_df\n",
    "    # return fig, fig_columns_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aadd9c-7c1d-4de9-bedd-0bc0a3eb4c62",
   "metadata": {},
   "source": [
    "### Assorted date & time functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545b6de-2ef5-45a3-a13e-1339daf0c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE GENERATE THE SEASON BASED ON MONTH\n",
    "\n",
    "# def month_to_season(month_num):\n",
    "    \n",
    "#     monthMar= (month_num>1)*(month_num-1) + (month_num<=1)*(11+month_num)\n",
    "#     season=int((monthMar) / 3)+1\n",
    "    \n",
    "#     return season\n",
    "# month 6 and 1 are wrong\n",
    "\n",
    "def month_to_season(month_num):\n",
    "    if (month_num == 11) | (month_num == 0) | (month_num == 1): # DEC-FEB\n",
    "        season = 4\n",
    "    if (month_num == 2) | (month_num == 3) | (month_num == 4): # MARCH - MAY\n",
    "        season = 1\n",
    "    if (month_num == 5) | (month_num == 6) | (month_num == 7): # JUNE - August \n",
    "        season = 2\n",
    "    if (month_num == 8) | (month_num == 9) | (month_num == 10): # SEP- NOV\n",
    "        season = 3\n",
    "    return season\n",
    "\n",
    "# check season function\n",
    "season= month_to_season(1)\n",
    "season\n",
    "\n",
    "# note 0 equals Jan and 11 equals december \n",
    "def monthofyear(EpochTime):  # January = 0\n",
    "    Base = 1546322400  # Jan. 1, 2019, 12:00 a.m. \n",
    "    Jan12021 = 731\n",
    "    Jan12022 = 1096\n",
    "    Jan12023 = 1461\n",
    "\n",
    "    FebStart = 31\n",
    "    MarStart = 59\n",
    "    AprStart = 90\n",
    "    MayStart = 120\n",
    "    JunStart = 151\n",
    "    JulStart = 181\n",
    "    AugStart = 212\n",
    "    SepStart = 243\n",
    "    OctStart = 273\n",
    "    NovStart = 304\n",
    "    DecStart = 334\n",
    "    leapyear2020 = 1582869600     # Feb 29, 2020\n",
    "\n",
    "    days_since_2019 = epoch_to_days_since_1_1_2019(EpochTime)\n",
    "    leapyearadjust = -1*(EpochTime >leapyear2020)\n",
    "    year = int((days_since_2019+ leapyearadjust)/365)\n",
    "    dayofyear = (days_since_2019) - 365*year +  leapyearadjust\n",
    "    month = 1\n",
    "    month = 1*(dayofyear>=FebStart) + \\\n",
    "            1*(dayofyear>=    MarStart ) + \\\n",
    "            1*(dayofyear>=    AprStart ) + \\\n",
    "            1*(dayofyear>=    MayStart ) + \\\n",
    "            1*(dayofyear>=    JunStart ) + \\\n",
    "            1*(dayofyear>=    JulStart ) + \\\n",
    "            1*(dayofyear>=    AugStart ) + \\\n",
    "            1*(dayofyear>=    SepStart ) + \\\n",
    "            1*(dayofyear>=    OctStart ) + \\\n",
    "            1*(dayofyear>=    NovStart ) + \\\n",
    "            1*(dayofyear>=    DecStart )\n",
    "    return month\n",
    "        \n",
    "\n",
    "def epoch_to_season(EpochTime):\n",
    "    month = monthofyear(EpochTime)\n",
    "    season = month_to_season(month)\n",
    "    return season\n",
    "\n",
    "def epoch_to_DOW(EpochTime):\n",
    "    Base = 1546322400  # Jan. 1, 2019, 12:00 a.m. \n",
    "    BaseDOW = 1\n",
    "    DaySince = (EpochTime - Base) / (24*60*60)\n",
    "    WeeksSinceBase = DaySince / 7\n",
    "    DayOfWeek = BaseDOW + int((WeeksSinceBase - int(WeeksSinceBase)) * 7)\n",
    "    return DayOfWeek\n",
    "\n",
    "def epoch_to_days_since_1_1_2019(EpochTime):\n",
    "    Base = 1546322400  # Jan. 1, 2019, 12:00 a.m. \n",
    "    DaySinceBase = int((EpochTime-Base) / (60*60*24))\n",
    "    return DaySinceBase\n",
    "\n",
    "def AISdate_to_epoch(AISDate):\n",
    "    from datetime import datetime\n",
    "\n",
    "    # date_string = \"2019-06-01T16:14:14\"\n",
    "\n",
    "    # Define the format of the input date string\n",
    "    date_format = \"%Y-%m-%dT%H:%M:%S\"\n",
    "\n",
    "    # Convert the date string to a datetime object\n",
    "    dt_object = datetime.strptime(AISDate, date_format)\n",
    "\n",
    "    # Convert the datetime object to epoch time\n",
    "    epoch_time = int(dt_object.timestamp())\n",
    "\n",
    "    # print(\"Epoch Time:\", epoch_time)\n",
    "    return epoch_time\n",
    "\n",
    "def epoch_to_hour_of_day(EpochTime):\n",
    "    Base = 1546322400  # Jan. 1, 2019, 12:00 a.m. \n",
    "    # Daylight Savings Time points for US Central Time\n",
    "    start2019 = 1552204800  \n",
    "    end2019 = 1572768000\n",
    "    start2020 = 1583654400\n",
    "    end2020 = 1604217600\n",
    "    start2021 = 1615708800\n",
    "    end2021 = 1636272000\n",
    "    start2022 = 1647158400\n",
    "    end2022 = 1667721600\n",
    "    start2023 = 1678608000\n",
    "    end2023 = 1699171200\n",
    "\n",
    "    \n",
    "    DayLightSavingsAdjust = +1 * (EpochTime > start2019) + \\\n",
    "                            -1 * (EpochTime > end2019) + \\\n",
    "                            +1 * (EpochTime > start2020) + \\\n",
    "                            -1 * (EpochTime > end2020) + \\\n",
    "                            +1 * (EpochTime > start2021) + \\\n",
    "                            -1 * (EpochTime > end2021) + \\\n",
    "                            +1 * (EpochTime > start2022) + \\\n",
    "                            -1 * (EpochTime > end2022) + \\\n",
    "                            +1 * (EpochTime > start2023) + \\\n",
    "                            -1 * (EpochTime > end2023)\n",
    "#    print(DayLightSavingsAdjust)\n",
    "    DaysSince = ((EpochTime-Base) / (60*60*24))\n",
    "    PortionOfDay = DaysSince - int(DaysSince)\n",
    "    HourOfDay = int(PortionOfDay*24) + DayLightSavingsAdjust\n",
    "    return HourOfDay\n",
    "\n",
    "def epoch_to_date(epoch_time):\n",
    "    # Convert epoch time to a datetime object\n",
    "    dt = datetime.fromtimestamp(epoch_time)\n",
    "    \n",
    "    # Format the datetime as 'YYYY-MM-DD'\n",
    "    formatted_date = dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    return formatted_date\n",
    "\n",
    "def epoch_to_datetime(epoch_time):\n",
    "    return datetime.fromtimestamp(epoch_time)\n",
    "\n",
    "def date_to_epoch(date_string, date_format='%m/%d/%Y'):\n",
    "    \"\"\"\n",
    "    Convert a date string to epoch time integer.\n",
    "    \n",
    "    Parameters:\n",
    "        date_string (str): The date string to convert.\n",
    "        date_format (str): The format of the date string. Default is '%m/%d/%Y'.\n",
    "    \n",
    "    Returns:\n",
    "        int: Epoch time integer.\n",
    "    \"\"\"\n",
    "    # Parse the date string to a datetime object\n",
    "    date_obj = datetime.strptime(date_string, date_format)\n",
    "    \n",
    "    # Convert the datetime object to epoch time\n",
    "    epoch_time = int(date_obj.timestamp())\n",
    "    \n",
    "    return epoch_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec5714-cdc0-4a53-b33d-24f76c3494ef",
   "metadata": {},
   "source": [
    "# Set directories to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7b40ae-2283-4ac1-93bb-f3994184273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Directory for this output\n",
    "OurTable_V3_directory = '~/RecFishing/Analysis with Our Tables and V3/Data Files'\n",
    "# Expand the tilde to the user's home directory\n",
    "OurTable_V3_directory = os.path.expanduser(OurTable_V3_directory)\n",
    "# Check to make sure the directory exist\n",
    "DirExist = os.path.exists(OurTable_V3_directory)\n",
    "print(OurTable_V3_directory, \"exists = \" ,DirExist)\n",
    "\n",
    "#################################################################\n",
    "# Directory for Groups of V3 Pings\n",
    "V3_Pings_Groups_directory = '~/RecFishing/Analysis with Our Tables and V3/Data Files/V3_Ping_Groups'\n",
    "V3_Pings_Groups_directory = os.path.expanduser(V3_Pings_Groups_directory)\n",
    "print(V3_Pings_Groups_directory, \"exists = \" ,os.path.exists(V3_Pings_Groups_directory))\n",
    "\n",
    "#################################################################\n",
    "# Directory some core data fro analysis\n",
    "CoreData_Directory = '~/RecFishing/CoreData'\n",
    "CoreData_Directory = os.path.expanduser(CoreData_Directory)\n",
    "print(CoreData_Directory, \"exists = \", os.path.exists(CoreData_Directory))\n",
    "\n",
    "#################################################################\n",
    "# Directory  with Original_directory material\n",
    "Original_directory = '~/RecFishing/DataflowStudioJobs'\n",
    "Original_directory = os.path.expanduser(Original_directory)\n",
    "DirExist = os.path.exists(Original_directory)\n",
    "print(Original_directory, \"exists = \", DirExist)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory  with Original_directory material\n",
    "Previously_Processed_directory = '~/RecFishing/DataflowStudioJobs/FinalCode - Rec Fishing Identification'\n",
    "Previously_Processed_directory = os.path.expanduser(Previously_Processed_directory)\n",
    "DirExist = os.path.exists(Previously_Processed_directory)\n",
    "print(Previously_Processed_directory, \"exists = \", DirExist)\n",
    "\n",
    "#################################################################\n",
    "# Directory with Travel Cost files\n",
    "Travel_Cost_directory = '~/RecFishing/Travel Costs with Dedicated Table/CSV Files'\n",
    "# Expand the tilde to the user's home directory\n",
    "Travel_Cost_directory = os.path.expanduser(Travel_Cost_directory)\n",
    "DirExist = os.path.exists(Travel_Cost_directory)\n",
    "print(Travel_Cost_directory, \"exists = \", DirExist)\n",
    "\n",
    "#################################################################\n",
    "# Directory with Weather data and related files\n",
    "Weather_Data_directory = '~/RecFishing/uploaded_files/Weather Data'\n",
    "# Expand the tilde to the user's home directory\n",
    "Weather_Data_directory = os.path.expanduser(Weather_Data_directory)\n",
    "print(Weather_Data_directory, \"exists = \", DirExist)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory with other Uploaded data \n",
    "Uploaded_Data_directory = '~/RecFishing/uploaded_files'\n",
    "# Expand the tilde to the user's home directory\n",
    "Uploaded_Data_directory = os.path.expanduser(Uploaded_Data_directory)\n",
    "\n",
    "####################################################################################\n",
    "####################  AIS Directory #################################################\n",
    "AIS_Directory = '~/RecFishing/AIS Files/Data'\n",
    "AIS_Directory = os.path.expanduser(AIS_Directory)\n",
    "DirExist = os.path.exists(AIS_Directory)\n",
    "print(AIS_Directory, \"exists = \", DirExist)\n",
    "\n",
    "# ID_list_RandomSample from ScheduledExecution5.pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417217e3-69d2-4b41-984b-02f54ad06676",
   "metadata": {},
   "source": [
    "## Set input and output files to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e72a9-2f4e-4e1f-a297-fab1119a0d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_existence(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"{file_path} Does NOT Exist\")\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "#########################  Log File  ################\n",
    "Log_filename  =  os.path.join(OurTable_V3_directory, 'Log.txt')\n",
    "\n",
    "######################################################################################################################\n",
    "########################## Complete list of randomized IDs- without bernouli sampling 740k #########################\n",
    "# PKL_File_With_Random_IDs_Filename  =  os.path.join(Original_directory, 'cuebiq_id_list_wo_sampling_740k.pkl')\n",
    "PKL_File_With_Random_IDs_Filename  =  os.path.join(CoreData_Directory, 'cuebiq_id_list_wo_sampling_740k.pkl')\n",
    "check_file_existence(PKL_File_With_Random_IDs_Filename)\n",
    "    \n",
    "# Data gathered and used prior to the NOAA Webinar in February 2024\n",
    "IDs_Used_in_NOAA_Webinar_filename = os.path.join(CoreData_Directory, 'IDs_From_Random_Draw_Prior_to_NOAA_Webinar.csv')\n",
    "check_file_existence(IDs_Used_in_NOAA_Webinar_filename)\n",
    "Ping_Used_in_NOAA_Webinar_filename = os.path.join(CoreData_Directory, 'Pings_From_Random_Draw_Prior_to_NOAA_Webinar.csv')\n",
    "check_file_existence(Ping_Used_in_NOAA_Webinar_filename)\n",
    "                                                     \n",
    "# List of IDs that have been processed for Indicators\n",
    "AlreadyFullyProcessedIDs_Filename  =  os.path.join(OurTable_V3_directory, 'RandomlyChosenCuebiq_ids.List_of_Processed_ids.csv')\n",
    "check_file_existence(AlreadyFullyProcessedIDs_Filename)\n",
    "    \n",
    "######################################################################################################################\n",
    "#########################  ID Checklist with columns for ID, Pings, Indicators Created (TF) & Trips  ################\n",
    "IDs_Pulled_from_Dedicated_Table_filename  =  os.path.join(OurTable_V3_directory, 'IDs_Pulled_From_Dedicated_Table.csv')\n",
    "check_file_existence(IDs_Pulled_from_Dedicated_Table_filename)\n",
    "    \n",
    "ID_For_V3_Queries_filename  =  os.path.join(OurTable_V3_directory, 'IDs_from_V3.csv')\n",
    "check_file_existence(ID_For_V3_Queries_filename)\n",
    "    \n",
    "RecTripRating_filename =  os.path.join(OurTable_V3_directory, 'RecTripRating.csv')\n",
    "check_file_existence(RecTripRating_filename)\n",
    "    \n",
    "# This file contains information about the rows of Pings_V3_temp_filename that can be used to avoid loading the entire file into a data frame\n",
    "V3_Pings_Index_filename =  os.path.join(OurTable_V3_directory, 'V3_Pings_File_Index.csv')\n",
    "check_file_existence(V3_Pings_Index_filename)\n",
    "    \n",
    "ID_Groups_filename = os.path.join(OurTable_V3_directory,'Cuebiq_ID_Groups.csv')\n",
    "check_file_existence(ID_Groups_filename)\n",
    "\n",
    "######################################################\n",
    "# Pings in the OurTable for a single large draw of IDs TEMPORARY FILE\n",
    "Pings_OurTable_temp_filename = os.path.join(OurTable_V3_directory,'Pings_OurTable_temp.csv')\n",
    "check_file_existence(Pings_OurTable_temp_filename)\n",
    "    \n",
    "# Pings from V3 corresponding with the IDs found in the OurTable \n",
    "Pings_V3_temp_filename = os.path.join(OurTable_V3_directory,'Pings_V3_temp.csv')\n",
    "check_file_existence(Pings_V3_temp_filename)\n",
    "    \n",
    "# Set output file names\n",
    "Indicators_IDs_checked_filename = os.path.join(OurTable_V3_directory, 'IDs_Checked_Indicators_OurTable.csv')\n",
    "check_file_existence(Indicators_IDs_checked_filename)\n",
    "\n",
    "cuebiq_id_list_and_count_filename= os.path.join(OurTable_V3_directory,'cuebiq_id_list_and_count.csv')\n",
    "check_file_existence(cuebiq_id_list_and_count_filename)\n",
    "\n",
    "# List of IDs and dates for V3 query Pings in the OurTable for a single large draw of IDs TEMPORARY FILE\n",
    "OurTable_IDs_and_Dates_filename = os.path.join(OurTable_V3_directory,'OurTable_IDs_and_Dates.csv')\n",
    "check_file_existence(OurTable_IDs_and_Dates_filename)\n",
    "    \n",
    "##########################################################################################\n",
    "############################### PINGS FILES   ##############################################\n",
    "Pings_OurTable_Gulf_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Gulf_ALL.csv')\n",
    "check_file_existence(Pings_OurTable_Gulf_filename)\n",
    "\n",
    "Pings_V3_Before_After_filename= os.path.join(OurTable_V3_directory,'Pings_V3_Before_After.csv')\n",
    "check_file_existence(Pings_V3_Before_After_filename)\n",
    "\n",
    "Pings_OurTable_Gulf_MT19_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Gulf_MT19.csv')\n",
    "check_file_existence(Pings_OurTable_Gulf_MT19_filename)\n",
    "\n",
    "Pings_OurTable_Coast_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Coast.csv')\n",
    "check_file_existence(Pings_OurTable_Coast_filename)\n",
    "\n",
    "Pings_OurTable_Outside_our_wkts_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Outside_our_wkts.csv')\n",
    "check_file_existence(Pings_OurTable_Outside_our_wkts_filename)\n",
    "    \n",
    "##########################################################################################\n",
    "############################### INDICATORS  ##############################################\n",
    "Indicators_filename = os.path.join(OurTable_V3_directory,'Indicators_OurTable.csv')\n",
    "check_file_existence(Indicators_filename)\n",
    "    \n",
    "# cuebiq_id_count_filename= os.path.join(EEZ_V3_directory,'cuebiq_id_count_distribution_EEZ_V3.csv')\n",
    "Indicators_Classified_filename = os.path.join(OurTable_V3_directory,'Indicators_OurTable.Predictions.csv')\n",
    "check_file_existence(Indicators_Classified_filename)\n",
    "\n",
    "Rec_Indicators_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable.csv')\n",
    "check_file_existence(Rec_Indicators_filename)\n",
    "\n",
    "Rec_Indicators_Step1_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable.Step1.csv')\n",
    "check_file_existence(Rec_Indicators_filename)\n",
    "\n",
    "V3_Indicators_filename =  os.path.join(OurTable_V3_directory,'V3_indicators.csv')\n",
    "check_file_existence(V3_Indicators_filename)\n",
    "\n",
    "Rec_indicators_with_V3_filename = os.path.join(OurTable_V3_directory,'rec_indicators_with_V3.csv')\n",
    "check_file_existence(Rec_indicators_with_V3_filename)\n",
    "\n",
    "Indicators_with_V3_indicators_filename= os.path.join(OurTable_V3_directory,'Indicators_with_V3_indicators_indicators.csv')\n",
    "check_file_existence(Indicators_with_V3_indicators_filename)\n",
    "\n",
    "# Rec_Indicators_Selected_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable_Selected.csv')\n",
    "Rec_Indicators_Selected_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable_Selected_May2024.csv')\n",
    "check_file_existence(Rec_Indicators_Selected_filename)\n",
    "\n",
    "Rec_Indicators_Final_All_Exclusions_And_Disappearance_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_Final_All_Exclusions_And_Disappearance.csv')\n",
    "check_file_existence(Rec_Indicators_Final_All_Exclusions_And_Disappearance_filename)\n",
    "\n",
    "# Sorted_Results_file_path = os.path.join(OurTable_V3_directory,'Indicators_EEZ_and_V3.Predictions.sorted.csv')\n",
    "# RecFishing_Results_file_path =  os.path.join(OurTable_V3_directory,'RecFishingBoat Predictions.sorted.csv')\n",
    "\n",
    "DisappearanceIndicators_filename = os.path.join(OurTable_V3_directory,'DisappearanceIndicators.csv')\n",
    "check_file_existence(DisappearanceIndicators_filename)\n",
    "\n",
    "DisappearanceAnalysis_filename = os.path.join(OurTable_V3_directory,'DisappearanceAnalysis.csv')\n",
    "check_file_existence(DisappearanceAnalysis_filename)\n",
    "\n",
    "Stops_Indicators_filename = os.path.join(OurTable_V3_directory,'Stops_Indicators.csv')\n",
    "check_file_existence(Stops_Indicators_filename)\n",
    "\n",
    "Trawls_Indicators_filename = os.path.join(OurTable_V3_directory,'Trawls_Indicators.csv')\n",
    "check_file_existence(Trawls_Indicators_filename)\n",
    "\n",
    "Stop_Trawls_Indicators_filename = os.path.join(OurTable_V3_directory,'Stop_Trawls_Indicators.csv')\n",
    "check_file_existence(Stop_Trawls_Indicators_filename)\n",
    "##########################################################################################\n",
    "############################### WEATHER data files ####################################\n",
    "Buoys_file_path  = os.path.join(Weather_Data_directory,'Buoys.csv')\n",
    "check_file_existence(Buoys_file_path)\n",
    "\n",
    "Weather_file_path  = os.path.join(Weather_Data_directory,'DailyWeatherData.csv')\n",
    "check_file_existence(Weather_file_path)\n",
    "\n",
    "##########################################################################################\n",
    "############################### SUPPLEMENTARY MAP DATA  ############################\n",
    "Industrial_polygons_filename  = os.path.join(Uploaded_Data_directory,'Polygons Around Industrial Sites.wkt')\n",
    "check_file_existence(Industrial_polygons_filename)\n",
    "\n",
    "##########################################################################################\n",
    "############################### AIS Files INCLUDING CLASSIFIER ############################\n",
    "RF_Classfier_filename = os.path.join(AIS_Directory, 'rf_model_AIS_2019.pkl')\n",
    "check_file_existence(RF_Classfier_filename)\n",
    "\n",
    "RF_Importance_Factors_filename = os.path.join(AIS_Directory, 'rf_classifier_importance_factors.csv')\n",
    "check_file_existence(RF_Importance_Factors_filename)\n",
    "\n",
    "AIS_Predictions_filename = os.path.join(AIS_Directory, 'RandomForest_Predictions2019AISData.csv')\n",
    "check_file_existence(AIS_Predictions_filename)\n",
    "\n",
    "AIS_indicators_file_path = os.path.join(AIS_Directory,'Indicators2019_All.C.csv')\n",
    "check_file_existence(AIS_indicators_file_path)\n",
    "  \n",
    "\n",
    "### Dedicate Table Names for reference\n",
    "# Dedicated table with all Pings within the Gulf WKT for 1/12019 - 4/22/2022\n",
    "#  Table Name:  dedicated.ScheduledExecution5.DeviceTable   \n",
    "#  Code used for call:  RecFishing/DataflowStudioJobs/ScheduledEx5-updated.ipynb\n",
    "\n",
    "# Dedicated table with all Pings within the Gulf WKT AND Origin for 1/12019 - 4/22/2022\n",
    "#  Table Name:  dedicated.ScheduledExecution5_parallel_origin.DeviceTable\n",
    "#  Code used for call:  RecFishing/DataflowStudioJobs/ScheduledEx5-origin.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ad978-5aeb-4fec-a49a-5da9737c5b62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Map Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78265b-1580-4d12-a73b-fc8332a63d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "platforms= gpd.read_file(\"~/RecFishing/Travel Costs with Dedicated Table/Shapefiles/platform.shx\").to_crs(epsg=4326)\n",
    "platforms['lng'] = platforms['geometry'].x\n",
    "platforms['lat'] = platforms['geometry'].y\n",
    "\n",
    "platforms['REMOVAL_DA'] = pd.to_datetime(platforms['REMOVAL_DA'], errors='coerce', format='%Y-%m-%d')\n",
    "platforms['INSTALL_DA'] = pd.to_datetime(platforms['INSTALL_DA'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "platforms = platforms[(platforms['REMOVAL_DA'].isna()) | \n",
    "    (platforms['REMOVAL_DA'] > pd.Timestamp('2019-01-01'))]\n",
    "\n",
    "platforms = platforms[(platforms['INSTALL_DA'] < pd.Timestamp('2022-04-22'))]\n",
    "\n",
    "LA_AR= gpd.read_file(\"~/RecFishing/Travel Costs with Dedicated Table/Shapefiles/Artificial_Reef_LA_2021_centroid.shx\").to_crs(epsg=4326)\n",
    "LA_AR['lng'] = LA_AR['geometry'].x\n",
    "LA_AR['lat'] = LA_AR['geometry'].y\n",
    "LA_AR['Deployment'] = pd.to_datetime(LA_AR['Deployment'], errors='coerce', format='%Y-%m-%d')\n",
    "LA_AR = LA_AR[(LA_AR['Deployment'] < pd.Timestamp('2022-04-22'))]\n",
    "LA_AR.Deployment.dtype\n",
    "\n",
    "TX_AR=gpd.read_file(\"~/RecFishing/Travel Costs with Dedicated Table/Shapefiles/Artificial_Reef_TX_2021_2.shx\").to_crs(epsg=4326)\n",
    "TX_AR['lng'] = TX_AR['geometry'].x\n",
    "TX_AR['lat'] = TX_AR['geometry'].y\n",
    "\n",
    "AL_AR=gpd.read_file(\"~/RecFishing/Travel Costs with Dedicated Table/Shapefiles/AL_AR_2024.shx\").to_crs(epsg=4326)\n",
    "AL_AR['lng'] = AL_AR['geometry'].x\n",
    "AL_AR['lat'] = AL_AR['geometry'].y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33de9dc-9398-4d86-accb-2b6ff1ad73d2",
   "metadata": {},
   "source": [
    "### Function to check if a point is within one of the industrial polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca50f1-11bf-4d19-87dc-975f8ec6d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "\n",
    "####################################################################\n",
    "##  LOAD INDUSTRIAL POLYGONS\n",
    "# Load polygons from WKT file\n",
    "def load_polygons_from_wkt(file_path):\n",
    "    polygons = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            polygon = wkt.loads(line.strip())\n",
    "            polygons.append(polygon)\n",
    "    return polygons\n",
    "\n",
    "# Load polygons\n",
    "Industrial_polygons = load_polygons_from_wkt(Industrial_polygons_filename)\n",
    "\n",
    "\n",
    "# ####################################################################\n",
    "# # Check if a point is within any of the polygons\n",
    "# def is_point_in_polygons(lat, lng, polygons):\n",
    "#     point = Point(lng, lat)  # Note that Point takes (lng, lat)\n",
    "#     for polygon in polygons:\n",
    "#         if polygon.contains(point):\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# # Path to the WKT file\n",
    "\n",
    "\n",
    "# Check if a point is within any of the polygons\n",
    "def is_point_in_polygons(lat, lng, polygons):\n",
    "    point = Point(lng, lat)  # Note that Point takes (lng, lat)\n",
    "    for polygon in polygons:\n",
    "        if polygon.contains(point):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Check if any point in the DataFrame is within any of the polygons\n",
    "def any_point_in_polygons(df, polygons):\n",
    "    for index, row in df.iterrows():\n",
    "        if is_point_in_polygons(row['lat'], row['lng'], polygons):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "####################################################################\n",
    "# EXAMPLE USAGE FOR A SINGLE POINT\n",
    "latitude = 31.0700\n",
    "# latitude = 30.0700\n",
    "longitude = -93.7300\n",
    "is_within = is_point_in_polygons(latitude, longitude, Industrial_polygons)\n",
    "\n",
    "if is_within:\n",
    "    print(f\"The point ({latitude}, {longitude}) is within one of the polygons\")\n",
    "else:\n",
    "    print(f\"The point ({latitude}, {longitude}) is NOT within one of the polygons\")\n",
    "\n",
    "####################################################################\n",
    "# EXAMPLE USAGE FOR A DATA FRAME\n",
    "# Sample DataFrame with lat and lng columns\n",
    "data = {'lat': [30.075, 29.765, 30.070], 'lng': [-93.728, -93.882, -93.730]}\n",
    "test_df = pd.DataFrame(data)\n",
    "\n",
    "# Check if any point in the DataFrame is within any of the polygons\n",
    "result = any_point_in_polygons(test_df, Industrial_polygons)\n",
    "\n",
    "print(\"The result of the data frame test is \", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3b337-cceb-4c54-bf0a-ac2f8f6cd23d",
   "metadata": {},
   "source": [
    "# Analysis of potential recreational fishing trips after the ML classificiation\n",
    "These steps were taken to analyze the trip. They are retained here to give the reader a sense of the tools that were used to develop the non-ML criteria for trip identification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3391460-914e-4bf4-becd-dc18e5f4b009",
   "metadata": {},
   "source": [
    "## Functions for creating a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6a800-c107-400e-93be-a203ef8c0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SelectATrip(cuebiq_id_i, Trip_number_i):\n",
    "    global random_trip_df\n",
    "    # cuebiq_id_i = input(\"cuebiq_id\")\n",
    "    # cuebiq_id_i = int(input(\"cuebiq_id: \"))\n",
    "\n",
    "    # Trip_number_i = input(\"Trip_number\").astype(int)\n",
    "    # Trip_number_i = int(input(\"Trip_number_i: \"))\n",
    "    Rec_Indicators_partial_df = Rec_Indicators_df[(Rec_Indicators_df['cuebiq_id']==cuebiq_id_i) & (Rec_Indicators_df['Trip_number'] == Trip_number_i)]\n",
    "    if len(Rec_Indicators_partial_df) >0:\n",
    "        random_trip_df = Rec_Indicators_partial_df.sample(n=1)\n",
    "    else:\n",
    "        random_trip_df = Rec_Indicators_partial_df\n",
    "                                                                                                   \n",
    "    return random_trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e96163-cb64-477b-b980-37e9d4890f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawATrip(Rec_Indicators_df):\n",
    "    global random_trip_df\n",
    "\n",
    "    # initialize Rec_Indicators_partial_df in case there are no exclusions\n",
    "    Rec_Indicators_partial_df=Rec_Indicators_df\n",
    "    \n",
    "    # Select a random trip from the remaining trips\n",
    "    random_trip_df = Rec_Indicators_partial_df.sample(n=1)\n",
    "    text = str(random_trip_df['cuebiq_id'].iloc[0])\n",
    "    text2 = str(random_trip_df['pct_during_outside_gulf'])\n",
    "    print(text, random_trip_df['Trip_number'].iloc[0], text2)\n",
    "    \n",
    "\n",
    "    return random_trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd91de5-5d64-4e7e-af62-95ffb83a357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function to create a kernel density plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_kde_plot(df, column):\n",
    "    \"\"\"\n",
    "    Creates a kernel density estimate plot for a specified column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the data.\n",
    "    column (str): The name of the column to plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check if the column exists in the DataFrame\n",
    "    if column not in df.columns:\n",
    "        print(f\"Column '{column}' does not exist in the DataFrame.\")\n",
    "        return\n",
    "    \n",
    "    # Create the kernel density plot\n",
    "    sns.kdeplot(data=df, x=column, fill=True)\n",
    "    \n",
    "    # Set the title and labels\n",
    "    plt.title(f'Kernel Density Estimate of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Density')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'DisappearanceAnalysis_df' is your DataFrame and you want to plot 'crit1'\n",
    "# create_kde_plot(DisappearanceAnalysis_df, 'crit1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af9f6b-7b9c-4156-a6f5-adaee25efb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function that creates a cumulative CDF\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_cdf(data, column, xlim_max=None, xlim_min=None):\n",
    "    \"\"\"\n",
    "    Plots the cumulative density function (CDF) for a given column in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The data frame containing the data.\n",
    "    column (str): The column for which the CDF is to be plotted.\n",
    "    xlim_min (float, optional): The lower limit of the horizontal axis.\n",
    "    xlim_max (float, optional): The upper limit of the horizontal axis.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Sort the data\n",
    "    sorted_data = np.sort(data[column].dropna())\n",
    "    # Calculate the CDF\n",
    "    cdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(sorted_data, cdf, marker='.', linestyle='none')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('CDF')\n",
    "    plt.title('Cumulative Density Function')\n",
    "    \n",
    "    # Set x-axis limit if provided\n",
    "    if xlim_min is not None and xlim_max is not None:\n",
    "        plt.xlim(xlim_min, xlim_max)\n",
    "    elif xlim_min is not None:\n",
    "        plt.xlim(left=xlim_min)\n",
    "    elif xlim_max is not None:\n",
    "        plt.xlim(right=xlim_max)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c37483f-5e4b-47fe-aea3-394be15a923d",
   "metadata": {},
   "source": [
    "### Functions that applies initial non-ML filtering criteria to the Indicators_df\n",
    "This involves criteria that are based entirely on variables using pings in the Gulf polygon and are not excluded because they are within polygons around islands.   Additional criteria are applied below to trips that pass these criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75497c65-b4a2-4e01-9dcf-185b8c7cf199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NonMLTripFiltering(Indicators_df):\n",
    "    warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "    print(len(Indicators_df), \"before processing\")\n",
    "    # retain only trips with pings both before and after\n",
    "\n",
    "    ####  This code has been cleaned up. In the original, there were errors that were removed using a function \n",
    "    ####   cleanup_repeated_column_names()\n",
    "    ####   It is possible that some errors remain. If so, it would be necessary to \n",
    "    \n",
    "\n",
    "    ############################## One-Way Trips ####################################\n",
    "    # Exlude trips that appear to be one way or at least mostly moving across destination, trips where the furthest distance from the origin is\n",
    "    #    at least twice the final from the origin\n",
    "    Indicators_df['final_over_max'] = Indicators_df['Distance_from_origin_t']/Rec_Indicators_df['Max_distance_traveled_origin_t']\n",
    "    Indicators_df1 = Indicators_df[Indicators_df['final_over_max'] < 0.5]\n",
    "    print(len(Indicators_df1), \"after excluding trips with final distance from origin/ max distance from origin > 0.5\")\n",
    "    # create_kde_plot(Indicators_df1,'Trip_Duration_t')\n",
    "    ############################## Exclude trips that are too long in duration ####################################\n",
    "    # Exclude trips that are more than 72 hours in length\n",
    "    max_hours = 72\n",
    "    Indicators_df2 = Indicators_df1[Indicators_df1['Trip_Duration_t'] <= max_hours*60]\n",
    "    print(f\"{len(Indicators_df2)} after limiting to trips less than {max_hours} hours\")\n",
    "    # create_kde_plot(Indicators_df2,'Trip_Duration_t')\n",
    "\n",
    "    ############################## Start too far from coast ####################################\n",
    "    Indicators_df2['Begin_End_Dist_from_Coast_max'] = Indicators_df2[\n",
    "        ['first_distance_from_coast_t', 'last_distance_from_coast_t']].max(axis=1)\n",
    "    Indicators_df2['Begin_End_Dist_from_Coast_min'] = Indicators_df2[\n",
    "        ['first_distance_from_coast_t', 'last_distance_from_coast_t']].min(axis=1)\n",
    "    min_distance = 5\n",
    "    Indicators_df3 = Indicators_df2[Indicators_df2['Begin_End_Dist_from_Coast_max']<min_distance]\n",
    "    print(len(Indicators_df3), \"after limiting to trips where the start & end distances from coast are less than \", min_distance)\n",
    "    # create_kde_plot(Indicators_df3,'Trip_Duration_t')\n",
    "\n",
    "    ############################## Exclude trips that are too short in duration ####################################\n",
    "    min_hours = 2\n",
    "    Indicators_df4=Indicators_df3[(Indicators_df3['Trip_Duration_t'] > min_hours*60)]\n",
    "    print(f\"{len(Indicators_df4)} after limiting to trips that are at least {min_hours} hours in length\")\n",
    "    # create_kde_plot(Indicators_df4,'Trip_Duration_t')\n",
    "\n",
    "    ############################## Exclude Trips that are more than 9 hous must go at least 50 km from the origin ####################################\n",
    "    Indicators_df4['Long_Near'] = 1*(Indicators_df4['Trip_Duration_t'] > 9*60)*(Indicators_df4['Max_distance_traveled_origin_t'] < 50)\n",
    "    Indicators_df5 = Indicators_df4[Indicators_df4['Long_Near'] == 0]\n",
    "    Indicators_df5.drop(columns=['Long_Near'], inplace=True)\n",
    "    print(len(Indicators_df5), \"after dropping trips more than 9 hours and didn't go at least 50 km from the origin\")\n",
    "    # create_kde_plot(Indicators_df5,'Trip_Duration_t')\n",
    "\n",
    "    ############################## Exclude trips do not travel far enough from the origin ####################################\n",
    "    min_dist = 0.5\n",
    "    Indicators_df6 = Indicators_df5[Indicators_df5['Max_distance_traveled_origin_t'] > min_dist]\n",
    "    print(f\"{len(Indicators_df6)} after dropping trips that didn't go at least {min_dist} km from the origin\")\n",
    "    # create_kde_plot(Indicators_df6,'Trip_Duration_t')\n",
    "    \n",
    "                  \n",
    "    return Indicators_df6\n",
    "    # return Indicators_df5  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8889617-fa30-45c1-afbf-d6a31842d47e",
   "metadata": {},
   "source": [
    "# Main Cells to Apply Criteria to select recreational fishing trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1e044-b129-4c9c-842e-f6bd0240546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load indicators after ML & associated ping \n",
    "Indicators_with_RF_Predictions_df = pd.read_csv(Indicators_Classified_filename)\n",
    "AllPings_OurTable_df = pd.read_csv(Pings_OurTable_Gulf_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8dcf68-1b56-48c1-8a14-363848c31497",
   "metadata": {},
   "source": [
    "## First apply criteria to get a list of potential rec trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799cfd3-3218-4fd7-925b-8be5fc995346",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rec_Indicators_df = pd.read_csv(Rec_Indicators_Step1_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1266ad-a23d-4129-b343-391388fd1f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to only ML predictions of 371\n",
    "Indicators_with_RF_Predictions_df = pd.read_csv(Indicators_Classified_filename)\n",
    "\n",
    "print(len(Indicators_with_RF_Predictions_df), \"trips were categorized with the ML algorithm (including duplicates)\")\n",
    "\n",
    "# Clean up the file, dropping duplicate rows, keeping the one with the lowest value of the pct variables since those had some errors\n",
    "Indicators_with_RF_Predictions_df['tot_pct'] = Indicators_with_RF_Predictions_df['pct_time_stopped'] +Indicators_with_RF_Predictions_df['pct_time_trawling'] +Indicators_with_RF_Predictions_df['pct_time_moving'] \n",
    "Indicators_with_RF_Predictions_df = Indicators_with_RF_Predictions_df.sort_values(by=['cuebiq_id', 'Trip_number', 'tot_pct'])\n",
    "Indicators_with_RF_Predictions_df = Indicators_with_RF_Predictions_df.drop_duplicates(subset=['cuebiq_id', 'Trip_number'], keep='first')\n",
    "\n",
    "print(len(Indicators_with_RF_Predictions_df), \"trips were categorized with the ML algorithm\")\n",
    "Rec_Indicators_df = Indicators_with_RF_Predictions_df[Indicators_with_RF_Predictions_df['Predicted_Class']==371]\n",
    "n_rec_trips0 =len(Rec_Indicators_df)\n",
    "print(n_rec_trips0, \"trips identified as 371\")\n",
    "\n",
    "# Exclude trips that do not require V3 pings\n",
    "Rec_Indicators_df = NonMLTripFiltering(Rec_Indicators_df)\n",
    "\n",
    "Rec_Indicators_df.to_csv(Rec_Indicators_Step1_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b2a1f-3a0b-4da2-b077-786757a70e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Rec_Indicators_Step1_filename)\n",
    "df['Trip_duration_hrs'] =(df['timestamp_end_t']- df['timestamp_start_t'])/(60*60)\n",
    "plot_cdf(df, 'Trip_duration_hrs', 10, xlim_min=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e205f0d-77e7-4892-afea-31c3f5e498f4",
   "metadata": {},
   "source": [
    "### Add any new trips to ID_For_V3_Queries_df for new queries of the complete Cuebiq Device Table (V3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df65e4-a251-4959-909f-7d8db65f12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(ID_For_V3_Queries_filename):\n",
    "    ID_For_V3_Queries_df = pd.read_csv(ID_For_V3_Queries_filename)\n",
    "\n",
    "    # Perform a left anti-join to identify rows in Rec_Indicators_df not in ID_For_V3_Queries_df\n",
    "    merged_df = Rec_Indicators_df.merge(\n",
    "        ID_For_V3_Queries_df[['cuebiq_id', 'Trip_number']],\n",
    "        on=['cuebiq_id', 'Trip_number'],\n",
    "        how='left',\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "    # Select rows where the merge indicator is 'left_only', meaning they are only in Rec_Indicators_df\n",
    "    missing_rows = merged_df[merged_df['_merge'] == 'left_only']\n",
    "\n",
    "    # Drop the merge indicator column\n",
    "    missing_rows = missing_rows.drop(columns=['_merge'])\n",
    "\n",
    "    # Create new column: V3_pings_pulled\n",
    "    missing_rows['V3_pings_pulled'] = False\n",
    "\n",
    "    # Append missing_rows to ID_For_V3_Queries_df\n",
    "    columns_to_append = ['cuebiq_id', 'Trip_number', 'timestamp_start_t', 'timestamp_end_t', 'V3_pings_pulled']\n",
    "    ID_For_V3_Queries_df = pd.concat([ID_For_V3_Queries_df, missing_rows[columns_to_append]], ignore_index=True)\n",
    "    ID_For_V3_Queries_df = ID_For_V3_Queries_df[columns_to_append]\n",
    "    ID_For_V3_Queries_df.to_csv(ID_For_V3_Queries_filename, index=False)\n",
    "    # ID_For_V3_Queries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb4c4a2-3d37-4b00-8782-cfedf54563f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cell that pulls data from the Cuebiq Device table\n",
    "### First pull V3 pings in bunches\n",
    "This will not capture orphan trips that were missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf667a-18cb-44d3-b6c8-d1aed705aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### With found start date\n",
    "if os.path.exists(ID_For_V3_Queries_filename):\n",
    "    ID_For_V3_Queries_df = pd.read_csv(ID_For_V3_Queries_filename)\n",
    "    columns_to_retain = ['cuebiq_id', 'Trip_number', 'timestamp_start_t', 'timestamp_end_t', 'V3_pings_pulled']\n",
    "    ID_For_V3_Queries_df = ID_For_V3_Queries_df[columns_to_retain]\n",
    "\n",
    "else:  # Create an empty data frame for the ID_For_V3_Queries_df\n",
    "    Indicators_df= pd.read_csv(Rec_Indicators_Step1_filename)\n",
    "    ID_For_V3_Queries_df = Indicators_df[['cuebiq_id', 'Trip_number', 'timestamp_start_t', 'timestamp_end_t']].copy()\n",
    "    ID_For_V3_Queries_df['V3_pings_pulled'] = False\n",
    "\n",
    "Remaining_ID_For_V3_Queries_df=ID_For_V3_Queries_df[ID_For_V3_Queries_df['V3_pings_pulled'] == False]\n",
    "if len(Remaining_ID_For_V3_Queries_df)==0:\n",
    "    print(\"V3 pings for all trips have been found\")\n",
    "    sys.exit()\n",
    "          \n",
    "min_start_timestamp_processed = Remaining_ID_For_V3_Queries_df['timestamp_start_t'].min()\n",
    "# max_start_timestamp_processed = ID_For_V3_Queries_df[ID_For_V3_Queries_df['V3_pings_pulled'] == True]['timestamp_start_t'].max()\n",
    "\n",
    "one_day = 24*60*60\n",
    "# start_date= epoch_to_datetime(max_start_timestamp_processed-2*one_day)\n",
    "start_date= epoch_to_datetime(min_start_timestamp_processed-2*one_day)\n",
    "\n",
    "end_date = datetime(2022, 5, 1)\n",
    "eight_hours = 8*60*60\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Find end_date_epoch_time to use at end of queries\n",
    "end_date_string = end_date.strftime('%m/%d/%Y') + ' 0:00'\n",
    "end_date_epoch_time = date_to_epoch(end_date_string, date_format='%m/%d/%Y %H:%M')\n",
    "\n",
    "current_date = start_date\n",
    "istep = 1\n",
    "while current_date <= end_date:\n",
    "# while current_date <= start_date:\n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=istep)\n",
    "\n",
    "    start_date_string = current_date.strftime('%m/%d/%Y') + ' 0:00'\n",
    "    start_day_epoch_time = date_to_epoch(start_date_string, date_format='%m/%d/%Y %H:%M')\n",
    "\n",
    "    # ID_For_V3_Queries_df = pd.read_csv(ID_For_V3_Queries_filename)\n",
    "    end_day_epoch_time = start_day_epoch_time +16*60*60\n",
    "\n",
    "    ########################################################################################################################################################\n",
    "    # DURING DEBUGGING A LOT OF VARIATIONS WERE TRIED TO FIGURE OUT HOW TO STRATEGICALLY PULL A LIMIT SET OF DATA\n",
    "    # The final options that worked were\n",
    "    # nIDs_per_pull=1000\n",
    "    # n_max_days = 30\n",
    "    # n_max_day_x_ids = 8000\n",
    "    ########################################################################################################################################################\n",
    "    # nIDs_per_pull=100 took 61.3 minutes for a single pull\n",
    "    # nIDs_per_pull=500 crashed. It appears that the data frame was too large. Rather than reducing this number, I reduced the maximum number of days that can be included\n",
    "    # nIDs_per_pull=500, n_max_days = 30: 39.8 min to process 64 trips\n",
    "    # nIDs_per_pull=500, n_max_days = 30: 46.5 min to process 188 trips\n",
    "    # nIDs_per_pull=500, n_max_days = 30: 56.9 min to process 431 trips\n",
    "    # nIDs_per_pull=500, n_max_days = 30: 37.3 min to process 652 trips\n",
    "    # nIDs_per_pull=1000, n_max_days = 30:35.2 min to process 1151 trips\n",
    "    # nIDs_per_pull=2000, n_max_days = 30: CRASHED too much\n",
    "    # nIDs_per_pull=2000, n_max_days = 30:limit of 20,000 for IDs*days CRASHED \n",
    "    # nIDs_per_pull= 1000 , n_max_days = 30 ID count: 848 Trip Count: 939\n",
    "    # nIDs_per_pull=2000, n_max_days = 30:limit of 15,000 for IDs*days CRASHED \n",
    "    # nIDs_per_pull= 1000 n_max_days = 30, n_max_day_x_ids = 10000 22.2 min to process 780 trips\n",
    "    # nIDs_per_pull: 1000 n_max_days: 30 n_max_day_x_ids: 10000 39.0 min to process 803 trips\n",
    "    # nIDs_per_pull: 1000 n_max_days: 30 n_max_day_x_ids: 10000 trips: 968 days x ids: 10440  CRASHED\n",
    "    ########################################################################################################################################################\n",
    "  \n",
    "    nIDs_per_pull=1000\n",
    "    n_max_days =  30\n",
    "    n_max_day_x_ids = 8000\n",
    "    nIDs = 0\n",
    "    istep = 0\n",
    "    start_time = time.time()\n",
    "    # Loop over time until nIDs_per_pull is just passed\n",
    "    istep = 1\n",
    "    while nIDs < nIDs_per_pull:\n",
    "        istep = istep+1\n",
    "        end_day_epoch_time = start_day_epoch_time +istep*24*60*60\n",
    "        selected_trips_df = Remaining_ID_For_V3_Queries_df[(Remaining_ID_For_V3_Queries_df['timestamp_start_t'] >= start_day_epoch_time) & \n",
    "                                                 (Remaining_ID_For_V3_Queries_df['timestamp_end_t'] <= end_day_epoch_time)]\n",
    "        \n",
    "\n",
    "        unique_cuebiq_ids = selected_trips_df['cuebiq_id'].unique()\n",
    "        nIDs = len(unique_cuebiq_ids)\n",
    "        if end_day_epoch_time>end_date_epoch_time:\n",
    "            nIDs = nIDs_per_pull+1\n",
    "        if istep>n_max_days:  # terminate if there are more than n_max_days days between start and en\n",
    "            nIDs = nIDs_per_pull+1\n",
    "        if istep*(len(unique_cuebiq_ids))>n_max_day_x_ids:  # terminate if ids* days > n_max_day_x_ids\n",
    "            nIDs = nIDs_per_pull+1\n",
    "\n",
    "    if len(selected_trips_df) == 0:\n",
    "        print(\"No trips were found in the \", istep, \" days between \" , start_day_epoch_time, \" and \", end_day_epoch_time)\n",
    "    if len(selected_trips_df) > 0:\n",
    "        print(len(selected_trips_df), \" trips were found in the \", istep, \" days between \" , start_day_epoch_time, \" and \", end_day_epoch_time)\n",
    "        # Create the days for use in the processing date query\n",
    "        start_date = current_date - timedelta(days=1)\n",
    "        end_date_q = current_date + timedelta(days=istep+2)\n",
    "\n",
    "        # Narrow the start and end dates based on the trips that were selected\n",
    "        start_time_epoch_time_selected = selected_trips_df['timestamp_start_t'].min()\n",
    "        end_time_epoch_time_selected = selected_trips_df['timestamp_end_t'].max()\n",
    "\n",
    "        start_date_query= epoch_to_datetime(start_time_epoch_time_selected-2*one_day)\n",
    "        end_date_query= epoch_to_datetime(end_time_epoch_time_selected+2*one_day)\n",
    "        \n",
    "        \n",
    "        # dayi_Q = start_date.strftime('%Y%m%d')\n",
    "        # end_window_Q = end_date_q.strftime('%Y%m%d')\n",
    "        dayi_Q = start_date_query.strftime('%Y%m%d')\n",
    "        end_window_Q = end_date_query.strftime('%Y%m%d')\n",
    "\n",
    "        # Convert id list to a tuple that can be used in a SQL query\n",
    "        cuebiq_id_df = pd.DataFrame({'cuebiq_id': unique_cuebiq_ids})\n",
    "        cuebiq_id_list = cuebiq_id_df['cuebiq_id'].tolist()\n",
    "        if len(cuebiq_id_list) == 1:\n",
    "            cuebiq_id_tuple = f\"({cuebiq_id_list[0]})\"  # Single element, still needs parentheses\n",
    "        else:\n",
    "            cuebiq_id_tuple = str(tuple(cuebiq_id_list))  # Multiple elements, convert list to tuple string\n",
    "        # cuebiq_id_tuple = ','.join(map(str, cuebiq_id_list))\n",
    "        # cuebiq_id_tuple = tuple(cuebiq_id_list)\n",
    "\n",
    "        current_time = datetime.now(local_timezone)\n",
    "        formatted_time = current_time.strftime(\"%H:%M:%S\")\n",
    "        # Write output for this query\n",
    "        text_to_write = (\n",
    "            f\"\\n\\nStarting at{formatted_time} \\nids: from {dayi_Q} to {end_window_Q}\\n\"\n",
    "            f\"# nIDs_per_pull: {nIDs_per_pull}  n_max_days: {n_max_days} n_max_day_x_ids: {n_max_day_x_ids}\\n\"\n",
    "            f\"# IDs_per_pull: {len(unique_cuebiq_ids)}         days: {istep},      day_x_ids: {istep * len(unique_cuebiq_ids)}  ,   trips: {len(selected_trips_df)}\"\n",
    "        )\n",
    "\n",
    "        print(text_to_write)\n",
    "        with open(Log_filename, 'a') as file:\n",
    "            file.write(text_to_write)\n",
    "        ###########  Start Query for this set of ID's and dates\n",
    "        max_retries = 5\n",
    "        retry_count = 0\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                specific_cuebiq_id_query = f\"\"\"\n",
    "                    SELECT *\n",
    "                    FROM paas_cda_pe_v3.device_location_uplevelled\n",
    "                    WHERE cuebiq_id IN {cuebiq_id_tuple}\n",
    "                    AND country_code = 'US'\n",
    "                    AND processing_date BETWEEN {dayi_Q} AND {end_window_Q}\n",
    "                    \"\"\"\n",
    "\n",
    "                specific_cuebiq_id_data = sql_engine.read_sql(specific_cuebiq_id_query)\n",
    "                # AND event_timestamp BETWEEN {start_query_time} AND {end_query_time}\n",
    "                current_time = datetime.now(local_timezone)\n",
    "                formatted_time = current_time.strftime(\"%H:%M:%S\")\n",
    "                print(formatted_time, \"Finished Query\")\n",
    "                break\n",
    "\n",
    "            except Exception as e:                \n",
    "                # Handle the 502 Bad Gateway error\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                retry_count += 1\n",
    "                time.sleep(10)  # Adjust the delay time as needed\n",
    "\n",
    "\n",
    "        ############  Retain from the query results pings that are not relevant for the trips in this group   ########\n",
    "        npings = 0\n",
    "        for index, row in selected_trips_df.iterrows():\n",
    "            cuebiq_id = row['cuebiq_id']\n",
    "            Trip_number = row['Trip_number']\n",
    "            timestamp_start_t = row['timestamp_start_t']\n",
    "            timestamp_end_t= row['timestamp_end_t']\n",
    "\n",
    "            this_trip_pings =  specific_cuebiq_id_data[(specific_cuebiq_id_data['cuebiq_id'] == cuebiq_id) &\n",
    "                                                (specific_cuebiq_id_data['event_timestamp'] >= timestamp_start_t - eight_hours) &\n",
    "                                              (specific_cuebiq_id_data['event_timestamp'] <= timestamp_end_t + eight_hours)]\n",
    "\n",
    "            columns_to_keep = ['cuebiq_id', 'event_timestamp', 'lat', 'lng']\n",
    "            this_trip_pings = this_trip_pings[columns_to_keep]\n",
    "\n",
    "            this_trip_pings.to_csv(Pings_V3_Before_After_filename, mode='a', index=False, header=not os.path.exists(Pings_V3_Before_After_filename))\n",
    "\n",
    "            print(cuebiq_id, Trip_number, \"was processed.  DELETE THIS PRINT STATEMENT AFTER DEBUGGING\")\n",
    "            Remaining_ID_For_V3_Queries_df.loc[(Remaining_ID_For_V3_Queries_df['cuebiq_id'] == cuebiq_id) & (Remaining_ID_For_V3_Queries_df['Trip_number'] == Trip_number), 'V3_pings_pulled'] = True\n",
    "            ID_For_V3_Queries_df.loc[(ID_For_V3_Queries_df['cuebiq_id'] == cuebiq_id) & (ID_For_V3_Queries_df['Trip_number'] == Trip_number), 'V3_pings_pulled'] = True\n",
    "            npings = npings + len(this_trip_pings)\n",
    "\n",
    "        # After looping over all trips for this query, save the updated ID_For_V3_Queries_df\n",
    "        print(npings, \" pings were found in this query\")\n",
    "        ID_For_V3_Queries_df.to_csv(ID_For_V3_Queries_filename, index=False)\n",
    "\n",
    "\n",
    "        count_false_pings_pulled = (ID_For_V3_Queries_df['V3_pings_pulled'] == False).sum()\n",
    "        count_true_pings_pulled = (ID_For_V3_Queries_df['V3_pings_pulled'] == True).sum()\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = round((end_time - start_time)/60,1)\n",
    "        current_time = datetime.now(local_timezone)\n",
    "        formatted_time = current_time.strftime(\"%H:%M:%S\")\n",
    "        text_to_write = (\n",
    "            f\"\\nCompleted at {formatted_time},   {elapsed_time} minutes to process\\n\"\n",
    "            f\"# nIDs_per_pull: {nIDs_per_pull}  n_max_days: {n_max_days} n_max_day_x_ids: {n_max_day_x_ids}\\n\"\n",
    "            f\"#  IDs_per_pull: {len(unique_cuebiq_ids)}         days: {istep},      day_x_ids: {istep * len(unique_cuebiq_ids)}  , trips: {len(selected_trips_df)} pings: {npings}\\n\"\n",
    "            f\"Covered dates from {dayi_Q}, to, {end_window_Q}\\n{count_true_pings_pulled} trips have been processed. {count_false_pings_pulled} trips remain.\\n \\n\"\n",
    "            )\n",
    "    print(text_to_write)\n",
    "    with open(Log_filename, 'a') as file:\n",
    "        file.write(text_to_write)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c47ce4-f09a-4d42-af86-03dffe62f0d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Finish up the few remaining rows that were not pulled in the first approach\n",
    "Some rows were missed between iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e11bdae-cc1c-4c75-b3af-67cb0a36f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### With found start date\n",
    "if os.path.exists(ID_For_V3_Queries_filename):\n",
    "    ID_For_V3_Queries_df = pd.read_csv(ID_For_V3_Queries_filename)\n",
    "    columns_to_retain = ['cuebiq_id', 'Trip_number', 'timestamp_start_t', 'timestamp_end_t', 'V3_pings_pulled']\n",
    "    ID_For_V3_Queries_df = ID_For_V3_Queries_df[columns_to_retain]\n",
    "\n",
    "else:  # Create an empty data frame for the ID_For_V3_Queries_df\n",
    "    print(\"something is seriously wrong\")\n",
    "    sys.exit()\n",
    "    \n",
    "# filter to get the remaining trips to pull\n",
    "remaining_trips_df = ID_For_V3_Queries_df[ID_For_V3_Queries_df['V3_pings_pulled']==False]\n",
    "remaining_trips_df = remaining_trips_df.sort_values(by='timestamp_start_t').reset_index(drop=True)\n",
    "\n",
    "day_gap = 10\n",
    "remaining_trips_df['timestamp_start_t_dt'] = pd.to_datetime(remaining_trips_df['timestamp_start_t'], unit='s')\n",
    "remaining_trips_df['days_diff'] = remaining_trips_df['timestamp_start_t_dt'].diff().dt.days.fillna(0)\n",
    "remaining_trips_df['group_num'] = (remaining_trips_df['days_diff'] >= day_gap).cumsum() + 1\n",
    "# remaining_trips_df = remaining_trips_df.drop(columns=['timestamp_start_t_dt', 'days_diff'])\n",
    "\n",
    "\n",
    "for group_num, group_data in remaining_trips_df.groupby('group_num'):\n",
    "    start_time = time.time()\n",
    "    selected_trips_df = remaining_trips_df[remaining_trips_df['group_num'] == group_num]\n",
    "\n",
    "    if len(selected_trips_df) == 0:\n",
    "        sys.exit()\n",
    "    # Create the days for use in the processing date query\n",
    "    current_date = selected_trips_df['timestamp_start_t_dt'].min()\n",
    "    end_date = selected_trips_df['timestamp_start_t_dt'].max()\n",
    "    istep = (end_date - current_date) + timedelta(days=2)\n",
    "\n",
    "    start_date = current_date - timedelta(days=1)\n",
    "    end_date_q = current_date + istep + timedelta(days=2)\n",
    "    dayi_Q = start_date.strftime('%Y%m%d')\n",
    "    end_window_Q = end_date_q.strftime('%Y%m%d')\n",
    "\n",
    "    # Convert id list to a tuple that can be used in a SQL query\n",
    "    unique_cuebiq_ids = selected_trips_df['cuebiq_id'].unique()\n",
    "    cuebiq_id_df = pd.DataFrame({'cuebiq_id': unique_cuebiq_ids})\n",
    "    cuebiq_id_list = cuebiq_id_df['cuebiq_id'].tolist()\n",
    "\n",
    "    current_time = datetime.now(local_timezone)\n",
    "    formatted_time = current_time.strftime(\"%H:%M:%S\")\n",
    "    # Write output for this query\n",
    "    text_to_write = (\n",
    "        f\"\\n\\nStarting Group Number {group_num} at {formatted_time} \\nids: from {dayi_Q} to {end_window_Q}\\n\"\n",
    "        f\"# IDs_per_pull: {len(unique_cuebiq_ids)}         days: {istep},      day_x_ids: {istep * len(unique_cuebiq_ids)}  ,   trips: {len(selected_trips_df)}\"\n",
    "    )\n",
    "\n",
    "    print(text_to_write)\n",
    "    with open(Log_filename, 'a') as file:\n",
    "        file.write(text_to_write)\n",
    "    ###########  Start Query for this set of ID's and dates\n",
    "    max_retries = 5\n",
    "    retry_count = 0\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            if len(cuebiq_id_list) == 1:\n",
    "                cuebiq_id_value = cuebiq_id_list[0]\n",
    "\n",
    "                specific_cuebiq_id_query = f\"\"\"\n",
    "                    SELECT *\n",
    "                    FROM paas_cda_pe_v3.device_location_uplevelled\n",
    "                    WHERE cuebiq_id = {cuebiq_id_value}\n",
    "                    AND country_code = 'US'\n",
    "                    AND processing_date BETWEEN {dayi_Q} AND {end_window_Q}\n",
    "                    \"\"\"\n",
    "            else:\n",
    "                cuebiq_id_tuple = tuple(cuebiq_id_list)\n",
    "\n",
    "                specific_cuebiq_id_query = f\"\"\"\n",
    "                    SELECT *\n",
    "                    FROM paas_cda_pe_v3.device_location_uplevelled\n",
    "                    WHERE cuebiq_id IN {cuebiq_id_tuple}\n",
    "                    AND country_code = 'US'\n",
    "                    AND processing_date BETWEEN {dayi_Q} AND {end_window_Q}\n",
    "                    \"\"\"\n",
    "\n",
    "\n",
    "            specific_cuebiq_id_data = sql_engine.read_sql(specific_cuebiq_id_query)\n",
    "            # AND event_timestamp BETWEEN {start_query_time} AND {end_query_time}\n",
    "            current_time = datetime.now(local_timezone)\n",
    "            formatted_time = current_time.strftime(\"%H:%M:%S\")\n",
    "            print(formatted_time, \"Finished Query\")\n",
    "            break\n",
    "\n",
    "        except Exception as e:                \n",
    "            # Handle the 502 Bad Gateway error\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            retry_count += 1\n",
    "            time.sleep(10)  # Adjust the delay time as needed\n",
    "\n",
    "\n",
    "    ############  Retain from the query results pings that are not relevant for the trips in this group   ########\n",
    "    npings = 0\n",
    "    for index, row in selected_trips_df.iterrows():\n",
    "        cuebiq_id = row['cuebiq_id']\n",
    "        Trip_number = row['Trip_number']\n",
    "        timestamp_start_t = row['timestamp_start_t']\n",
    "        timestamp_end_t= row['timestamp_end_t']\n",
    "        \n",
    "        eight_hours = 8*60*60\n",
    "\n",
    "        this_trip_pings =  specific_cuebiq_id_data[(specific_cuebiq_id_data['cuebiq_id'] == cuebiq_id) &\n",
    "                                            (specific_cuebiq_id_data['event_timestamp'] >= timestamp_start_t - eight_hours) &\n",
    "                                          (specific_cuebiq_id_data['event_timestamp'] <= timestamp_end_t + eight_hours)]\n",
    "\n",
    "        columns_to_keep = ['cuebiq_id', 'event_timestamp', 'lat', 'lng']\n",
    "        this_trip_pings = this_trip_pings[columns_to_keep]\n",
    "\n",
    "        this_trip_pings.to_csv(Pings_V3_Before_After_filename, mode='a', index=False, header=not os.path.exists(Pings_V3_Before_After_filename))\n",
    "\n",
    "        ID_For_V3_Queries_df.loc[(ID_For_V3_Queries_df['cuebiq_id'] == cuebiq_id) & (ID_For_V3_Queries_df['Trip_number'] == Trip_number), 'V3_pings_pulled'] = True\n",
    "        npings = npings + len(this_trip_pings)\n",
    "\n",
    "    # After looping over all trips for this query, save the updated ID_For_V3_Queries_df retaining only the columns I want to avoid creation of many columns of the index\n",
    "    columns_to_retain = ['cuebiq_id', 'Trip_number', 'timestamp_start_t', 'timestamp_end_t', 'V3_pings_pulled']\n",
    "    ID_For_V3_Queries_df = ID_For_V3_Queries_df[columns_to_retain]\n",
    "    ID_For_V3_Queries_df.to_csv(ID_For_V3_Queries_filename, index=False)\n",
    "\n",
    "\n",
    "    count_false_pings_pulled = (ID_For_V3_Queries_df['V3_pings_pulled'] == False).sum()\n",
    "    count_true_pings_pulled = (ID_For_V3_Queries_df['V3_pings_pulled'] == True).sum()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = round((end_time - start_time)/60,1)\n",
    "    current_time = datetime.now(local_timezone)\n",
    "    formatted_time = current_time.strftime(\"%H:%M:%S\")\n",
    "    text_to_write = (\n",
    "        f\"\\nCompleted at {formatted_time},   {elapsed_time} minutes to process\\n\"\n",
    "        f\"#  IDs_per_pull: {len(unique_cuebiq_ids)}         days: {istep},      day_x_ids: {istep * len(unique_cuebiq_ids)}  , trips: {len(selected_trips_df)} pings: {npings}\\n\"\n",
    "        f\"Covered dates from {dayi_Q}, to, {end_window_Q}\\n{count_true_pings_pulled} trips have been processed. {count_false_pings_pulled} trips remain.\\n \\n\"\n",
    "        )\n",
    "    print(text_to_write)\n",
    "    with open(Log_filename, 'a') as file:\n",
    "        file.write(text_to_write)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845e7ea-c9f7-476c-8042-f4347d9ce142",
   "metadata": {},
   "source": [
    "# Apply additional non-ML criteria that use data from the Cuebiq Device table (V3) that are not within the Gulf polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8610a819-c099-4d83-9517-4fa9490a266e",
   "metadata": {},
   "source": [
    "### First load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3869f9-0259-47b8-940a-a4f669208992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rec_Indicators_df =pd.read_csv(Rec_Indicators_Step1_filename, index=False)\n",
    "\n",
    "# Load V3 pings\n",
    "if 'Pings_V3_Before_After_df' not in locals():\n",
    "    Pings_V3_Before_After_df =pd.read_csv(Pings_V3_Before_After_filename)\n",
    "\n",
    "# Load OurTable pings\n",
    "if 'AllPings_OurTable_df' not in locals():\n",
    "    AllPings_OurTable_df =pd.read_csv(Pings_OurTable_Gulf_filename)\n",
    "\n",
    "\n",
    "# Make the ping df's consistent\n",
    "ping_columns_to_keep = ['cuebiq_id', 'lat', 'lng', 'event_timestamp']\n",
    "AllPings_OurTable_df=AllPings_OurTable_df[ping_columns_to_keep]\n",
    "Pings_V3_Before_After_df=Pings_V3_Before_After_df[ping_columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee95911-65ca-4c3a-a13d-4fa5bfabd8d2",
   "metadata": {},
   "source": [
    "### Now loop through all the trips and calculate the ping-level indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5890de9-f92c-425a-9291-b6ca0869f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the indicators with the first set of non-ML indicators\n",
    "indicators_df =pd.read_csv(Rec_Indicators_Step1_filename)\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(V3_Indicators_filename):\n",
    "    # Load the existing file\n",
    "    V3_Indicators_df = pd.read_csv(V3_Indicators_filename)\n",
    "    existing_ids = set(zip(V3_Indicators_df['cuebiq_id'], V3_Indicators_df['Trip_number']))\n",
    "\n",
    "    # Filter out rows from Indicators_df that are in the set of existing_ids\n",
    "    remaining_indicators_df = indicators_df[\n",
    "        ~indicators_df.apply(lambda row: (row['cuebiq_id'], row['Trip_number']) in existing_ids, axis=1)\n",
    "    ]    \n",
    "    print(\"V3 indicators have already been created for \", len(V3_Indicators_df), \"trips.\")\n",
    "    print(\"There are \", len(indicators_df), \" valid trips for which indicators need to be created\")\n",
    "    print(\"There are \", len(remaining_indicators_df), \" trips for which V3 indicators need to be created.\")\n",
    "          \n",
    "else:\n",
    "    remaining_indicators_df = indicators_df\n",
    "    print(\"V3 indicators do not exist. Need to start from scratch\")\n",
    "\n",
    "# Keep only a few columns\n",
    "# indicator_columns = ['cuebiq_id', 'Trip_number', 'timestamp_start_t', 'timestamp_end_t']\n",
    "# remaining_indicators_df = remaining_indicators_df[indicator_columns]\n",
    "\n",
    "\n",
    "# Loop through all of the IDs in the remaining indicators and \n",
    "\n",
    "unique_cuebiq_ids = remaining_indicators_df['cuebiq_id'].unique()\n",
    "\n",
    "for cuebiq_id in tqdm(unique_cuebiq_ids):\n",
    "    V3_indicators_this_id_df = remaining_indicators_df[remaining_indicators_df['cuebiq_id'] == cuebiq_id]\n",
    "    V3_pings_this_id_df = Pings_V3_Before_After_df[Pings_V3_Before_After_df['cuebiq_id'] == cuebiq_id]\n",
    "    # Initialize the empty DataFrame with the specified columns\n",
    "    # trip_rows = pd.DataFrame(columns=indicator_columns)\n",
    "\n",
    "    for index, row in V3_indicators_this_id_df.iterrows():\n",
    "        cuebiq_id = row['cuebiq_id']\n",
    "        Trip_number = row['Trip_number']\n",
    "        timestamp_start_t = row['timestamp_start_t']\n",
    "        timestamp_end_t = row['timestamp_end_t']\n",
    "\n",
    "        eight_hours = 8*60*60\n",
    "        pings_from_v3_df = V3_pings_this_id_df[(V3_pings_this_id_df['event_timestamp'] >= timestamp_start_t-eight_hours) &\n",
    "                                                    (V3_pings_this_id_df['event_timestamp'] <= timestamp_end_t+eight_hours)]\n",
    "\n",
    "        pings_from_v3_df= EliminateErrantPingsSpeed(pings_from_v3_df, 90)\n",
    "        ############## Before  ##############\n",
    "        onetrip_before_pings = pings_from_v3_df[\n",
    "            (pings_from_v3_df['event_timestamp'] >= (timestamp_start_t - eight_hours)) &\n",
    "            (pings_from_v3_df['event_timestamp'] <= timestamp_start_t)\n",
    "            ]\n",
    "        npings_before = len(onetrip_before_pings)\n",
    "\n",
    "        #################### After  ##############\n",
    "        onetrip_after_pings = pings_from_v3_df[\n",
    "            (pings_from_v3_df['event_timestamp'] <= (timestamp_end_t + eight_hours)) &\n",
    "            (pings_from_v3_df['event_timestamp'] >= timestamp_end_t)\n",
    "            ]\n",
    "        npings_after = len(onetrip_after_pings)\n",
    "\n",
    "        ##### Check to see if any of the before & after pings were in one of the industrial polygons\n",
    "        before_after_pings  = pd.concat([onetrip_before_pings, onetrip_after_pings], axis=0)\n",
    "        industrial_TF = any_point_in_polygons(before_after_pings, Industrial_polygons)\n",
    "\n",
    "\n",
    "        #################### V3 During the Trip ##############\n",
    "        onetrip_V3_during_pings = pings_from_v3_df[\n",
    "            (pings_from_v3_df['event_timestamp'] > (timestamp_start_t)) &\n",
    "            (pings_from_v3_df['event_timestamp'] < (timestamp_end_t)) \n",
    "            ]\n",
    "\n",
    "        onetrip_OurTable_pings = AllPings_OurTable_df[\n",
    "            (AllPings_OurTable_df['cuebiq_id'] == cuebiq_id) &\n",
    "            (AllPings_OurTable_df['event_timestamp'] > (timestamp_start_t)) &\n",
    "            (AllPings_OurTable_df['event_timestamp'] < (timestamp_end_t)) \n",
    "            ]\n",
    "\n",
    "        onetrip_pings = pd.concat([onetrip_OurTable_pings, onetrip_V3_during_pings], axis=0)\n",
    "        onetrip_pings = EliminateErrantPingsSpeed(onetrip_pings, 60)\n",
    "\n",
    "        # Identify pings that are in the Gulf and NOT on the islands \n",
    "\n",
    "        onetrip_pings['is_in_Gulf_waters'] = onetrip_pings.apply(lambda row: is_point_in_Gulf_not_Islands(row), axis=1)\n",
    "        onetrip_pings['prev_is_in_Gulf_waters'] = onetrip_pings['is_in_Gulf_waters'].shift(1)\n",
    "        onetrip_pings['consecutive_outside'] = (onetrip_pings['is_in_Gulf_waters'] == False) & (onetrip_pings['prev_is_in_Gulf_waters'] == False)\n",
    "\n",
    "        npings_during_outside_gulf = onetrip_pings['consecutive_outside'].sum()  # Corrected line\n",
    "        minutes_during_outside_gulf = onetrip_pings.loc[onetrip_pings['consecutive_outside'], 'time_diff_minutes_from_previous'].sum()\n",
    "        km_during_outside_gulf = onetrip_pings.loc[onetrip_pings['consecutive_outside'], 'dist_fwd'].sum()\n",
    "        avg_mph_during_outside_gulf = (km_during_outside_gulf/(0.000001 + minutes_during_outside_gulf))*37.2823\n",
    "        trip_minutes = (timestamp_end_t- timestamp_start_t)/60\n",
    "        pct_during_outside_gulf = minutes_during_outside_gulf/trip_minutes\n",
    "\n",
    "        new_row = pd.DataFrame([{\n",
    "            'cuebiq_id': cuebiq_id,\n",
    "            'Trip_number': Trip_number,\n",
    "            'timestamp_start_t': timestamp_start_t,\n",
    "            'timestamp_end_t': timestamp_end_t,\n",
    "            'npings_before': npings_before,\n",
    "            'npings_after': npings_after,\n",
    "            'npings_during_outside_gulf': npings_during_outside_gulf,\n",
    "            'minutes_during_outside_gulf': minutes_during_outside_gulf,\n",
    "            'pct_during_outside_gulf': pct_during_outside_gulf,\n",
    "            'avg_mph_during_outside_gulf': avg_mph_during_outside_gulf,\n",
    "            'industrial_TF': industrial_TF\n",
    "            }])\n",
    "        new_row.to_csv(V3_Indicators_filename, mode='a', index=False, header=not os.path.exists(V3_Indicators_filename))\n",
    "        # trip_rows.append(new_row)        \n",
    "\n",
    "    # trip_rows.to_csv(V3_Indicators_filename, mode='a', index=False, header=not os.path.exists(V3_Indicators_filename))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bc4d8c-fd74-4173-a97f-3fe7c4a89f98",
   "metadata": {},
   "source": [
    "## Now, use the ping-level indicators to identify recreational trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973fda80-d463-4dff-8cc4-8dbaaf22e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ping_Indicator_filtering(Indicators_df):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # THE REMAINING CRITERIA MAKE USE OF V3 PING -- THESE NEED TO BE PULLED SEPARATELY\n",
    "    # Filter in  trips with  pings before and after\n",
    "    print(len(Indicators_df), \"before ping-level filtering. There were\",  (Indicators_df['Trip_duration_hrs'] > 10).sum(), \"trips more than 10 hours\")\n",
    "\n",
    "    Indicators_df1=Indicators_df[(Indicators_df['npings_before'] > 0) & (Indicators_df['npings_after']>0)]\n",
    "    print(len(Indicators_df1), \"after eliminating trips that don't have pings both before & after\")\n",
    "    \n",
    "    pct_threshold = 0.1\n",
    "    Indicators_df2 = Indicators_df1[Indicators_df1['pct_during_outside_gulf'] <pct_threshold ]\n",
    "    print(f\"{len(Indicators_df2)} after dropping trips more than {pct_threshold*100}% of time during trip but outside the Gulf WKT. This reults in many lost trips\")\n",
    "\n",
    "    \n",
    "    Indicators_df3 = Indicators_df2[Indicators_df2['industrial_TF']== False]\n",
    "    print(len(Indicators_df3), \"after dropping trips that went into one of the identified industrial polygons\")\n",
    "\n",
    "    # plot_cdf(Indicators_df, 'Trip_duration_hrs', 10)\n",
    "    # plot_cdf(Indicators_df1, 'Trip_duration_hrs', 10)\n",
    "    # plot_cdf(Indicators_df2, 'Trip_duration_hrs', 10)\n",
    "    # plot_cdf(Indicators_df3, 'Trip_duration_hrs', 10)\n",
    "    \n",
    "    return Indicators_df3\n",
    "    # return Indicators_df5  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1aede-d480-4283-9131-ae8270082ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the V3 & ping-level indicators and save\n",
    "indicators_df =pd.read_csv(Rec_Indicators_Step1_filename)\n",
    "V3_Indicators_df =pd.read_csv(V3_Indicators_filename)\n",
    "# Combine the original indicators_df with tne new ones\n",
    "# Merge the DataFrames with an outer join\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    indicators_df,\n",
    "    V3_Indicators_df,\n",
    "    on=['cuebiq_id', 'Trip_number', 'timestamp_start_t', 'timestamp_end_t'],\n",
    "    how='outer', # Use 'inner' if you want only the intersection\n",
    "    indicator=True  # This adds a column to indicate the source of each row\n",
    ")\n",
    "\n",
    "merge_counts = merged_df['_merge'].value_counts()\n",
    "\n",
    "# Check to make sure that V3 indicators have been found for all the valid trips in indicators_df\n",
    "print(f\"# Only indicators_df =: {merge_counts.get('left_only', 0)} , # only V3_Indicators_df = {merge_counts.get('right_only', 0)} , # both =  {merge_counts.get('both', 0)}\")\n",
    "\n",
    "if merge_counts.get('left_only', 0) > 0:\n",
    "    print(\"There are selected trip from indicators_df for which V3 indicators have not been found. This should be fixed before proceeding. If you want to override this, you this if statement must be commented out.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Filter to retain only rows that appear in indicators_df\n",
    "rec_indicators_with_V3_df = merged_df[merged_df['_merge'] == 'both'].copy()\n",
    "\n",
    "rec_indicators_with_V3_df['Trip_duration_hrs'] =(rec_indicators_with_V3_df['timestamp_end_t']- rec_indicators_with_V3_df['timestamp_start_t'])/(60*60)\n",
    "\n",
    "Rec_indicators_with_V3_df = Ping_Indicator_filtering(rec_indicators_with_V3_df)\n",
    "Rec_indicators_with_V3_df.to_csv(Rec_indicators_with_V3_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456adf98-95f2-435a-aa56-bbc5ce16c76f",
   "metadata": {},
   "source": [
    "# Create the indicators used for identifying fully-tracked trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1fea6f-5713-44f6-a096-0f518b88acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rec_indicators_with_V3_df =pd.read_csv(Rec_indicators_with_V3_filename)\n",
    "\n",
    "if os.path.exists(DisappearanceIndicators_filename):\n",
    "    # Load the existing file\n",
    "    DisappearanceIndicators_df = pd.read_csv(DisappearanceIndicators_filename)\n",
    "    print(\"DisappearanceIndicators have already been created for \", len(DisappearanceIndicators_df), \"trips.\")\n",
    "    existing_trips = set(zip(DisappearanceIndicators_df['cuebiq_id'], DisappearanceIndicators_df['Trip_number']))\n",
    "\n",
    "    # Filter out rows from Indicators_df that are in the set of existing_tris\n",
    "    remaining_indicators_df = Rec_indicators_with_V3_df[\n",
    "        ~Rec_indicators_with_V3_df.apply(lambda row: (row['cuebiq_id'], row['Trip_number']) in existing_trips, axis=1)\n",
    "    ]    \n",
    "    # Filter DisappearanceIndicators_df to remove rows that do not appear in Rec_indicators_with_V3_df\n",
    "    new_trips = set(zip(Rec_indicators_with_V3_df['cuebiq_id'], Rec_indicators_with_V3_df['Trip_number']))\n",
    "    DisappearanceIndicators_df = DisappearanceIndicators_df[\n",
    "        DisappearanceIndicators_df.apply(lambda row: (row['cuebiq_id'], row['Trip_number']) in new_trips, axis=1)\n",
    "    ]\n",
    "else:\n",
    "    remaining_indicators_df = Rec_indicators_with_V3_df\n",
    "\n",
    "print(\"DisappearanceIndicators for \", len(DisappearanceIndicators_df), \"trips are retained\")\n",
    "print(\"DisappearanceIndicators for \", len(remaining_indicators_df), \" more trips need to be created\")\n",
    "\n",
    "# Re-save DisappearanceIndicators_df, replacing the original one reflecting any cleaning that has occurred\n",
    "DisappearanceIndicators_df.to_csv(DisappearanceIndicators_filename, index=False)\n",
    "    \n",
    "# ##############################  DEBUGGING #########################################\n",
    "# row_num = 2200\n",
    "# rec_indicators_with_V3_df.reset_index(drop=True, inplace=True)\n",
    "# cuebiq_id = rec_indicators_with_V3_df['cuebiq_id'].iloc[row_num]\n",
    "# Trip_number = rec_indicators_with_V3_df['Trip_number'].iloc[row_num]\n",
    "# timestamp_start_t = rec_indicators_with_V3_df['timestamp_start_t'].iloc[row_num]\n",
    "# timestamp_end_t = rec_indicators_with_V3_df['timestamp_end_t'].iloc[row_num]\n",
    "# ############################  DEBUGGING #########################################\n",
    "\n",
    "\n",
    "for index, row in tqdm(remaining_indicators_df.iterrows(), total=remaining_indicators_df.shape[0], leave=True):\n",
    "    # Access row data using row['column_name']\n",
    "    cuebiq_id = row['cuebiq_id']\n",
    "    Trip_number = row['Trip_number']\n",
    "    timestamp_start_t = row['timestamp_start_t']\n",
    "    timestamp_end_t = row['timestamp_end_t']\n",
    "    ThisTrip_df = AllPings_OurTable_df[(AllPings_OurTable_df['cuebiq_id'] == cuebiq_id) &\n",
    "                                       (AllPings_OurTable_df['event_timestamp'] >= timestamp_start_t) &\n",
    "                                       (AllPings_OurTable_df['event_timestamp'] <= timestamp_end_t)]\n",
    "\n",
    "    ThisTrip_df = EliminateErrantPingsSpeed(ThisTrip_df, 60)\n",
    "    ThisTrip_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    Trip_duration_hrs = ThisTrip_df['time_diff_minutes_to_next'].sum()/60\n",
    "\n",
    "    # find iBefore and iAfter, the indices of the pings before & after the longest break in the trip\n",
    "    maxdiff_bkwd = ThisTrip_df['time_diff_minutes_to_next'].max()\n",
    "    ThisTrip_df['Max_bkwd_i'] = 1*(maxdiff_bkwd == ThisTrip_df['time_diff_minutes_to_next'])\n",
    "    iBefore = ThisTrip_df['time_diff_minutes_to_next'].idxmax()\n",
    "\n",
    "    maxdiff_fwd  = ThisTrip_df['time_diff_minutes_from_previous'].max()\n",
    "    iAfter = ThisTrip_df['time_diff_minutes_from_previous'].idxmax()\n",
    "    ThisTrip_df['Max_fwd_i'] = 1*(maxdiff_fwd == ThisTrip_df['time_diff_minutes_from_previous'])\n",
    "\n",
    "    max_hrs_gap = maxdiff_bkwd/60\n",
    "    dist_during_gap = ThisTrip_df['dist_bkwd'].iloc[iBefore]\n",
    "\n",
    "    # These lines were used to check the speeds before and after\n",
    "    # ThisTrip_df['cum_dist'] = ThisTrip_df['dist_fwd'].cumsum()\n",
    "    # ThisTrip_df['cum_time'] = ThisTrip_df['time_diff_minutes_from_previous'].cumsum()\n",
    "    # speedbefore1 = ((ThisTrip_df['cum_dist'].iloc[iBefore] - ThisTrip_df['cum_dist'].iloc[iBefore-1]) / \n",
    "    #                (ThisTrip_df['cum_time'].iloc[iBefore] - ThisTrip_df['cum_time'].iloc[iBefore-1]))*37.2823\n",
    "\n",
    "    # speedafter1 = ((ThisTrip_df['cum_dist'].iloc[iAfter+1] - ThisTrip_df['cum_dist'].iloc[iAfter]) / \n",
    "    #                (ThisTrip_df['cum_time'].iloc[iAfter+1] - ThisTrip_df['cum_time'].iloc[iAfter]))*37.2823\n",
    "\n",
    "    # speedduring1 = ((ThisTrip_df['cum_dist'].iloc[iAfter] - ThisTrip_df['cum_dist'].iloc[iBefore]) / \n",
    "    #                (ThisTrip_df['cum_time'].iloc[iAfter] - ThisTrip_df['cum_time'].iloc[iBefore]))*37.2823\n",
    "\n",
    "    # Calculate the speeds before, during and after the longest break in the series of pings\n",
    "    speedbefore = (ThisTrip_df['ping_speed_fwd'].iloc[iBefore])*37.2823\n",
    "    speedafter = (ThisTrip_df['ping_speed_bkwd'].iloc[iAfter])*37.2823\n",
    "    speedduring = (ThisTrip_df['ping_speed_fwd'].iloc[iAfter])*37.2823\n",
    "\n",
    "    ThisTrip_df['dist_from_origin'] = ThisTrip_df.apply(\n",
    "        lambda row: haversine(row['lat'], row['lng'], ThisTrip_df['lat'].iloc[0], ThisTrip_df['lng'].iloc[0]), axis=1\n",
    "    )\n",
    "    ThisTrip_df['dist_from_end'] = ThisTrip_df.apply(\n",
    "        lambda row: haversine(row['lat'], row['lng'], ThisTrip_df['lat'].iloc[-1], ThisTrip_df['lng'].iloc[-1]), axis=1\n",
    "    )\n",
    "\n",
    "    MaxDistFromOrigin = ThisTrip_df['dist_from_origin'].max()\n",
    "    MaxDistFromEnd = ThisTrip_df['dist_from_end'].max()\n",
    "\n",
    "    DistanceFromOriginBefore = ThisTrip_df['dist_from_origin'].loc[iBefore]\n",
    "    DistanceFromOriginAfter = ThisTrip_df['dist_from_origin'].loc[iAfter]\n",
    "\n",
    "    # DistanceFromEndBefore = haversine(ThisTrip_df['lat'].iloc[iBefore], ThisTrip_df['lng'].iloc[iBefore], ThisTrip_df['lat'].iloc[-1], ThisTrip_df['lng'].iloc[-1])\n",
    "    # DistanceFromEndAfter = haversine(ThisTrip_df['lat'].iloc[iAfter], ThisTrip_df['lng'].iloc[iAfter], ThisTrip_df['lat'].iloc[-1], ThisTrip_df['lng'].iloc[-1])\n",
    "    DistanceFromEndBefore = ThisTrip_df['dist_from_end'].loc[iBefore]\n",
    "    DistanceFromEndAfter = ThisTrip_df['dist_from_end'].loc[iAfter]\n",
    "\n",
    "    # DistanceFromCoastBefore = distance_to_coast_lat_lon(ThisTrip_df['lat'].iloc[iBefore], ThisTrip_df['lng'].iloc[iBefore])\n",
    "    # DistanceFromCoastAfter = distance_to_coast_lat_lon(ThisTrip_df['lat'].iloc[iBefore], ThisTrip_df['lng'].iloc[iAfter])\n",
    "\n",
    "    BeforeOverMaxEndOfTrip = DistanceFromEndBefore/MaxDistFromEnd\n",
    "    AfterOverMaxEndOfTrip = DistanceFromEndAfter/MaxDistFromEnd\n",
    "    BeforeOverMaxOrigin = DistanceFromOriginBefore/MaxDistFromOrigin\n",
    "    AfterOverMaxOrigin = DistanceFromOriginAfter/MaxDistFromOrigin\n",
    "\n",
    "    # Identify the rows in ThisTrip_df that meet the condition of being at least 1/2 km of max distance before & after gap\n",
    "    dist_adjustment = 0.5\n",
    "    max_distance_gap_pts = max(DistanceFromOriginBefore, DistanceFromOriginAfter) - dist_adjustment\n",
    "    condition_origin = ThisTrip_df['dist_from_origin'] > max_distance_gap_pts\n",
    "    condition_end = ThisTrip_df['dist_from_end'] > max_distance_gap_pts\n",
    "\n",
    "    # Exclude the rows before or after the gap\n",
    "    condition_origin.iloc[iBefore] = False\n",
    "    condition_origin.iloc[iAfter] = False\n",
    "    condition_end.iloc[iBefore] = False\n",
    "    condition_end.iloc[iAfter] = False\n",
    "\n",
    "    # Calcuate the number of pingswithin 0.5 km of the most distant point\n",
    "    pings_further_out_origin = condition_origin.sum()\n",
    "    pings_further_out_end = condition_end.sum()\n",
    "    min_further_out_origin = ThisTrip_df.loc[condition_origin, 'time_diff_minutes_from_previous'].sum()\n",
    "    min_further_out_end = ThisTrip_df.loc[condition_end, 'time_diff_minutes_from_previous'].sum()\n",
    "\n",
    "    data = {\n",
    "            'cuebiq_id': cuebiq_id,\n",
    "            'Trip_number': Trip_number,\n",
    "            'Trip_duration_hrs': Trip_duration_hrs,\n",
    "            'max_hrs_gap': max_hrs_gap, \n",
    "            'dist_during_gap': dist_during_gap,\n",
    "            'speedbefore': speedbefore,\n",
    "            'speedafter': speedafter,\n",
    "            'speedduring': speedduring,\n",
    "            'DistanceFromOriginBefore': DistanceFromOriginBefore,\n",
    "            'DistanceFromOriginAfter': DistanceFromOriginAfter,\n",
    "            'DistanceFromEndBefore': DistanceFromEndBefore,\n",
    "            'DistanceFromEndAfter': DistanceFromEndAfter,\n",
    "            'pings_further_out':pings_further_out_origin,\n",
    "            # 'pings_further_out_end':pings_further_out_end,\n",
    "            'min_further_out': min_further_out_origin,\n",
    "            # 'min_further_out_end':min_further_out_end,\n",
    "            'BeforeOverMaxEndOfTrip': BeforeOverMaxEndOfTrip, \n",
    "            'AfterOverMaxEndOfTrip': AfterOverMaxEndOfTrip, \n",
    "            'BeforeOverMaxOrigin': BeforeOverMaxOrigin, \n",
    "            'AfterOverMaxOrigin': AfterOverMaxOrigin\n",
    "            }\n",
    "    DisappearanceIndicators_df = pd.DataFrame(data, index=[0])\n",
    "    DisappearanceIndicators_df.to_csv(DisappearanceIndicators_filename, mode='a', header=not os.path.exists(DisappearanceIndicators_filename), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba4332-614d-45a4-8aba-33c797c92642",
   "metadata": {},
   "source": [
    "## Now apply non-ML criteria to exclude trips that are not fully tracked, \n",
    "then calculate a weight that can be used to infer representativeness of trips that do not disappear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb54314d-e063-41b5-92a8-0e116db7b6bc",
   "metadata": {},
   "source": [
    "### Custom Moving Average function that expands window if there are not enough observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6076366-463d-4956-9022-9e6d0f3ed635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION that calculates modified moving average ensuring at least min_rows are included in each average\n",
    "def custom_moving_average(df, column, time_column, initial_time_window, min_rows):\n",
    "    # Define the initial time window and minimum number of rows\n",
    "    avg_values = []\n",
    "    for i in range(len(df)):\n",
    "        current_time = df[time_column].iloc[i]\n",
    "        window_size = initial_time_window\n",
    "        while True:\n",
    "            mask = (df[time_column] >= (current_time - window_size)) & (df[time_column] <= (current_time + window_size))\n",
    "            selected_rows = df.loc[mask, column]\n",
    "            if len(selected_rows) >= min_rows or window_size > df[time_column].max() - df[time_column].min():\n",
    "                avg_values.append(selected_rows.mean())\n",
    "                break\n",
    "            window_size += 0.1  # Increment the window size\n",
    "    return avg_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7987e-d397-4af5-8344-c32c1c91a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DisappearanceAnalysis_df = pd.read_csv(DisappearanceIndicators_filename)\n",
    "\n",
    "if 'crit1' in DisappearanceAnalysis_df.columns:\n",
    "    recreate = input(\"The disappearance criterion have already been created. Do you want to start all over 'crit1' exists. Do you want to recreate the columns? (yes/no): \").strip().lower()\n",
    "    if recreate != 'yes':\n",
    "        print(\"Exiting without recreating the columns.\")\n",
    "        sys.exit(\"Exiting as per user's decision.\")\n",
    "\n",
    "# Criterion #1: Any gap < min_hrs_for_disappearance is an interruption\n",
    "min_hrs_for_disappearance = 0.5\n",
    "DisappearanceAnalysis_df['crit1'] = 1*(DisappearanceAnalysis_df['max_hrs_gap'] < min_hrs_for_disappearance)\n",
    "\n",
    "# Criterion #2: Check to see if there is time spent further away from both the origin and the end point\n",
    "ping_threshhold = 3\n",
    "minute_threshhold = 30\n",
    "DisappearanceAnalysis_df['crit2'] = 1*(DisappearanceAnalysis_df['pings_further_out']>=ping_threshhold)*(DisappearanceAnalysis_df['min_further_out']>=minute_threshhold/60)\n",
    "\n",
    "# Criterion #3: Check to see if the time spent at the end appears to be a stopped based on speed before and after the gap\n",
    "speed_threshhold3 = 5\n",
    "hr_threshhold3 = 1\n",
    "DisappearanceAnalysis_df['crit3'] = 1*(np.maximum(DisappearanceAnalysis_df['speedbefore'],DisappearanceAnalysis_df['speedafter'])<speed_threshhold3) \\\n",
    "                                    *(DisappearanceAnalysis_df['max_hrs_gap']<hr_threshhold3)\n",
    "\n",
    "# Criterion #4: Check to see if the time spent at the end appears to be a stopped based on speed before, after and during the gap \n",
    "speed_threshhold4 = 5\n",
    "hr_threshhold4 = 2\n",
    "DisappearanceAnalysis_df['crit4'] = 1*(np.maximum.reduce([\n",
    "                                        DisappearanceAnalysis_df['speedbefore'],\n",
    "                                        DisappearanceAnalysis_df['speedafter'],\n",
    "                                        DisappearanceAnalysis_df['speedduring']\n",
    "                                    ])<speed_threshhold4) \\\n",
    "                                            *(DisappearanceAnalysis_df['max_hrs_gap']<hr_threshhold4)\n",
    "\n",
    "# If any of the 4 interruption criteria are satisified -- classify as an interruption\n",
    "DisappearanceAnalysis_df['Interruption_01'] = np.maximum.reduce([\n",
    "    DisappearanceAnalysis_df['crit1'],\n",
    "    DisappearanceAnalysis_df['crit2'],\n",
    "    DisappearanceAnalysis_df['crit3'],\n",
    "    DisappearanceAnalysis_df['crit4']\n",
    "])\n",
    "\n",
    "########################################################################################################\n",
    "# Find the average % of trips of a given trip length that have interruptions. \n",
    "# The variable Avg_Interruption_01 should then be used as a weight in the travel cost model:\n",
    "#\n",
    "#         weight = 1/Avg_Interruption_01\n",
    "#\n",
    "########################################################################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Sort the DataFrame based on Trip_duration_hrs\n",
    "DisappearanceAnalysis_df = DisappearanceAnalysis_df.sort_values(by='Trip_duration_hrs').reset_index(drop=True)\n",
    "\n",
    "# initial_time_window = 1\n",
    "# min_rows = 100\n",
    "\n",
    "# Compute the custom moving average\n",
    "DisappearanceAnalysis_df['Avg_Interruption_01'] = custom_moving_average(DisappearanceAnalysis_df, 'Interruption_01', 'Trip_duration_hrs', initial_time_window, min_rows)\n",
    "DisappearanceAnalysis_df.to_csv(DisappearanceIndicators_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8e19f-7d02-4283-b770-8cbbe36ce147",
   "metadata": {},
   "outputs": [],
   "source": [
    "DisappearanceIndicators_df =pd.read_csv(DisappearanceIndicators_filename)\n",
    "indicators_df =pd.read_csv(Rec_indicators_with_V3_filename)\n",
    "\n",
    "\n",
    "print(len(DisappearanceIndicators_df), len(indicators_df))\n",
    "\n",
    "initial_time_window = 1\n",
    "min_rows = 100\n",
    "\n",
    "DisappearanceIndicators_df['Avg_Interruption_01'] = custom_moving_average(DisappearanceIndicators_df, 'Interruption_01', 'Trip_duration_hrs', initial_time_window, min_rows)\n",
    "DisappearanceIndicators_df.to_csv(DisappearanceIndicators_filename, index=False)\n",
    "\n",
    "print(\"From the total of \", len(DisappearanceAnalysis_df), \"identified recreational trips,\", DisappearanceAnalysis_df['Interruption_01'].sum(), \"trips do not disappear, so are valid for travel cost model\")\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    DisappearanceIndicators_df,\n",
    "    indicators_df,\n",
    "    on=['cuebiq_id', 'Trip_number'],\n",
    "    how='outer',  # Use 'inner' if you want only the intersection\n",
    "    suffixes=('_disappear', '_indicator')\n",
    ")\n",
    "\n",
    "# Drop duplicate columns if any were created during the merge\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "merged_df.to_csv(Rec_Indicators_Final_All_Exclusions_And_Disappearance_filename, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b780ef-7414-4d70-83fb-4dbc7dc38640",
   "metadata": {},
   "source": [
    "###  Graph the average interruption rate and the density of # of trips by duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f71d8-7431-4a6f-97b0-5fc7ea416258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Line plot for Avg_Interruption_01 vs. Trip_duration_hrs\n",
    "ax1.plot(DisappearanceAnalysis_df['Trip_duration_hrs'], DisappearanceAnalysis_df['Avg_Interruption_01'], linestyle='-', color='b')\n",
    "ax1.set_xlabel('Trip Duration (hrs)')\n",
    "ax1.set_ylabel('Average Interruption (01)', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "# Create a secondary y-axis for the KDE plot\n",
    "ax2 = ax1.twinx()\n",
    "sns.kdeplot(DisappearanceAnalysis_df['Trip_duration_hrs'], ax=ax2, color='r', linestyle='--')\n",
    "ax2.set_ylabel('Density of trips', color='r')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "# Add title\n",
    "GraphTitle = f'Average Interruption vs. Trip Duration:\\nTime Window = {initial_time_window} & Minimum Rows = {min_rows}' \n",
    "plt.title(GraphTitle)\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae30de-22e5-4041-9130-a9d7ca0d2ea8",
   "metadata": {},
   "source": [
    "# Add new variables that track all of the stops and trawls within a given trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1bf692-b612-4a7b-8e3f-bcf01c5bd990",
   "metadata": {},
   "outputs": [],
   "source": [
    "DisappearanceIndicators_df =pd.read_csv(DisappearanceIndicators_filename)\n",
    "# In initial coding, a trip was called \"interrupted\" if it was fully tracked, meaning the interruption was for a short time.\n",
    "CompleteRecTrips_df = DisappearanceIndicators_df[DisappearanceIndicators_df['Interruption_01'] == 1]\n",
    "print(\"len(CompleteRecTrips_df)\", len(CompleteRecTrips_df))\n",
    "stop_trawl_df=pd.read_csv(Stop_Trawls_Indicators_filename)\n",
    "print(\"len(stop_trawl_df)\", len(stop_trawl_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0649797-6272-42df-ab47-74d808290ddf",
   "metadata": {},
   "source": [
    "Remove rows from the stop and trawl files that were dropped in the duplicate cleaning process (8-22-2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52772e77-c6e0-44b8-952a-e314ae8a509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an up-to-date list of the complete trips\n",
    "complete_trips_set = set(zip(CompleteRecTrips_df['cuebiq_id'], CompleteRecTrips_df['Trip_number']))\n",
    "\n",
    "#######################################################\n",
    "# Filter stop_trawl_df \n",
    "# to keep only rows that are in the set of complete trips\n",
    "stop_trawl_df = pd.read_csv(Stop_Trawls_Indicators_filename)\n",
    "filtered_df = stop_trawl_df[\n",
    "    stop_trawl_df.apply(lambda row: (row['cuebiq_id'], row['Trip_number']) in complete_trips_set, axis=1)\n",
    "].copy()\n",
    "stop_trawl_df = filtered_df\n",
    "stop_trawl_df.to_csv(Stop_Trawls_Indicators_filename, index=False)\n",
    "\n",
    "#######################################################\n",
    "# Filter trawls_df \n",
    "# to keep only rows that are in the set of complete trips\n",
    "trawls_df= pd.read_csv(Trawls_Indicators_filename)\n",
    "filtered_df = trawls_df[\n",
    "    trawls_df.apply(lambda row: (row['cuebiq_id'], row['Trip_number']) in complete_trips_set, axis=1)\n",
    "].copy()\n",
    "trawls_df = filtered_df\n",
    "trawls_df.to_csv(Trawls_Indicators_filename, index=False)\n",
    "\n",
    "#######################################################\n",
    "# Filter stop_trawl_df \n",
    "# to keep only rows that are in the set of complete trips\n",
    "stops_df= pd.read_csv(Stops_Indicators_filename)\n",
    "filtered_df = stops_df[\n",
    "    stops_df.apply(lambda row: (row['cuebiq_id'], row['Trip_number']) in complete_trips_set, axis=1)\n",
    "].copy()\n",
    "stops_df = filtered_df\n",
    "stops_df.to_csv(Stops_Indicators_filename, index=False)\n",
    "\n",
    "print(len(CompleteRecTrips_df), len(stop_trawl_df), len(trawls_df), len(stops_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71473f6f-2ae3-4b30-978d-5ec8ea95524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OperationCancelled(Exception):\n",
    "    pass\n",
    "# Use only the trips that do not dissapear\n",
    "CompleteRecTrips_df = DisappearanceIndicators_df[DisappearanceIndicators_df['Interruption_01'] == 1]\n",
    "CompleteRecTrips_df = CompleteRecTrips_df.reset_index(drop=True)\n",
    "\n",
    "Debugging = True\n",
    "NotDebugging = not Debugging\n",
    "\n",
    "    # Focus on a single trip\n",
    "if Debugging:\n",
    "    CompleteRecTrips_df=CompleteRecTrips_df[(CompleteRecTrips_df['cuebiq_id']==1931836223) & (CompleteRecTrips_df['Trip_number']==1)]\n",
    "    # CompleteRecTrips_df=CompleteRecTrips_df.head(1)\n",
    "\n",
    "\n",
    "if NotDebugging:\n",
    "    # Check for the existing of the Stop_Trawls_Indicators already. If they already exist there are various options\n",
    "    if os.path.exists(Stop_Trawls_Indicators_filename):\n",
    "        stop_trawl_df = pd.read_csv(Stop_Trawls_Indicators_filename)\n",
    "        inputtext = (f\"There are {len(stop_trawl_df)} rows already processed and \"\n",
    "                     f\"{len(CompleteRecTrips_df)} trips that need to be processed. \\n\"\n",
    "                     \"Type    yes  to backup and start from scratch \\n\"\n",
    "                     \"Type    no   to terminate \\n\"\n",
    "                     f\"Type         anything else or enter to eliminate the {len(stop_trawl_df)} rows already processed and proceed.\")\n",
    "        confirm = input(inputtext)        \n",
    "        # Check the user's input\n",
    "        if confirm.lower() != 'no':\n",
    "            raise OperationCancelled('Exited because I did not want to recreate the stops and trawls indicators files.')\n",
    "        elif confirm.lower() != 'no':\n",
    "            backup_filename = Stop_Trawls_Indicators_filename + \".backup\"\n",
    "            os.rename(Stop_Trawls_Indicators_filename, backup_filename)\n",
    "            print(\"Stop_Trawls file has been backed up\")\n",
    "            if os.path.exists(Trawls_Indicators_filename):\n",
    "                backup_filename = Trawls_Indicators_filename + \".backup\"\n",
    "                os.rename(Trawls_Indicators_filename, backup_filename)\n",
    "                print(\"Trawls file has been backed up\")\n",
    "            if os.path.exists(Stops_Indicators_filename):\n",
    "                backup_filename = Stops_Indicators_filename + \".backup\"\n",
    "                os.rename(Stops_Indicators_filename, backup_filename)\n",
    "                print(\"Stops file has been backed up\")\n",
    "        else:\n",
    "            # Eliminate the rows from CompleteRecTrips_df that have already been processed\n",
    "            merged = pd.merge(CompleteRecTrips_df, stop_trawl_df, on=['cuebiq_id', 'Trip_number'], how='left', indicator=True)\n",
    "            CompleteRecTrips_df = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "            CompleteRecTrips_df.reset_index(drop=True, inplace=True)            \n",
    "    \n",
    "if 'AllPings_OurTable_df' not in locals():\n",
    "    print(\"loading AllPings_OurTable_df\")\n",
    "    AllPings_OurTable_df =pd.read_csv(Pings_OurTable_Gulf_filename)\n",
    "\n",
    "    \n",
    "##  LOOP OVER ALL COMPLETE TRIPS AND CREATE INDICATORS OF STOPS AND TRAWLS\n",
    "# for index, row in CompleteRecTrips_df.head().iterrows():\n",
    "# for index, row in CompleteRecTrips_df.iterrows():\n",
    "for index, row in tqdm(CompleteRecTrips_df.iterrows(), total=CompleteRecTrips_df.shape[0]):\n",
    "\n",
    "    cuebiq_id = row['cuebiq_id']\n",
    "    Trip_number = row['Trip_number']\n",
    "    ThisTripIndicators = Rec_Indicators_df[(Rec_Indicators_df['cuebiq_id'] == cuebiq_id) & (Rec_Indicators_df['Trip_number'] == Trip_number)]\n",
    "    timestamp_start_t = ThisTripIndicators['timestamp_start_t'].iloc[0]\n",
    "    timestamp_end_t = ThisTripIndicators['timestamp_end_t'].iloc[0]\n",
    "    Max_distance_traveled_origin_t = ThisTripIndicators['Max_distance_traveled_origin_t'].iloc[0]\n",
    "\n",
    "    \n",
    "    ThisTripPings_df = AllPings_OurTable_df[(AllPings_OurTable_df['cuebiq_id'] == cuebiq_id) & \n",
    "                                         (AllPings_OurTable_df['event_timestamp'] >= timestamp_start_t) &\n",
    "                                         (AllPings_OurTable_df['event_timestamp'] <= timestamp_end_t)]\n",
    "\n",
    "    ThisTripPings_df= EliminateErrantPingsSpeed(ThisTripPings_df, 60)\n",
    "    ThisTripPings_df['dist_fwd'].fillna(value=0, inplace=True)\n",
    "    ThisTripPings_df = ThisTripPings_df.reset_index(drop=True)\n",
    "\n",
    "    origin_lat = ThisTripPings_df['lat'].iloc[0]\n",
    "    origin_lng = ThisTripPings_df['lng'].iloc[0]\n",
    "\n",
    "    ThisTripPings_df = ThisTripPings_df.drop(columns=['classification_type', 'row_index'])\n",
    "\n",
    "    # cumulative time and distance\n",
    "    ThisTripPings_df['cumulative_time_minutes'] = ThisTripPings_df['time_diff_minutes_from_previous'].cumsum()\n",
    "    ThisTripPings_df['cumulative_distance'] = ThisTripPings_df['dist_fwd'].cumsum()\n",
    "\n",
    "    Trip_Duration_t = ThisTripPings_df.cumulative_time_minutes.max()\n",
    "\n",
    "    # Define the column names\n",
    "    stopcolumns = ['cuebiq_id', 'Trip_number', 'stop_num', 'stop_duration', \n",
    "                   'stop_avg_lat', 'stop_avg_lng', \n",
    "                   'stop_max_lat', 'stop_max_lng', \n",
    "                   'stop_avg_dist_from_origin','stop_max_dist_from_origin', 'Max_distance_traveled_origin_t']\n",
    "    stops_df = pd.DataFrame(columns=stopcolumns)\n",
    "    stops_row_df = pd.DataFrame(columns=stopcolumns)\n",
    "\n",
    "    trawlcolumns = ['cuebiq_id', 'Trip_number', \n",
    "                    'trawl_num', 'trawl_duration', 'cumulative_dist_trawl', \n",
    "                    'trawl_start_lat', 'trawl_start_lng', \n",
    "                    'trawl_end_lat', 'trawl_end_lng', \n",
    "                    'trawl_mid_lat', 'trawl_mid_lng', \n",
    "                    'trawl_max_lat', 'trawl_max_lng', \n",
    "                    'trawl_avg_speed', 'trawl_max_speed',\n",
    "                    'trawl_avg_dist_from_origin', 'trawl_max_dist_from_origin',\n",
    "                    'Max_distance_traveled_origin_t']\n",
    "    trawls_df = pd.DataFrame(columns=trawlcolumns)\n",
    "    trawl_row_df = pd.DataFrame(columns=trawlcolumns)\n",
    "\n",
    "    stop_trawl_columns = ['cuebiq_id', 'Trip_number', 'stop_num', 'trawl_num']\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------            \n",
    "    #Speed calculation using the liear interpoloation algorithm\n",
    "    if Trip_Duration_t > 60:\n",
    "\n",
    "        ################################################################\n",
    "        # Assuming ThisTripPings_df is already defined and has the columns 'cumulative_time_minutes' and 'cumulative_distance'\n",
    "\n",
    "        # Initialize Time_step and data arrays\n",
    "        Time_step = 5\n",
    "        x = ThisTripPings_df.cumulative_time_minutes.values\n",
    "        xx = np.append(-(Time_step + 1), x)\n",
    "        xx = np.append(xx, xx[-1] + (Time_step + 1))\n",
    "        y = ThisTripPings_df.cumulative_distance.values\n",
    "        yy = np.append(y[0], y)\n",
    "        yy = np.append(yy, yy[-1])\n",
    "\n",
    "        # Create the interpolation function\n",
    "        # f_totdist = interpolate.interp1d(xx, yy)\n",
    "        f_totdist = interpolate.interp1d(xx, yy, fill_value=\"extrapolate\")\n",
    "\n",
    "        # Define speed calculation function at a time  x\n",
    "        def speed_next_5min(x):   \n",
    "            return (f_totdist(x + Time_step) - f_totdist(x)) / Time_step\n",
    "\n",
    "        # Define speed calculation function at a time x\n",
    "        def speed_prev_5min(x):   \n",
    "            return (f_totdist(x) - f_totdist(x-Time_step)) / Time_step\n",
    "\n",
    "        # Initialize the column to store the speed\n",
    "        ThisTripPings_df['speed_next_5min'] = np.nan\n",
    "\n",
    "        ThisTripPings_df['speed_prev_5min']=speed_prev_5min(x)*37.2823\n",
    "        ThisTripPings_df['speed_next_5min']=speed_next_5min(x)*37.2823\n",
    "    \n",
    "        mph_stop = 1\n",
    "        mph_trawl = 5\n",
    "        stop_speed = mph_stop\n",
    "        trawl_speed = mph_trawl\n",
    "\n",
    "        # speed_next_5min - speed in the next 5 minutes\n",
    "        # speed_prev_5min - speed in the previous 5 minutes\n",
    "        ThisTripPings_df['StopAhead'] = 1*((ThisTripPings_df['speed_next_5min'] <= stop_speed))\n",
    "        ThisTripPings_df['StopAhead'].iloc[-1] = 0\n",
    "        ThisTripPings_df['StopBehind'] = 1*((ThisTripPings_df['speed_prev_5min'] <= stop_speed))\n",
    "        ThisTripPings_df['StopBehind'].iloc[0] = 0\n",
    "        ThisTripPings_df['InStop'] = ThisTripPings_df[['StopAhead', 'StopBehind']].max(axis=1)\n",
    "\n",
    "        # Trawling will be considered to be a period during which maximum speed was above stop speed and all consecutive pings are below the trawl_speed\n",
    "        ThisTripPings_df['TrawlAhead'] = 1*(ThisTripPings_df['speed_next_5min'] <= trawl_speed)\n",
    "        ThisTripPings_df['TrawlAhead'].iloc[-1] = 0\n",
    "        ThisTripPings_df['TrawlBehind'] = 1*(ThisTripPings_df['speed_prev_5min'] <= trawl_speed)\n",
    "        ThisTripPings_df['TrawlBehind'].iloc[0] = 0\n",
    "        ThisTripPings_df['InTrawl'] = ThisTripPings_df[['TrawlAhead', 'TrawlBehind']].max(axis=1)\n",
    "        ThisTripPings_df['dist_from_origin'] = ThisTripPings_df.apply(\n",
    "            lambda row: haversine(row['lat'], row['lng'], origin_lat, origin_lng), axis=1\n",
    "        )\n",
    "        ThisTripPings_df['ping_speed_fwd']=ThisTripPings_df['ping_speed_fwd']*37.2823\n",
    "        ThisTripPings_df['ping_speed_bkwd']=ThisTripPings_df['ping_speed_bkwd']*37.2823\n",
    "        # Iterate over the DataFrame\n",
    "        istop = 0\n",
    "        itrawl = 0\n",
    "\n",
    "        \n",
    "        npings_in_stop = 0\n",
    "        cumulative_time_stop = 0\n",
    "        cum_lat_stop = 0\n",
    "        cum_lng_stop = 0\n",
    "\n",
    "        npings_in_trawl = 0\n",
    "        cumulative_time_trawl = 0\n",
    "        cum_lat_trawl = 0\n",
    "        cum_lng_trawl = 0\n",
    "        \n",
    "        for index, row in ThisTripPings_df.iterrows():\n",
    "            ######################################################################################\n",
    "            # code for stop variables\n",
    "            if row['InStop'] == 1:\n",
    "                npings_in_stop = npings_in_stop+1\n",
    "                cumulative_time_stop += row['time_diff_minutes_from_previous']\n",
    "                cum_lat_stop += row['lat']\n",
    "                cum_lng_stop += row['lng']\n",
    "                if row['dist_from_origin']>stop_max_dist_from_origin:\n",
    "                    stop_max_dist_from_origin=ThisTripPings_df.loc[index, 'dist_from_origin']\n",
    "                    stop_max_lat = row['lat']\n",
    "                    stop_max_lng = row['lng']\n",
    "                    \n",
    "                if (index + 1 < len(ThisTripPings_df) and ThisTripPings_df.iloc[index + 1]['InStop'] == 0) | (index==len(ThisTripPings_df)):\n",
    "                    istop += 1\n",
    "                    stop_avg_lat = cum_lat_stop/npings_in_stop\n",
    "                    stop_avg_lng = cum_lng_stop/npings_in_stop\n",
    "\n",
    "                    stop_avg_dist_from_origin = haversine(stop_avg_lat, stop_avg_lng, origin_lat, origin_lng)\n",
    "                    # Create a one-row DataFrame with the calculated values\n",
    "                    stops_row_data = {'cuebiq_id': [cuebiq_id], 'Trip_number': Trip_number, \n",
    "                                      'stop_num': [istop],'stop_duration': [cumulative_time_stop],\n",
    "                                      'stop_avg_lat': [stop_avg_lat],'stop_avg_lng': [stop_avg_lng], \n",
    "                                      'stop_max_lat': [stop_avg_lat],'stop_max_lng': [stop_avg_lng], \n",
    "                                      'stop_avg_dist_from_origin': [stop_avg_dist_from_origin], 'stop_max_dist_from_origin': [stop_max_dist_from_origin],\n",
    "                                      'Max_distance_traveled_origin_t': [Max_distance_traveled_origin_t]\n",
    "                                     }\n",
    "                    stops_row_df = pd.DataFrame(stops_row_data)\n",
    "                    \n",
    "                    stops_df = pd.concat([stops_df, stops_row_df], ignore_index=True)\n",
    "                    \n",
    "            else:\n",
    "                npings_in_stop = 0\n",
    "                cumulative_time_stop = 0\n",
    "                cum_lat_stop = 0\n",
    "                cum_lng_stop = 0\n",
    "                stop_max_dist_from_origin=0\n",
    "\n",
    "            ######################################################################################\n",
    "            # code for Trawl variables\n",
    "            if row['InTrawl'] == 1:\n",
    "                npings_in_trawl = npings_in_trawl+1\n",
    "                cumulative_time_trawl += row['time_diff_minutes_from_previous']\n",
    "                cumulative_dist_trawl += row['dist_fwd']\n",
    "                cum_lat_trawl += row['lat']\n",
    "                cum_lng_trawl += row['lng']\n",
    "                \n",
    "                if trawl_start_lat== 0:\n",
    "                    trawl_start_lat = row['lat']\n",
    "                    trawl_start_lng = row['lng'] \n",
    "                # trawl_max_speed = np.maximum(trawl_max_speed, ThisTripPings_df['speed_prev_5min'])\n",
    "                trawl_max_speed = np.maximum(trawl_max_speed, ThisTripPings_df.loc[index, 'speed_prev_5min'])\n",
    "                if row['dist_from_origin']>trawl_max_dist_from_origin:\n",
    "                    trawl_max_dist_from_origin=row['dist_from_origin']\n",
    "                    trawl_max_lat = row['lat']\n",
    "                    trawl_max_lng = row['lng']\n",
    "        \n",
    "\n",
    "                # if ((index + 1 < len(ThisTripPings_df) and ThisTripPings_df.iloc[index + 1]['InTrawl'] == 0) or \n",
    "                if (index + 1 < len(ThisTripPings_df) and ThisTripPings_df.iloc[index + 1]['InTrawl'] == 0) | (index==len(ThisTripPings_df)-1):\n",
    "                    # Save this trawl only if it is not entirely a stop\n",
    "                    if trawl_max_speed>stop_speed:\n",
    "                        trawl_avg_speed = cumulative_dist_trawl/cumulative_time_trawl\n",
    "                        trawl_end_lat = row['lat']\n",
    "                        trawl_end_lng = row['lng']\n",
    "\n",
    "                        trawl_mid_lat = (trawl_end_lat + trawl_start_lat)/2\n",
    "                        trawl_mid_lng = (trawl_end_lng + trawl_start_lng)/2\n",
    "\n",
    "                        itrawl += 1\n",
    "                        trawl_lat = cum_lat_trawl/npings_in_trawl\n",
    "                        trawl_lng = cum_lng_trawl/npings_in_trawl\n",
    "\n",
    "                        trawl_avg_dist_from_origin = haversine(trawl_mid_lat, trawl_mid_lng, origin_lat, origin_lng)\n",
    "                        \n",
    "                        trawls_row_data = {'cuebiq_id': [cuebiq_id], 'Trip_number': Trip_number, \n",
    "                            'trawl_num': [itrawl],'trawl_duration': [cumulative_time_trawl],'cumulative_dist_trawl': [cumulative_dist_trawl],\n",
    "                            'trawl_start_lat': [trawl_start_lat],'trawl_start_lng': [trawl_start_lng], \n",
    "                            'trawl_end_lat': [trawl_end_lat],'trawl_end_lng': [trawl_end_lng], \n",
    "                            'trawl_mid_lat': [trawl_mid_lat],'trawl_mid_lng': [trawl_mid_lng], \n",
    "                            'trawl_max_lat': [trawl_max_lat],'trawl_max_lng': [trawl_max_lng], \n",
    "                            'trawl_avg_speed': [trawl_avg_speed],'trawl_max_speed': [trawl_max_speed], \n",
    "                            'trawl_avg_dist_from_origin': [trawl_avg_dist_from_origin], 'trawl_max_dist_from_origin': [trawl_max_dist_from_origin],\n",
    "                            'Max_distance_traveled_origin_t': [Max_distance_traveled_origin_t]\n",
    "                        }\n",
    "                        trawls_row_df = pd.DataFrame(trawls_row_data)\n",
    "\n",
    "                        trawls_df = pd.concat([trawls_df, trawls_row_df], ignore_index=True)\n",
    "            else:\n",
    "                npings_in_trawl = 0\n",
    "                firstrow = 0\n",
    "                cumulative_time_trawl = 0\n",
    "                cumulative_dist_trawl = 0 \n",
    "                trawl_max_speed = 0\n",
    "                trawl_start_lat = 0\n",
    "                trawl_start_lng = 0\n",
    "                trawl_max_dist_from_origin = 0\n",
    "\n",
    "    # Write to CSV files as long as not debugging\n",
    "    if NotDebugging:\n",
    "        if len(stops_df) ==0:\n",
    "            values = [cuebiq_id, Trip_number, 0, 0, 0, 0, 0,0, 0, 0, Max_distance_traveled_origin_t]\n",
    "            stops_df = pd.DataFrame([values], columns=stopcolumns)\n",
    "        stops_df.to_csv(Stops_Indicators_filename, mode='a', header=not os.path.exists(Stops_Indicators_filename), index=False)\n",
    "\n",
    "        if len(trawls_df) ==0:\n",
    "            values = [cuebiq_id, Trip_number, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, Max_distance_traveled_origin_t]\n",
    "            trawls_df = pd.DataFrame([values], columns=trawlcolumns)\n",
    "        trawls_df.to_csv(Trawls_Indicators_filename, mode='a', header=not os.path.exists(Trawls_Indicators_filename), index=False)\n",
    "\n",
    "        values = [cuebiq_id, Trip_number, istop, itrawl]\n",
    "        stop_trawl_df = pd.DataFrame([values], columns=stop_trawl_columns)\n",
    "        stop_trawl_df.to_csv(Stop_Trawls_Indicators_filename, mode='a', header=not os.path.exists(Stop_Trawls_Indicators_filename), index=False)\n",
    "    \n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e657429-1a14-421d-97e9-f50b97f64702",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions that are used to inspect a single trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b336184-1869-49a8-bf5a-9bd22e74f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectpings(random_trip_df):\n",
    "\n",
    "    # Identify the cuebiq_id, and times for this random trip\n",
    "    global timestamp_start_t, timestamp_end_t, trip_t, cuebiq_id_t\n",
    "    cuebiq_id_t = random_trip_df['cuebiq_id'].iloc[0]\n",
    "    timestamp_start_t = random_trip_df['timestamp_start_t'].iloc[0] \n",
    "    timestamp_end_t= random_trip_df['timestamp_end_t'].iloc[0] \n",
    "    trip_t= random_trip_df['Trip_number'].iloc[0] \n",
    "\n",
    "    start_time = time.time()\n",
    "    ########################################################################################\n",
    "    ###### SELECT THE RELEVANT DATA FROM THE GULF PINGS DATA FRAME ########################\n",
    "    pings_Gulf_df = AllPings_OurTable_df[\n",
    "        (AllPings_OurTable_df['cuebiq_id'] == cuebiq_id_t) &\n",
    "        (AllPings_OurTable_df['event_timestamp'] >= (timestamp_start_t)) &\n",
    "        (AllPings_OurTable_df['event_timestamp'] <= (timestamp_end_t))\n",
    "    ]\n",
    "    print(\"Selecting pings for ID\",cuebiq_id_t, \"trip\", trip_t, \"The Gulf DF had \", len(pings_Gulf_df) , \"pings\" )\n",
    "\n",
    "    if len(pings_Gulf_df) < 10:\n",
    "        print(\"The Gulf DF had \", len(pings_Gulf_df) , \"pings\")\n",
    "        pings_from_v3_df = []\n",
    "        return pings_from_v3_df, pings_Gulf_df\n",
    "        \n",
    "    else:\n",
    "        This_Group_df = ID_Groups_df[(ID_Groups_df['Min_In_Group'] <= cuebiq_id_t) & (ID_Groups_df['Max_In_Group'] >= cuebiq_id_t)]\n",
    "        Group_num = This_Group_df['Cubeq_ID_Group'].iloc[0]\n",
    "        filename = f\"V3_pings_{Group_num}.csv\"\n",
    "        Pings_V3_Group_filename= os.path.join(V3_Pings_Groups_directory,filename)\n",
    "        Pings_V3_Group_df = pd.read_csv(Pings_V3_Group_filename)\n",
    "        \n",
    "        eight_hours = 8*60*60\n",
    "        # All relevant V3 pings\n",
    "        pings_from_v3_df = Pings_V3_Group_df[\n",
    "            (Pings_V3_Group_df['cuebiq_id'] == cuebiq_id_t) &\n",
    "            (Pings_V3_Group_df['event_timestamp'] >= (timestamp_start_t-eight_hours)) &\n",
    "            (Pings_V3_Group_df['event_timestamp'] <= (timestamp_end_t+eight_hours))\n",
    "            ]\n",
    "\n",
    "        pings_from_v3_df.drop_duplicates(inplace=True)\n",
    "        pings_from_v3_df = pings_from_v3_df.sort_values(by='event_timestamp')\n",
    "        pings_from_v3_df=EliminateErrantPings(pings_from_v3_df)\n",
    "\n",
    "        # V3 pings in the middle of the trip\n",
    "        ping_V3_during_trip = pings_from_v3_df[\n",
    "            (pings_from_v3_df['event_timestamp'] >= (timestamp_start_t)) &\n",
    "            (pings_from_v3_df['event_timestamp'] <= (timestamp_end_t))\n",
    "            ]\n",
    "\n",
    "        ping_V3_during_trip.drop_duplicates(inplace=True)\n",
    "        ping_V3_during_trip = ping_V3_during_trip.sort_values(by='event_timestamp')\n",
    "        ping_V3_during_trip=EliminateErrantPings(ping_V3_during_trip)\n",
    "\n",
    "        # Drop from ping_V3_during_trip any pings with the same event_timestamp, lat, and lng as in pings_Gulf_df\n",
    "        merged_df = pd.merge(ping_V3_during_trip, pings_Gulf_df, on=['event_timestamp', 'lat', 'lng'], how='left', indicator=True)\n",
    "\n",
    "        # Filter out rows that are only in ping_V3_during_trip\n",
    "        filtered_df = merged_df[merged_df['_merge'] == 'left_only']\n",
    "        filtered_df.drop('_merge', axis=1, inplace=True)\n",
    "        ping_V3_during_trip = filtered_df\n",
    "        \n",
    "        return pings_from_v3_df, pings_Gulf_df, ping_V3_during_trip\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf333dac-b0c6-4b33-a242-8e0be464470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organizepings(pings_from_v3_df, pings_Gulf_df, ping_V3_during_trip):\n",
    "    global onetrip_before_pings, onetrip_after_pings, onetrip_gulf_pings\n",
    "    # Select the pings 8hours before and 8 hours after\n",
    "    eight_hours = 8*60*60\n",
    "\n",
    "    ################# During Trip from Gulf Pings  #################\n",
    "    onetrip_gulf_pings = pings_Gulf_df\n",
    "    # onetrip_gulf_pings=is_point_outside_all_islands(onetrip_gulf_pings)\n",
    "\n",
    "    if len(onetrip_gulf_pings)>0:\n",
    "        onetrip_gulf_pings = onetrip_gulf_pings.sort_values(by='event_timestamp')\n",
    "        onetrip_gulf_pings = EliminateErrantPingsSpeed(onetrip_gulf_pings, 60)\n",
    "        onetrip_gulf_pings['Hours_from_start'] = round((onetrip_gulf_pings['event_timestamp']-timestamp_start_t)/(60*60),1)\n",
    "        onetrip_gulf_pings['during_trip'] = \"DD\"\n",
    "        onetrip_gulf_pings.loc[onetrip_gulf_pings.index[0], 'during_trip'] = \"DS\"\n",
    "        onetrip_gulf_pings.loc[onetrip_gulf_pings.index[-1], 'during_trip'] = \"DE\"\n",
    "    else:\n",
    "        onetrip_gulf_pings = pd.DataFrame(columns=['event_timestamp', 'lat', 'lng','Hours_from_start',  'during_trip'])\n",
    "\n",
    "    ############## Before\n",
    "    onetrip_before_pings = pings_from_v3_df[\n",
    "        (pings_from_v3_df['event_timestamp'] >= (timestamp_start_t - eight_hours)) &\n",
    "        (pings_from_v3_df['event_timestamp'] <= timestamp_start_t)\n",
    "        ]\n",
    "\n",
    "    if len(onetrip_before_pings)>0:\n",
    "        onetrip_before_pings = onetrip_before_pings.sort_values(by='event_timestamp')\n",
    "        onetrip_before_pings = EliminateErrantPingsSpeed(onetrip_before_pings,90)\n",
    "        onetrip_before_pings['Hours_from_start'] = round((onetrip_before_pings['event_timestamp']-timestamp_start_t)/(60*60),1)\n",
    "        onetrip_before_pings['during_trip'] = \"BB\"\n",
    "        onetrip_before_pings.loc[onetrip_before_pings.index[0], 'during_trip'] = \"BS\"\n",
    "        onetrip_before_pings.loc[onetrip_before_pings.index[-1], 'during_trip'] = \"BE\"\n",
    "    else:\n",
    "        onetrip_before_pings = pd.DataFrame(columns=['event_timestamp', 'lat', 'lng','Hours_from_start',  'during_trip'])\n",
    "    \n",
    "    #################### After\n",
    "    onetrip_after_pings = pings_from_v3_df[\n",
    "        (pings_from_v3_df['event_timestamp'] <= (timestamp_end_t + eight_hours)) &\n",
    "        (pings_from_v3_df['event_timestamp'] >= timestamp_end_t)\n",
    "        ]\n",
    "    if len(onetrip_after_pings)>0:\n",
    "        onetrip_after_pings = onetrip_after_pings.sort_values(by='event_timestamp')\n",
    "        onetrip_after_pings = EliminateErrantPingsSpeed(onetrip_after_pings,90)\n",
    "        \n",
    "        onetrip_after_pings['Hours_from_start'] = round((onetrip_after_pings['event_timestamp']-timestamp_start_t)/(60*60),1)\n",
    "        onetrip_after_pings['during_trip'] = \"AA\"\n",
    "        onetrip_after_pings.loc[onetrip_after_pings.index[0], 'during_trip'] = \"AS\"\n",
    "        onetrip_after_pings.loc[onetrip_after_pings.index[-1], 'during_trip'] = \"AE\"\n",
    "    else:\n",
    "        onetrip_after_pings = pd.DataFrame(columns=['event_timestamp', 'lat', 'lng','Hours_from_start',  'during_trip'])\n",
    "\n",
    "    #################### V3 During the Trip\n",
    "    onetrip_V3_during_pings = pings_from_v3_df[\n",
    "        (pings_from_v3_df['event_timestamp'] > (timestamp_start_t)) &\n",
    "        (pings_from_v3_df['event_timestamp'] < (timestamp_end_t)) \n",
    "        ]\n",
    "    \n",
    "    ### New Code to narrow points that don't overlap with our Gulf WKT excluding islands\n",
    "    polygons = [wkt.loads(Gulf_wkt)]  \n",
    "    df = onetrip_V3_during_pings\n",
    "    df['is_in_Gulf'] = df.apply(lambda row: is_inside_any_polygon(row['lat'], row['lng'], polygons), axis=1)\n",
    "    outside_gulf = df[df['is_in_Gulf'] == False]\n",
    "    # print(\"len(outside_gulf)\", len(outside_gulf))\n",
    "    inside_gulf = df[df['is_in_Gulf'] == True]\n",
    "    # print(\"len(inside_gulf)\", len(inside_gulf))\n",
    "    \n",
    "    on_islands = is_point_on_islands(inside_gulf)\n",
    "    # print(\"len(on_islands)\", len(on_islands))\n",
    "    onetrip_V3_during_pings = pd.concat([outside_gulf, on_islands])\n",
    "    # print(\"len(onetrip_V3_during_pings)\", len(onetrip_V3_during_pings))\n",
    "    columns_to_drop = ['is_in_Gulf']\n",
    "    onetrip_V3_during_pings.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    ########################\n",
    "    if len(onetrip_V3_during_pings)>0:\n",
    "        onetrip_V3_during_pings = onetrip_V3_during_pings.sort_values(by='event_timestamp')\n",
    "        onetrip_V3_during_pings['Hours_from_start'] = round((onetrip_V3_during_pings['event_timestamp']-timestamp_start_t)/(60*60),1)\n",
    "        onetrip_V3_during_pings['during_trip'] = \"II\"\n",
    "        # onetrip_V3_during_pings.loc[onetrip_island_pings.index[0], 'during_trip'] = \"IS\"\n",
    "        # onetrip_V3_during_pings.loc[onetrip_island_pings.index[-1], 'during_trip'] = \"IE\"\n",
    "        # print(\"len(onetrip_V3_during_pings)\", len(onetrip_V3_during_pings))\n",
    "    else:\n",
    "        onetrip_V3_during_pings = pd.DataFrame(columns=['event_timestamp', 'lat', 'lng','Hours_from_start',  'during_trip'])\n",
    "\n",
    "    ########################\n",
    "    # Concatenate the Four DataFrames\n",
    "    onetrip_df = pd.concat([\n",
    "                            onetrip_V3_during_pings[['event_timestamp', 'lat', 'lng', 'Hours_from_start', 'during_trip']],\n",
    "                            onetrip_before_pings[['event_timestamp', 'lat', 'lng','Hours_from_start',  'during_trip']],\n",
    "                            onetrip_gulf_pings[['event_timestamp', 'lat', 'lng', 'Hours_from_start', 'during_trip']],\n",
    "                            onetrip_after_pings[['event_timestamp', 'lat', 'lng', 'Hours_from_start', 'during_trip']]\n",
    "                           ])\n",
    "    # delete empty rows and fill in blanks\n",
    "    onetrip_df = onetrip_df[onetrip_df['event_timestamp'].notna() & (onetrip_df['event_timestamp'] != '')]\n",
    "    onetrip_df = onetrip_df.dropna()\n",
    "\n",
    "    \n",
    "    # Sort the DataFrame by 'event_timestamp'\n",
    "    onetrip_df = onetrip_df.sort_values(by='event_timestamp')\n",
    "\n",
    "    # Clean up to eliminate bad pings\n",
    "    onetrip_df = EliminateErrantPingsSpeed(onetrip_df, 60)\n",
    "    \n",
    "    # Should the next row be commented out? It seems redundant but in some cases onetrip_V3_during_pings doesn't have Hours_from_start\n",
    "    onetrip_df['Hours_from_start'] = (onetrip_df['event_timestamp']-timestamp_start_t)/(60*60)\n",
    "    # Reset index\n",
    "    onetrip_df.reset_index(drop=True, inplace=True)\n",
    "    return onetrip_df\n",
    "    print(\"pings are organized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429fd0ee-1a3a-4151-a10e-0533b889d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from shapely import wkt\n",
    "\n",
    "# def makethemap():\n",
    "def makethemap(onetrip_df, width, height):\n",
    "\n",
    "\n",
    "    # Calculate the bounding box\n",
    "    min_lat = onetrip_df['lat'].min()\n",
    "    max_lat = onetrip_df['lat'].max()\n",
    "    min_lng = onetrip_df['lng'].min()\n",
    "    max_lng = onetrip_df['lng'].max()\n",
    "\n",
    "    # Create a map with bounds and satellite tiles\n",
    "    m = folium.Map(location=[(min_lat + max_lat) / 2, (min_lng + max_lng) / 2], zoom_start=10, tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google Satellite', width=width, height=height)\n",
    "    m.fit_bounds([[min_lat, min_lng], [max_lat, max_lng]])\n",
    "\n",
    "    # Create feature groups\n",
    "    circle_markers = folium.FeatureGroup(name='Circle Markers')\n",
    "    poly_lines = folium.FeatureGroup(name='PolyLines')\n",
    "\n",
    "    # Keep track of the first and last occurrence of during_trip == 0\n",
    "    first_during_trip_0 = None\n",
    "    last_during_trip_0 = None\n",
    "\n",
    "    # Keep track of previous coordinates\n",
    "    prev_coords = None\n",
    "\n",
    "    AlabamaIsland_polygon = wkt.loads(AlabamaIsland)\n",
    "\n",
    "    # Add the AlabamaIsland polygon to the map as a GeoJson layer\n",
    "    folium.GeoJson(data=AlabamaIsland_polygon.__geo_interface__, name='Alabama Island').add_to(m)\n",
    "\n",
    "    # Add the industrial polygons to the folium map\n",
    "    for polygon in Industrial_polygons:\n",
    "        folium.GeoJson(data=polygon.__geo_interface__, name='Industrial Polygon').add_to(m)\n",
    "\n",
    "\n",
    "    ##################### CHAT GPT CODE TO ADD COLORS BASED ON SPEED\n",
    "    onetrip_df=pingspeed(onetrip_df)\n",
    "\n",
    "    # Add CircleMarker and lines for each ping\n",
    "    for index, row in onetrip_df.iterrows():\n",
    "        radius = 5\n",
    "        if row['during_trip'][0] == 'D':\n",
    "            if row['Avg_ping_speed'] < 0.02682238:  # 1 mph\n",
    "                color = 'green'\n",
    "            elif row['Avg_ping_speed'] <= 0.134111898:  # 5 mph\n",
    "                color = 'orange'\n",
    "            elif row['Avg_ping_speed'] <= 3*0.134111898:  # 15 mph\n",
    "                color = 'pink'\n",
    "            else:\n",
    "                color = 'red'\n",
    "\n",
    "            if first_during_trip_0 is None:\n",
    "                first_during_trip_0 = True\n",
    "        elif row['during_trip'][0] == 'B':\n",
    "            color = 'blue'\n",
    "        elif row['during_trip'][0] == 'I':\n",
    "            color = 'black'\n",
    "        else:\n",
    "            color = 'red'\n",
    "            last_during_trip_0 = True\n",
    "\n",
    "        # Set larger radius for the first and last occurrence of during_trip == 0\n",
    "        if row['during_trip'] == 'DS':\n",
    "            radius = 7 \n",
    "            # radius = 7 if first_during_trip_0 or last_during_trip_0 else 3\n",
    "            # border_color = 'darkblue' if first_during_trip_0 else 'darkblue'\n",
    "            border_color = 'darkblue' \n",
    "        elif row['during_trip'][-1] == 'DE':\n",
    "            # radius = 6 if first_during_trip_0 or last_during_trip_0 else 3\n",
    "            radius = 7 \n",
    "            border_color = 'maroon' \n",
    "            # border_color = 'maroon' if first_during_trip_0 else 'maroon'\n",
    "        else:\n",
    "            border_color = color\n",
    "\n",
    "        # Add CircleMarker to circle_markers feature group\n",
    "        popup_text = f\"{round(row['Hours_from_start'],1)}hrs/{round(row['Avg_ping_speed'] * 37.2823, 1)}mph\"\n",
    "        folium.CircleMarker(\n",
    "            location=[row['lat'], row['lng']],\n",
    "            radius=radius,  # Adjust the radius as needed\n",
    "            color=border_color,\n",
    "            fill=True,\n",
    "            fill_color=color,  # Assign color based on ping_speed\n",
    "            fill_opacity=0.6,\n",
    "            popup=popup_text\n",
    "        ).add_to(circle_markers)\n",
    "\n",
    "\n",
    "        # Draw line if during_trip == 'D' and prev_coords is not None\n",
    "        if row['during_trip'][0] == 'I' and row['during_trip'][-1] == 'I':\n",
    "            folium.PolyLine(locations=[prev_coords, [row['lat'], row['lng']]], color='black').add_to(poly_lines)\n",
    "        elif row['during_trip'][0] == 'D' and row['during_trip'][-1] == 'D':\n",
    "            folium.PolyLine(locations=[prev_coords, [row['lat'], row['lng']]], color='pink').add_to(poly_lines)\n",
    "        elif row['during_trip'][0] == 'B' and row['during_trip'][-1] == 'B':\n",
    "            folium.PolyLine(locations=[prev_coords, [row['lat'], row['lng']]], color='blue').add_to(poly_lines)\n",
    "        elif row['during_trip'][0] == 'A' and row['during_trip'][-1] == 'A':\n",
    "            folium.PolyLine(locations=[prev_coords, [row['lat'], row['lng']]], color='red').add_to(poly_lines)\n",
    "\n",
    "        # Update prev_coords\n",
    "        prev_coords = [row['lat'], row['lng']]\n",
    "\n",
    "\n",
    "    for index, row in platforms.iterrows():\n",
    "        lat, lng = row['lat'], row['lng']\n",
    "        folium.CircleMarker(location=[lat, lng], radius=3, color='yellow', fill=True, fill_color='grey').add_to(m)\n",
    "\n",
    "    for index, row in LA_AR.iterrows():\n",
    "        lat, lng = row['lat'], row['lng']\n",
    "        folium.CircleMarker(location=[lat, lng], radius=3, color='blue', fill=True, fill_color='grey').add_to(m)\n",
    "\n",
    "    for index, row in TX_AR.iterrows():\n",
    "        lat, lng = row['lat'], row['lng']\n",
    "        folium.CircleMarker(location=[lat, lng], radius=3, color='green', fill=True, fill_color='grey').add_to(m)\n",
    "\n",
    "\n",
    "    for index, row in AL_AR.iterrows():\n",
    "        lat, lng = row['lat'], row['lng']\n",
    "        folium.CircleMarker(location=[lat, lng], radius=3, color='grey', fill=True, fill_color='grey').add_to(m)\n",
    "\n",
    "    # Add feature groups to the map\n",
    "    circle_markers.add_to(m)\n",
    "    poly_lines.add_to(m)\n",
    "\n",
    "    ping_V3_during_trip['Hours_from_start'] = (ping_V3_during_trip['event_timestamp']-timestamp_start_t)/(60*60)\n",
    "    for idx, row in ping_V3_during_trip.iterrows():\n",
    "        popup_text = f\"{round(row['Hours_from_start'],1)}hrs/{round(row['Avg_ping_speed'] * 37.2823, 1)}mph\"\n",
    "        folium.CircleMarker(\n",
    "            location=[row['lat'], row['lng']],\n",
    "            radius=5,  # Adjust the radius as needed\n",
    "            color='purple',\n",
    "            fill=True,\n",
    "            fill_color='purple',\n",
    "            popup=popup_text\n",
    "        ).add_to(m)\n",
    "\n",
    "\n",
    "    print(\"idi\", cuebiq_id_t, \"trip_t\", trip_t, \"Pings: Before\", len(onetrip_before_pings), \"After\", len(onetrip_after_pings), \"During \", len(onetrip_gulf_pings))\n",
    "    # Display the map\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f431b52b-d004-452e-b110-b136b6ff5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_summary(summary_indicators_df):\n",
    "    \n",
    "    summary_indicators_df['RecTripRating'] = rec_trip_rating\n",
    "    if len(rec_trip_rating)>0:\n",
    "        summary_indicators_df['comments']=comments\n",
    "        # Assuming summary_indicators_df is your DataFrame\n",
    "        # summary_indicators_df = summary_indicators_df[['cuebiq_id', 'Trip_number', 'RecTripRating', 'comments', \n",
    "        #                                            'Prob_371', 'Total_distance', 'Max_distance_traveled_origin_t', \n",
    "        #                                            'maxspeed_mph', 'Trip_Duration_hrs', 'final_over_max', \n",
    "        #                                            'Begin_End_Dist_from_Coast_max', 'Begin_End_Dist_from_Coast_min']]\n",
    "\n",
    "        merged_df = pd.concat([summary_indicators_df, random_trip_df], axis=1)\n",
    "        merged_df.to_csv(RecTripRating_filename, mode='a', header=not os.path.exists(RecTripRating_filename), index=False)\n",
    "\n",
    "    del summary_indicators_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41686d45-9f13-43af-aa75-6e837088b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_indicators():\n",
    "    # if 'summary_indicators_df' in globals():\n",
    "    #     del summary_indicators_df\n",
    "\n",
    "    summary_indicators_df = pd.DataFrame()\n",
    "    # print(summary_indicators_df.columns)\n",
    "    summary_indicators_df['cuebiq_id'] = random_trip_df['cuebiq_id']\n",
    "    summary_indicators_df['Trip_number'] = random_trip_df['Trip_number']\n",
    "    summary_indicators_df['Prob_371']=random_trip_df['Probability_371']\n",
    "    summary_indicators_df['Trip__hrs']=random_trip_df['Trip_Duration_t']/60\n",
    "    summary_indicators_df['max_time_gap']=onetrip_gulf_pings['time_diff_minutes_from_previous'].max()/60\n",
    "    summary_indicators_df['stopped_pct']=random_trip_df['pdf_GT_0_LE_0_010_km_per_min_t']+random_trip_df['pdf_GT_0_010_LE_0_025_km_per_min_t']+random_trip_df['pdf_GT_0_025_LE_0_050_km_per_min_t']\n",
    "    summary_indicators_df['island_min']=random_trip_df['minutes_during_outside_gulf']\n",
    "    summary_indicators_df['island_pct']=random_trip_df['pct_during_outside_gulf']\n",
    "    summary_indicators_df['Max_distance_traveled_origin_t']=random_trip_df['Max_distance_traveled_origin_t']\n",
    "    summary_indicators_df['Total_distance']=random_trip_df['Total_distance_traveled_t']\n",
    "    summary_indicators_df['island_speed'] = random_trip_df['avg_mph_during_outside_gulf']\n",
    "    # summary_indicators_df['stop_min']=random_trip_df['time_stopped_t']\n",
    "    # summary_indicators_df['stopped_pct']=random_trip_df['pct_time_stopped']\n",
    "\n",
    "    # summary_indicators_df['trawl_min']=random_trip_df['time_trawling_t']\n",
    "    \n",
    "    summary_indicators_df['maxspeed_mph']=random_trip_df['maxspeed_t']*37.2823\n",
    "    summary_indicators_df['maxspeed_2']=(onetrip_gulf_pings['ping_speed_fwd'].max())*37.2823\n",
    "    summary_indicators_df['final_over_max'] = random_trip_df['Distance_from_origin_t']/random_trip_df['Max_distance_traveled_origin_t']\n",
    "    # summary_indicators_df['Begin_End_Dist_from_Coast_max'] = random_trip_df[\n",
    "    #     ['first_distance_from_coast_t', 'last_distance_from_coast_t']].max(axis=1)\n",
    "    \n",
    "    \n",
    "    # summary_indicators_df['Begin_End_Dist_from_Coast_min'] = random_trip_df[\n",
    "    #     ['first_distance_from_coast_t', 'last_distance_from_coast_t']].min(axis=1)\n",
    "\n",
    "    return summary_indicators_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e366c9-c1e5-4880-8898-fd7c1ed92ac4",
   "metadata": {},
   "source": [
    "# Create summary stats of Raw indicators \n",
    "to compare Spectus and AIS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a89d7c-eb73-425c-b5e2-00c0420a46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllIndicators_df = pd.read_csv(DisappearanceIndicators_filename)\n",
    "AllIndicators_df = pd.read_csv(Rec_indicators_with_V3_filename)\n",
    "\n",
    "print(\"len(AllIndicators_df)\", len(AllIndicators_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb2df3f-a411-4bdd-84ad-1da1a71a9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming AllIndicators_df is your DataFrame and it contains the required columns\n",
    "variables = [\n",
    "    'trips_per_day', \n",
    "    'break_duration_t',\n",
    "    'Weekday_trips', \n",
    "    'Trip_Duration_t',\n",
    "    'Total_distance_traveled_t', \n",
    "    'Distance_from_origin_t', \n",
    "    'first_distance_from_coast_t',\n",
    "    'last_distance_from_coast_t', \n",
    "    'Max_distance_traveled_origin_t',\n",
    "    'Weekend_trip_t', \n",
    "    'time_stopped_t', \n",
    "    'pct_time_stopped_2',\n",
    "    'number_of_stops_t', \n",
    "    'longest_stop_t',\n",
    "    'shortest_stop_t',\n",
    "    'time_trawling_t',\n",
    "    'pct_time_trawling_2',\n",
    "    'number_of_trawl_t',\n",
    "    'longest_trawl_t', \n",
    "    'shortest_trawl_t', \n",
    "    'distance_trawling_t',\n",
    "    'time_moving_t', \n",
    "    'pct_time_moving_2', \n",
    "    'number_of_moves_t',\n",
    "    'longest_move_t', \n",
    "    'shortest_move_t',\n",
    "    'distance_moving_t',\n",
    "    'dist_from_origin_during_stops', \n",
    "    'Trip_pings_mov_traw_per_time_t',\n",
    "    'move_efficiency_t', \n",
    "    'move_speed_t',\n",
    "    'maxspeed_t', \n",
    "    'max_distance_from_coast_t',\n",
    "    'WSPD', \n",
    "    'GST',\n",
    "    'WVHT', \n",
    "    'ATMP'\n",
    "]\n",
    "\n",
    "# Calculate mean and standard deviation for the specified variables\n",
    "means = AllIndicators_df[variables].mean()\n",
    "medians = AllIndicators_df[variables].median()\n",
    "stds = AllIndicators_df[variables].std()\n",
    "mins = AllIndicators_df[variables].min()\n",
    "maxes = AllIndicators_df[variables].max()\n",
    "\n",
    "\n",
    "# Create a new DataFrame to store the results\n",
    "summary_df = pd.DataFrame({\n",
    "    'Mean': means,\n",
    "    'Median': medians,\n",
    "    'Standard Deviation': stds,\n",
    "    'Min': mins,\n",
    "    'Max': maxes    \n",
    "})\n",
    "\n",
    "summary_df.loc['Trips'] = [len(AllIndicators_df), None, None, None, None]\n",
    "\n",
    "# Display the summary table\n",
    "print(summary_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3555ca6-4f50-4feb-b9bb-677699924147",
   "metadata": {},
   "source": [
    "### Now create a the same table for the AIS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c354fbc-03c3-47ba-9e8e-aa32b13b1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIS_indicators_df = pd.read_csv(AIS_indicators_file_path)\n",
    "\n",
    "### REPLACE THE pct_time VARIABLES BECAUSE THESE WERE INCORRECT IN THE RAW SPECTUS DATA -- THEY DO NOT APPEAR TO BE INCORRECT IN THE AIS DATA, BUT THIS IS DONE FO THE SAKE OF CONSISTENCY\n",
    "AIS_indicators_df['pct_time_stopped_2']=AIS_indicators_df['pdf_EQ_0_km_per_min_t']+AIS_indicators_df['pdf_GT_0_LE_0_010_km_per_min_t']+AIS_indicators_df['pdf_GT_0_010_LE_0_025_km_per_min_t']\n",
    "AIS_indicators_df['pct_time_trawling_2']=AIS_indicators_df['pdf_GT_0_025_LE_0_050_km_per_min_t']+AIS_indicators_df['pdf_GT_0_05_LE_0_0.75_km_per_min_t']+AIS_indicators_df['pdf_GT_0_07.5_LE_0_1_km_per_min_t']\n",
    "AIS_indicators_df['pct_time_moving_2']=AIS_indicators_df['pdf_GT_0_2_LE_0_3_km_per_min_t']+AIS_indicators_df['pdf_GT_0_3_LE_0_4_km_per_min_t']+AIS_indicators_df['pdf_GT_0_4_LE_0_5_km_per_min_t']+AIS_indicators_df['pdf_GT_0_5_LE_0_6_km_per_min_t']+AIS_indicators_df['pdf_GT_0_6_LE_0_7_km_per_min_t']+AIS_indicators_df['pdf_GT_0_6_LE_0_7_km_per_min_t']+AIS_indicators_df['pdf_GT_0_7_LE_0_8_km_per_min_t']+AIS_indicators_df['pdf_GT_0_8_LE_0_9_km_per_min_t']+AIS_indicators_df['pdf_GT_0_9_LE_1_0_km_per_min_t']+AIS_indicators_df['pdf_GT_1_km_per_min_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2b0fd-55ea-4377-827c-4917a19efdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming AllIndicators_df is your DataFrame and it contains the required columns\n",
    "variables = [\n",
    "    'trips_per_day', \n",
    "    'break_duration_t',\n",
    "    'Weekday_trips', \n",
    "    'Trip_Duration_t',\n",
    "    'Total_distance_traveled_t', \n",
    "    'Distance_from_origin_t', \n",
    "    'first_distance_from_coast_t',\n",
    "    'last_distance_from_coast_t', \n",
    "    'Max_distance_traveled_origin_t',\n",
    "    'Weekend_trip_t', \n",
    "    'time_stopped_t', \n",
    "    'pct_time_stopped_2',\n",
    "    'number_of_stops_t', \n",
    "    'longest_stop_t',\n",
    "    'shortest_stop_t',\n",
    "    'time_trawling_t',\n",
    "    'pct_time_trawling_2',\n",
    "    'number_of_trawl_t',\n",
    "    'longest_trawl_t', \n",
    "    'shortest_trawl_t', \n",
    "    'distance_trawling_t',\n",
    "    'time_moving_t', \n",
    "    'pct_time_moving_2', \n",
    "    'number_of_moves_t',\n",
    "    'longest_move_t', \n",
    "    'shortest_move_t',\n",
    "    'distance_moving_t',\n",
    "    'dist_from_origin_during_stops', \n",
    "    'Trip_pings_mov_traw_per_time_t',\n",
    "    'move_efficiency_t', \n",
    "    'move_speed_t',\n",
    "    'maxspeed_t', \n",
    "    'max_distance_from_coast_t',\n",
    "    'WSPD', \n",
    "    'GST',\n",
    "    'WVHT', \n",
    "    'ATMP'\n",
    "]\n",
    "\n",
    "# Calculate mean and standard deviation for the specified variables\n",
    "means = AIS_indicators_df[variables].mean()\n",
    "stds = AIS_indicators_df[variables].std()\n",
    "medians = AIS_indicators_df[variables].median()\n",
    "mins = AIS_indicators_df[variables].min()\n",
    "maxes = AIS_indicators_df[variables].max()\n",
    "\n",
    "# Create a new DataFrame to store the results\n",
    "AIS_summary_df = pd.DataFrame({\n",
    "    'AIS_Mean': means,\n",
    "    'AIS_Median': medians,\n",
    "    'AIS_Standard Deviation': stds,\n",
    "    'AIS_Mins': mins,\n",
    "    'AIS_Maxes': maxes\n",
    "})\n",
    "\n",
    "AIS_summary_df.loc['Trips'] = [len(AIS_indicators_df), None, None, None, None]\n",
    "\n",
    "# Display the summary table\n",
    "print(AIS_summary_df.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d1663-546c-4c8d-b3e8-2d677d69c456",
   "metadata": {},
   "source": [
    "### Save the summary stats to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77869e10-4b4b-46e5-9cda-69eab9f46673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_summary_df = pd.merge(AIS_summary_df, summary_df)\n",
    "merged_summary_df = summary_df.merge(AIS_summary_df, left_index=True, right_index=True)\n",
    "\n",
    "FeaturesSummaryTable_filename = os.path.join(Results_directory,'FeaturesSummaryTable.csv')\n",
    "merged_summary_df.to_csv(FeaturesSummaryTable_filename, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c54012-669a-444f-b2be-6d4bce278c42",
   "metadata": {},
   "source": [
    "# DEBUGGING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c862d-efa2-4fe3-af23-0cee1384fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "disappear_df = pd.read_csv(DisappearanceIndicators_filename)\n",
    "indicators_df = pd.read_csv(Rec_indicators_with_V3_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def83d3-191f-486d-8fc1-3934429d24c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2de16-108a-44f5-9af6-d9e3204004b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(\n",
    "    disappear_df,\n",
    "    indicators_df,\n",
    "    on=['cuebiq_id', 'Trip_number'],\n",
    "    how='outer',  # Use 'outer' to keep all rows from both DataFrames\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5238b59-2716-4cf0-b946-183c80b68525",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(disappear_df), len(indicators_df), len(merged)\")\n",
    "print(len(disappear_df), len(indicators_df), len(merged))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
