{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "022c8ac4-70ba-4dbf-9be8-d519e0538e0c",
   "metadata": {},
   "source": [
    "# This notebook carries out the valididation analysis, comparing the temporal and spatial variation in the identified recreational fishing trips with data from MRIP and TPWD "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd43842-92c1-4b5d-8521-99850d0f8058",
   "metadata": {},
   "source": [
    "## Key inputs: \n",
    "* **Station_NonStationAnalysis_full.csv** -- includes an indicator of whether the trip stopped at a station\n",
    "* **rec_indicators_with_V3** - features used for classification\n",
    "* **DisappearanceIndicators.csv** - indicators of whether a trip is fully tracked\n",
    "\n",
    "## Key outputs:\n",
    "* **Validation figures** for temporal and spatial correlations between mobility data trips and those in data from Texas and the MRIP states of Alabama and Louisiana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e589575d-3447-47c3-882b-07231184d09c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e3058-d05a-4dda-a61a-f3163dbf7986",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d6e03-31a5-4364-8a2c-8d5170969e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h3\n",
    "!pip install --upgrade h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7fd74-d818-4530-b514-722cf774c2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Install modules\n",
    "!pip install tqdm\n",
    "!pip install statsmodels\n",
    "\n",
    "# # Suppress all warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime\n",
    "import pytz  \n",
    "import warnings\n",
    "import numpy as np\n",
    "import math\n",
    "local_timezone = pytz.timezone('US/Central')\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "import datetime as dt\n",
    "import geopandas as gpd\n",
    "import heapq\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "import statsmodels\n",
    "import inspect\n",
    "import folium\n",
    "from folium import plugins\n",
    "import h3\n",
    "\n",
    "from scipy import interpolate\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "from zipfile import ZipFile\n",
    "from shapely.geometry import Point  # Import the Point class from shapely.geometry\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "class OperationCancelled(Exception):\n",
    "    pass\n",
    "\n",
    "local_timezone = pytz.timezone('US/Central')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6324afe-3f34-4f73-b562-a26f0d631361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856bd9a2-337b-4ab2-a161-f282e5f74295",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set directories to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6ac76-7613-43f1-9b36-0f37f2ba9ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Directory for this output\n",
    "OurTable_V3_directory = '~/RecFishing/Analysis with Our Tables and V3/Data Files'\n",
    "# Expand the tilde to the user's home directory\n",
    "OurTable_V3_directory = os.path.expanduser(OurTable_V3_directory)\n",
    "# Check to make sure the directory exist\n",
    "DirExist = os.path.exists(OurTable_V3_directory)\n",
    "print(OurTable_V3_directory, \"exists = \" ,DirExist)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory for output from the first draw\n",
    "Batch01_directory = '~/RecFishing/Analysis with Our Tables and V3/Data Files/Batch01'\n",
    "# Expand the tilde to the user's home directory\n",
    "Batch01_directory = os.path.expanduser(Batch01_directory)\n",
    "# Check to make sure the directory exist\n",
    "DirExist = os.path.exists(Batch01_directory)\n",
    "print(Batch01_directory, \"exists = \" ,DirExist)\n",
    "\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory for Groups of V3 Pings\n",
    "V3_Pings_Groups_directory = '~/RecFishing/Analysis with Our Tables and V3/Data Files/V3_Ping_Groups'\n",
    "V3_Pings_Groups_directory = os.path.expanduser(V3_Pings_Groups_directory)\n",
    "print(V3_Pings_Groups_directory, \"exists = \" ,os.path.exists(V3_Pings_Groups_directory))\n",
    "\n",
    "#################################################################\n",
    "# Directory some core data fro analysis\n",
    "CoreData_Directory = '~/RecFishing/CoreData'\n",
    "CoreData_Directory = os.path.expanduser(CoreData_Directory)\n",
    "print(CoreData_Directory, \"exists = \", os.path.exists(CoreData_Directory))\n",
    "\n",
    "#################################################################\n",
    "# Directory  with Original_directory material\n",
    "Original_directory = '~/RecFishing/DataflowStudioJobs'\n",
    "Original_directory = os.path.expanduser(Original_directory)\n",
    "DirExist = os.path.exists(Original_directory)\n",
    "print(Original_directory, \"exists = \", DirExist)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory  with Original_directory material\n",
    "Previously_Processed_directory = '~/RecFishing/DataflowStudioJobs/FinalCode - Rec Fishing Identification'\n",
    "Previously_Processed_directory = os.path.expanduser(Previously_Processed_directory)\n",
    "DirExist = os.path.exists(Previously_Processed_directory)\n",
    "print(Previously_Processed_directory, \"exists = \", DirExist)\n",
    "\n",
    "#################################################################\n",
    "# Directory with Travel Cost files\n",
    "Travel_Cost_directory = '~/RecFishing/Travel Costs with Dedicated Table/CSV Files'\n",
    "# Expand the tilde to the user's home directory\n",
    "Travel_Cost_directory = os.path.expanduser(Travel_Cost_directory)\n",
    "DirExist = os.path.exists(Travel_Cost_directory)\n",
    "print(Travel_Cost_directory, \"exists = \", DirExist)\n",
    "\n",
    "Travel_Cost_Shapefile_directory = '~/RecFishing/Travel Costs with Dedicated Table/Shapefiles'\n",
    "# Expand the tilde to the user's home directory\n",
    "Travel_Cost_Shapefile_directory = os.path.expanduser(Travel_Cost_Shapefile_directory)\n",
    "DirExist = os.path.exists(Travel_Cost_Shapefile_directory)\n",
    "print(Travel_Cost_Shapefile_directory, \"exists = \", DirExist)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory with Weather data and related files\n",
    "Weather_Data_directory = '~/RecFishing/uploaded_files/Weather Data'\n",
    "# Expand the tilde to the user's home directory\n",
    "Weather_Data_directory = os.path.expanduser(Weather_Data_directory)\n",
    "print(Weather_Data_directory, \"exists = \", DirExist)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory with other Uploaded data \n",
    "Uploaded_Data_directory = '~/RecFishing/uploaded_files'\n",
    "Uploaded_Data_directory = os.path.expanduser(Uploaded_Data_directory)\n",
    "# FishBrain Data\n",
    "Fishbrain_directory = '~/RecFishing/uploaded_files/FishBrain'\n",
    "Fishbrain_directory = os.path.expanduser(Fishbrain_directory)\n",
    "\n",
    "# FishAngler Data\n",
    "FishAngler_directory = '~/RecFishing/uploaded_files/FishAngler'\n",
    "FishAngler_directory = os.path.expanduser(FishAngler_directory)\n",
    "\n",
    "#################################################################\n",
    "# Results and Analysis\n",
    "Results_directory = '~/RecFishing/Analysis with Our Tables and V3/Results'\n",
    "Results_directory = os.path.expanduser(Results_directory)\n",
    "\n",
    "Figures_directory = '~/RecFishing/Analysis with Our Tables and V3/Results/Figures'\n",
    "Figures_directory = os.path.expanduser(Figures_directory)\n",
    "\n",
    "TrajectoryMaps_directory = '~/RecFishing/Analysis with Our Tables and V3/Results/Figures/Maps'\n",
    "TrajectoryMaps_directory = os.path.expanduser(TrajectoryMaps_directory)\n",
    "\n",
    "Validation_directory = '~/RecFishing/Analysis with Our Tables and V3/Results/Validation'\n",
    "Validation_directory = os.path.expanduser(Validation_directory)\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "####################  AIS Directory #################################################\n",
    "AIS_Directory = '~/RecFishing/AIS Files/Data'\n",
    "AIS_Directory = os.path.expanduser(AIS_Directory)\n",
    "DirExist = os.path.exists(AIS_Directory)\n",
    "print(AIS_Directory, \"exists = \", DirExist)\n",
    "\n",
    "# ID_list_RandomSample from ScheduledExecution5.pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e583a397-c173-40b8-824c-c2cb81ec0671",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set input and output files to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db7026-6a9a-4369-9695-cb4a9ddafb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_existence(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"{file_path} Does NOT Exist\")\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "#########################  Log File  ################\n",
    "Log_filename  =  os.path.join(OurTable_V3_directory, 'Log.txt')\n",
    "\n",
    "######################################################################################################################\n",
    "########################## Complete list of randomized IDs- without bernouli sampling 740k #########################\n",
    "# PKL_File_With_Random_IDs_Filename  =  os.path.join(Original_directory, 'cuebiq_id_list_wo_sampling_740k.pkl')\n",
    "PKL_File_With_Random_IDs_Filename  =  os.path.join(CoreData_Directory, 'cuebiq_id_list_wo_sampling_740k.pkl')\n",
    "check_file_existence(PKL_File_With_Random_IDs_Filename)\n",
    "    \n",
    "# Data gathered and used prior to the NOAA Webinar in February 2024\n",
    "IDs_Used_in_NOAA_Webinar_filename = os.path.join(CoreData_Directory, 'IDs_From_Random_Draw_Prior_to_NOAA_Webinar.csv')\n",
    "check_file_existence(IDs_Used_in_NOAA_Webinar_filename)\n",
    "Ping_Used_in_NOAA_Webinar_filename = os.path.join(CoreData_Directory, 'Pings_From_Random_Draw_Prior_to_NOAA_Webinar.csv')\n",
    "check_file_existence(Ping_Used_in_NOAA_Webinar_filename)\n",
    "                                                     \n",
    "# List of IDs that have been processed for Indicators\n",
    "AlreadyFullyProcessedIDs_Filename  =  os.path.join(OurTable_V3_directory, 'RandomlyChosenCuebiq_ids.List_of_Processed_ids.csv')\n",
    "check_file_existence(AlreadyFullyProcessedIDs_Filename)\n",
    "    \n",
    "######################################################################################################################\n",
    "#########################  ID Checklist with columns for ID, Pings, Indicators Created (TF) & Trips  ################\n",
    "IDs_Pulled_from_Dedicated_Table_filename  =  os.path.join(OurTable_V3_directory, 'IDs_Pulled_From_Dedicated_Table.csv')\n",
    "check_file_existence(IDs_Pulled_from_Dedicated_Table_filename)\n",
    "    \n",
    "ID_For_V3_Queries_filename  =  os.path.join(OurTable_V3_directory, 'IDs_from_V3.csv')\n",
    "check_file_existence(ID_For_V3_Queries_filename)\n",
    "    \n",
    "RecTripRating_filename =  os.path.join(OurTable_V3_directory, 'RecTripRating.csv')\n",
    "check_file_existence(RecTripRating_filename)\n",
    "    \n",
    "# This file contains information about the rows of Pings_V3_temp_filename that can be used to avoid loading the entire file into a data frame\n",
    "V3_Pings_Index_filename =  os.path.join(OurTable_V3_directory, 'V3_Pings_File_Index.csv')\n",
    "check_file_existence(V3_Pings_Index_filename)\n",
    "    \n",
    "ID_Groups_filename = os.path.join(OurTable_V3_directory,'Cuebiq_ID_Groups.csv')\n",
    "check_file_existence(ID_Groups_filename)\n",
    "\n",
    "######################################################\n",
    "# Pings in the OurTable for a single large draw of IDs TEMPORARY FILE\n",
    "Pings_OurTable_temp_filename = os.path.join(OurTable_V3_directory,'Pings_OurTable_temp.csv')\n",
    "check_file_existence(Pings_OurTable_temp_filename)\n",
    "    \n",
    "# Pings from V3 corresponding with the IDs found in the OurTable \n",
    "Pings_V3_temp_filename = os.path.join(OurTable_V3_directory,'Pings_V3_temp.csv')\n",
    "check_file_existence(Pings_V3_temp_filename)\n",
    "    \n",
    "# Set output file names\n",
    "Indicators_IDs_checked_filename = os.path.join(OurTable_V3_directory, 'IDs_Checked_Indicators_OurTable.csv')\n",
    "check_file_existence(Indicators_IDs_checked_filename)\n",
    "\n",
    "cuebiq_id_list_and_count_filename= os.path.join(OurTable_V3_directory,'cuebiq_id_list_and_count.csv')\n",
    "check_file_existence(cuebiq_id_list_and_count_filename)\n",
    "\n",
    "# List of IDs and dates for V3 query Pings in the OurTable for a single large draw of IDs TEMPORARY FILE\n",
    "OurTable_IDs_and_Dates_filename = os.path.join(OurTable_V3_directory,'OurTable_IDs_and_Dates.csv')\n",
    "check_file_existence(OurTable_IDs_and_Dates_filename)\n",
    "    \n",
    "##########################################################################################\n",
    "############################### PINGS FILES   ##############################################\n",
    "Pings_OurTable_Gulf_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Gulf_ALL.csv')\n",
    "check_file_existence(Pings_OurTable_Gulf_filename)\n",
    "\n",
    "Combined_Pings_OurTable_Gulf_filename= os.path.join(OurTable_V3_directory,'Combined_Pings_OurTable_Gulf_ALL.csv')\n",
    "check_file_existence(Combined_Pings_OurTable_Gulf_filename)\n",
    "\n",
    "Pings_V3_Before_After_filename= os.path.join(OurTable_V3_directory,'Pings_V3_Before_After.csv')\n",
    "check_file_existence(Pings_V3_Before_After_filename)\n",
    "\n",
    "Combined_Pings_V3_Before_After_filename= os.path.join(OurTable_V3_directory,'Combined_Pings_V3_Before_After.csv')\n",
    "check_file_existence(Combined_Pings_V3_Before_After_filename)\n",
    "\n",
    "Pings_OurTable_Gulf_MT19_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Gulf_MT19.csv')\n",
    "check_file_existence(Pings_OurTable_Gulf_MT19_filename)\n",
    "\n",
    "Pings_OurTable_Coast_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Coast.csv')\n",
    "check_file_existence(Pings_OurTable_Coast_filename)\n",
    "\n",
    "Pings_OurTable_Outside_our_wkts_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Outside_our_wkts.csv')\n",
    "check_file_existence(Pings_OurTable_Outside_our_wkts_filename)\n",
    "    \n",
    "##########################################################################################\n",
    "############################### INDICATORS  ##############################################\n",
    "Indicators_filename = os.path.join(OurTable_V3_directory,'Indicators_OurTable.csv')\n",
    "check_file_existence(Indicators_filename)\n",
    "    \n",
    "# cuebiq_id_count_filename= os.path.join(EEZ_V3_directory,'cuebiq_id_count_distribution_EEZ_V3.csv')\n",
    "Indicators_Classified_filename = os.path.join(OurTable_V3_directory,'Indicators_OurTable.Predictions.csv')\n",
    "check_file_existence(Indicators_Classified_filename)\n",
    "\n",
    "Rec_Indicators_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable.csv')\n",
    "check_file_existence(Rec_Indicators_filename)\n",
    "\n",
    "Rec_Indicators_Step1_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable.Step1.csv')\n",
    "check_file_existence(Rec_Indicators_filename)\n",
    "\n",
    "V3_Indicators_filename =  os.path.join(OurTable_V3_directory,'V3_indicators.csv')\n",
    "check_file_existence(V3_Indicators_filename)\n",
    "\n",
    "Rec_indicators_with_V3_filename = os.path.join(OurTable_V3_directory,'rec_indicators_with_V3.csv')\n",
    "check_file_existence(Rec_indicators_with_V3_filename)\n",
    "\n",
    "Indicators_with_V3_indicators_filename= os.path.join(OurTable_V3_directory,'Indicators_with_V3_indicators_indicators.csv')\n",
    "check_file_existence(Indicators_with_V3_indicators_filename)\n",
    "\n",
    "# Rec_Indicators_Selected_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable_Selected.csv')\n",
    "Rec_Indicators_Selected_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable_Selected_May2024.csv')\n",
    "check_file_existence(Rec_Indicators_Selected_filename)\n",
    "\n",
    "Rec_Indicators_Final_All_Exclusions_And_Disappearance_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_Final_All_Exclusions_And_Disappearance.csv')\n",
    "check_file_existence(Rec_Indicators_Final_All_Exclusions_And_Disappearance_filename)\n",
    "\n",
    "# Sorted_Results_file_path = os.path.join(OurTable_V3_directory,'Indicators_EEZ_and_V3.Predictions.sorted.csv')\n",
    "# RecFishing_Results_file_path =  os.path.join(OurTable_V3_directory,'RecFishingBoat Predictions.sorted.csv')\n",
    "\n",
    "DisappearanceIndicators_filename = os.path.join(OurTable_V3_directory,'DisappearanceIndicators.csv')\n",
    "check_file_existence(DisappearanceIndicators_filename)\n",
    "\n",
    "DisappearanceAnalysis_filename = os.path.join(OurTable_V3_directory,'DisappearanceAnalysis.csv')\n",
    "check_file_existence(DisappearanceAnalysis_filename)\n",
    "\n",
    "Stops_Indicators_filename = os.path.join(OurTable_V3_directory,'Stops_Indicators.csv')\n",
    "check_file_existence(Stops_Indicators_filename)\n",
    "\n",
    "Trawls_Indicators_filename = os.path.join(OurTable_V3_directory,'Trawls_Indicators.csv')\n",
    "check_file_existence(Trawls_Indicators_filename)\n",
    "\n",
    "Stop_Trawls_Indicators_filename = os.path.join(OurTable_V3_directory,'Stop_Trawls_Indicators.csv')\n",
    "check_file_existence(Stop_Trawls_Indicators_filename)\n",
    "\n",
    "Combined_Stops_Indicators_filename = os.path.join(OurTable_V3_directory,'Combined_Stops_Indicators.csv')\n",
    "check_file_existence(Combined_Stops_Indicators_filename)\n",
    "\n",
    "Combined_Trawls_Indicators_filename = os.path.join(OurTable_V3_directory,'Combined_Trawls_Indicators.csv')\n",
    "check_file_existence(Combined_Trawls_Indicators_filename)\n",
    "\n",
    "Combined_Stop_Trawls_Indicators_filename = os.path.join(OurTable_V3_directory,'Combined_Stop_Trawls_Indicators.csv')\n",
    "check_file_existence(Combined_Stop_Trawls_Indicators_filename)\n",
    "\n",
    "##########################################################################################\n",
    "############################### Files that COMBINE BATCH01 and newer data ################\n",
    "Combined_indicators_with_disappearance_filename = os.path.join(OurTable_V3_directory,'Combined_indicators_with_disappearance.csv')\n",
    "check_file_existence(Combined_indicators_with_disappearance_filename)\n",
    "\n",
    "#################################################################\n",
    "####################   Results and Analysis #################################################\n",
    "Station_NonStationAnalysis_filename  = os.path.join(Results_directory,'Station_NonStationAnalysis.csv')\n",
    "check_file_existence(Station_NonStationAnalysis_filename)\n",
    "\n",
    "Station_NonStationAnalysis_full_filename  = os.path.join(Results_directory,'Station_NonStationAnalysis_full.csv')\n",
    "check_file_existence(Station_NonStationAnalysis_full_filename)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "####################   Validation Files #################################################\n",
    "ValidationAnalysisCompleteRecTrips_filename = os.path.join(Validation_directory, 'ValidationAnalysisCompleteRecTrips.csv')\n",
    "check_file_existence(Station_NonStationAnalysis_full_filename)\n",
    "\n",
    "ValidationAnalysisNonRecTrips_filename = os.path.join(Validation_directory, 'ValidationAnalysisComplete_Non371_Trips.csv')\n",
    "check_file_existence(ValidationAnalysisNonRecTrips_filename)\n",
    "\n",
    "Non_Rec_Trips_checked_filename = os.path.join(Validation_directory, 'Non_Rec_Trips_checked.csv')\n",
    "check_file_existence(Non_Rec_Trips_checked_filename)\n",
    "\n",
    "ValidationAnalysis_sites_With_Stops_filename = os.path.join(Validation_directory,'ValidationAnalysis_sites_With_Stops.csv')\n",
    "check_file_existence(Non_Rec_Trips_checked_filename)\n",
    "\n",
    "##########################################################################################\n",
    "############################### WEATHER data files ####################################\n",
    "Buoys_file_path  = os.path.join(Weather_Data_directory,'Buoys.csv')\n",
    "check_file_existence(Buoys_file_path)\n",
    "\n",
    "Weather_file_path  = os.path.join(Weather_Data_directory,'DailyWeatherData.csv')\n",
    "check_file_existence(Weather_file_path)\n",
    "\n",
    "##########################################################################################\n",
    "############################### SUPPLEMENTARY MAP DATA  ############################\n",
    "Industrial_polygons_filename  = os.path.join(Uploaded_Data_directory,'Polygons Around Industrial Sites.wkt')\n",
    "check_file_existence(Industrial_polygons_filename)\n",
    "\n",
    "\n",
    "station_points_filename = os.path.join(Uploaded_Data_directory,'LA_TX_Union_Station_WGS84.csv')\n",
    "check_file_existence(station_points_filename)\n",
    "\n",
    "MRIP_station_points_filename =os.path.join(Uploaded_Data_directory,'MRIP_stations.csv')\n",
    "check_file_existence(MRIP_station_points_filename)\n",
    "\n",
    "FishBrain_filename =os.path.join(Fishbrain_directory,'Fishbrain_public_offshore_catches_at_gulfofmexico_2019-2021_shared-dataset.csv')\n",
    "check_file_existence(FishBrain_filename)\n",
    "\n",
    "FishAngler_filename =os.path.join(FishAngler_directory,'captures-2018-to-2022.csv')\n",
    "check_file_existence(FishAngler_filename)\n",
    "\n",
    "# TX_Roving_Station_Lists_filename = os.path.join(Uploaded_Data_directory,'Fishing Site List - Roving and Creel Sites (from Stata) - with state, county, and zcta codes - final (from Jesse Backstrom).csv')\n",
    "# check_file_existence(TX_Roving_Station_Lists_filename)\n",
    "TX_Roving_Counts_filename = os.path.join(Uploaded_Data_directory,'TPWD roving counts.csv')\n",
    "check_file_existence(TX_Roving_Counts_filename)\n",
    "\n",
    "MRIP_trips_filename = os.path.join(Uploaded_Data_directory,'AL_MS_MRIPTripData.csv')\n",
    "check_file_existence(MRIP_trips_filename)\n",
    "\n",
    "\n",
    "Gardner_Point_filename = os.path.join(Travel_Cost_Shapefile_directory,'GOM_AR_points.csv')\n",
    "check_file_existence(TX_Roving_Counts_filename)\n",
    "\n",
    "##########################################################################################\n",
    "############################### AIS Files INCLUDING CLASSIFIER ############################\n",
    "RF_Classfier_filename = os.path.join(AIS_Directory, 'rf_model_AIS_2019.pkl')\n",
    "check_file_existence(RF_Classfier_filename)\n",
    "\n",
    "RF_Importance_Factors_filename = os.path.join(AIS_Directory, 'rf_classifier_importance_factors.csv')\n",
    "check_file_existence(RF_Importance_Factors_filename)\n",
    "\n",
    "AIS_Predictions_filename = os.path.join(AIS_Directory, 'RandomForest_Predictions2019AISData.csv')\n",
    "check_file_existence(AIS_Predictions_filename)\n",
    "\n",
    "\n",
    "### Dedicate Table Names for reference\n",
    "# Dedicated table with all Pings within the Gulf WKT for 1/12019 - 4/22/2022\n",
    "#  Table Name:  dedicated.ScheduledExecution5.DeviceTable   \n",
    "#  Code used for call:  RecFishing/DataflowStudioJobs/ScheduledEx5-updated.ipynb\n",
    "\n",
    "# Dedicated table with all Pings within the Gulf WKT AND Origin for 1/12019 - 4/22/2022\n",
    "#  Table Name:  dedicated.ScheduledExecution5_parallel_origin.DeviceTable\n",
    "#  Code used for call:  RecFishing/DataflowStudioJobs/ScheduledEx5-origin.ipynb\n",
    "\n",
    "##########################################################################################\n",
    "############################### Results & Analysis ############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b919be9d-511a-4ab1-b071-5394663bdcca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Date and Distance Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb110de-d149-4bbc-bf04-fb66a6006713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE GENERATE THE SEASON BASED ON MONTH\n",
    "\n",
    "# def month_to_season(month_num):\n",
    "    \n",
    "#     monthMar= (month_num>1)*(month_num-1) + (month_num<=1)*(11+month_num)\n",
    "#     season=int((monthMar) / 3)+1\n",
    "    \n",
    "#     return season\n",
    "# month 6 and 1 are wrong\n",
    "\n",
    "def month_to_season(month_num):\n",
    "    if (month_num == 11) | (month_num == 0) | (month_num == 1): # DEC-FEB\n",
    "        season = 4\n",
    "    if (month_num == 2) | (month_num == 3) | (month_num == 4): # MARCH - MAY\n",
    "        season = 1\n",
    "    if (month_num == 5) | (month_num == 6) | (month_num == 7): # JUNE - August \n",
    "        season = 2\n",
    "    if (month_num == 8) | (month_num == 9) | (month_num == 10): # SEP- NOV\n",
    "        season = 3\n",
    "    return season\n",
    "\n",
    "# check season function\n",
    "season= month_to_season(1)\n",
    "season\n",
    "\n",
    "# note 0 equals Jan and 11 equals december \n",
    "def monthofyear(EpochTime):  # January = 0\n",
    "    Base = 1546322400  # Jan. 1, 2019, 12:00 a.m. \n",
    "    Jan12021 = 731\n",
    "    Jan12022 = 1096\n",
    "    Jan12023 = 1461\n",
    "\n",
    "    FebStart = 31\n",
    "    MarStart = 59\n",
    "    AprStart = 90\n",
    "    MayStart = 120\n",
    "    JunStart = 151\n",
    "    JulStart = 181\n",
    "    AugStart = 212\n",
    "    SepStart = 243\n",
    "    OctStart = 273\n",
    "    NovStart = 304\n",
    "    DecStart = 334\n",
    "    leapyear2020 = 1582869600     # Feb 29, 2020\n",
    "\n",
    "    days_since_2019 = epoch_to_days_since_1_1_2019(EpochTime)\n",
    "    leapyearadjust = -1*(EpochTime >leapyear2020)\n",
    "    year = int((days_since_2019+ leapyearadjust)/365)\n",
    "    dayofyear = (days_since_2019) - 365*year +  leapyearadjust\n",
    "    month = 1\n",
    "    month = 1*(dayofyear>=FebStart) + \\\n",
    "            1*(dayofyear>=    MarStart ) + \\\n",
    "            1*(dayofyear>=    AprStart ) + \\\n",
    "            1*(dayofyear>=    MayStart ) + \\\n",
    "            1*(dayofyear>=    JunStart ) + \\\n",
    "            1*(dayofyear>=    JulStart ) + \\\n",
    "            1*(dayofyear>=    AugStart ) + \\\n",
    "            1*(dayofyear>=    SepStart ) + \\\n",
    "            1*(dayofyear>=    OctStart ) + \\\n",
    "            1*(dayofyear>=    NovStart ) + \\\n",
    "            1*(dayofyear>=    DecStart )\n",
    "    return month\n",
    "        \n",
    "\n",
    "def epoch_to_season(EpochTime):\n",
    "    month = monthofyear(EpochTime)\n",
    "    season = month_to_season(month)\n",
    "    return season\n",
    "\n",
    "def epoch_to_DOW(EpochTime):\n",
    "    Base = 1546322400  # Jan. 1, 2019, 12:00 a.m. \n",
    "    BaseDOW = 1\n",
    "    DaySince = (EpochTime - Base) / (24*60*60)\n",
    "    WeeksSinceBase = DaySince / 7\n",
    "    DayOfWeek = BaseDOW + int((WeeksSinceBase - int(WeeksSinceBase)) * 7)\n",
    "    return DayOfWeek\n",
    "\n",
    "def epoch_to_days_since_1_1_2019(EpochTime):\n",
    "    Base = 1546322400  # Jan. 1, 2019, 12:00 a.m. \n",
    "    DaySinceBase = int((EpochTime-Base) / (60*60*24))\n",
    "    return DaySinceBase\n",
    "\n",
    "def AISdate_to_epoch(AISDate):\n",
    "    from datetime import datetime\n",
    "\n",
    "    # date_string = \"2019-06-01T16:14:14\"\n",
    "\n",
    "    # Define the format of the input date string\n",
    "    date_format = \"%Y-%m-%dT%H:%M:%S\"\n",
    "\n",
    "    # Convert the date string to a datetime object\n",
    "    dt_object = datetime.strptime(AISDate, date_format)\n",
    "\n",
    "    # Convert the datetime object to epoch time\n",
    "    epoch_time = int(dt_object.timestamp())\n",
    "\n",
    "    # print(\"Epoch Time:\", epoch_time)\n",
    "    return epoch_time\n",
    "\n",
    "def epoch_to_hour_of_day(EpochTime):\n",
    "    Base = 1546322400  # Jan. 1, 2019, 12:00 a.m. \n",
    "    # Daylight Savings Time points for US Central Time\n",
    "    start2019 = 1552204800  \n",
    "    end2019 = 1572768000\n",
    "    start2020 = 1583654400\n",
    "    end2020 = 1604217600\n",
    "    start2021 = 1615708800\n",
    "    end2021 = 1636272000\n",
    "    start2022 = 1647158400\n",
    "    end2022 = 1667721600\n",
    "    start2023 = 1678608000\n",
    "    end2023 = 1699171200\n",
    "\n",
    "    \n",
    "    DayLightSavingsAdjust = +1 * (EpochTime > start2019) + \\\n",
    "                            -1 * (EpochTime > end2019) + \\\n",
    "                            +1 * (EpochTime > start2020) + \\\n",
    "                            -1 * (EpochTime > end2020) + \\\n",
    "                            +1 * (EpochTime > start2021) + \\\n",
    "                            -1 * (EpochTime > end2021) + \\\n",
    "                            +1 * (EpochTime > start2022) + \\\n",
    "                            -1 * (EpochTime > end2022) + \\\n",
    "                            +1 * (EpochTime > start2023) + \\\n",
    "                            -1 * (EpochTime > end2023)\n",
    "#    print(DayLightSavingsAdjust)\n",
    "    DaysSince = ((EpochTime-Base) / (60*60*24))\n",
    "    PortionOfDay = DaysSince - int(DaysSince)\n",
    "    HourOfDay = int(PortionOfDay*24) + DayLightSavingsAdjust\n",
    "    return HourOfDay\n",
    "\n",
    "def epoch_to_date(epoch_time):\n",
    "    if isinstance(epoch_time, pd.Series):\n",
    "        # If input is a Series, apply the function to each element\n",
    "        return epoch_time.apply(epoch_to_date)\n",
    "    else:\n",
    "        # Convert epoch time to a datetime object\n",
    "        epoch_time = int(epoch_time)\n",
    "        dt = datetime.fromtimestamp(epoch_time)\n",
    "\n",
    "        # Format the datetime as 'YYYY-MM-DD'\n",
    "        formatted_date = dt.strftime('%Y-%m-%d')\n",
    "        return formatted_date\n",
    "    \n",
    "# def epoch_to_date(epoch_time):\n",
    "#     # Convert epoch time to a datetime object\n",
    "#     epoch_time=int(epoch_time)\n",
    "#     dt = datetime.fromtimestamp(epoch_time)\n",
    "#     print(epoch_time, dt)\n",
    "    \n",
    "#     # Format the datetime as 'YYYY-MM-DD'\n",
    "#     formatted_date = dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "#     return formatted_date\n",
    "\n",
    "def epoch_to_datetime(epoch_time):\n",
    "    return datetime.fromtimestamp(epoch_time)\n",
    "\n",
    "def date_to_epoch(date_string, date_format='%m/%d/%Y'):\n",
    "    \"\"\"\n",
    "    Convert a date string to epoch time integer.\n",
    "    \n",
    "    Parameters:\n",
    "        date_string (str): The date string to convert.\n",
    "        date_format (str): The format of the date string. Default is '%m/%d/%Y'.\n",
    "    \n",
    "    Returns:\n",
    "        int: Epoch time integer.\n",
    "    \"\"\"\n",
    "    # Parse the date string to a datetime object\n",
    "    date_obj = datetime.strptime(date_string, date_format)\n",
    "    \n",
    "    # Convert the datetime object to epoch time\n",
    "    epoch_time = int(date_obj.timestamp())\n",
    "    \n",
    "    return epoch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec950a-6d79-42d4-9a5e-dec6f63c0ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates_sequence(\n",
    "    date_start, \n",
    "    date_end, \n",
    "    date_format\n",
    "):\n",
    "    return [\n",
    "        (datetime.strptime(date_start, date_format) + timedelta(days=x)).strftime(date_format)\n",
    "        for x in range (\n",
    "        0,\n",
    "        (datetime.strptime(date_end, date_format) - datetime.strptime(date_start, date_format) + timedelta(days=1)).days\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0adbab-1a4a-42b8-b8bb-dea678bf7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format = \"%Y%m%d\"\n",
    "\n",
    "first_date = \"20190101\"\n",
    "last_date_to_compute = \"20220422\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346c10f-1cba-4528-aef9-360ca2ae0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2, to_radians=True, earth_radius=6371):\n",
    "    if to_radians:\n",
    "        lat1, lon1, lat2, lon2 = np.radians([lat1, lon1, lat2, lon2])\n",
    "\n",
    "    a = np.sin((lat2-lat1)/2.0)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin((lon2-lon1)/2.0)**2\n",
    "\n",
    "    return earth_radius * 2 * np.arcsin(np.sqrt(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe2104-d2e3-444f-aaa9-ff5864ac16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_speed(df):\n",
    "    # Sort the DataFrame based on 'event_timestamp'\n",
    "    df.sort_values(by='event_timestamp', inplace=True)\n",
    "    \n",
    "    # Calculate distances and speeds\n",
    "    df['dist_fwd'] = haversine(df['lat'], df['lng'], df['lat'].shift(1), df['lng'].shift(1))\n",
    "    df['time_fwd'] = df['event_timestamp'] - df['event_timestamp'].shift(1)\n",
    "    df['ping_speed_fwd'] = 60 * abs(df['dist_fwd'] / (0.000001 + df['time_fwd']))\n",
    "    df['ping_speed_fwd'].iloc[0] = 0.0  # KM/minute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7105a7-d897-4183-935d-3e704e6d2b14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## H3 Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5311de9b-42b6-4f59-81ac-3921a0384509",
   "metadata": {},
   "source": [
    "#### Default resolution for all H3 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e8904-efed-4f8e-b614-e97390731fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolution 10 has an average edge length of 75.86 meters, meaning, the distance between two opposite vertices is 151.5 meters\n",
    "resolution = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b9e84-51b6-46e7-ab10-d1f993c47005",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Function to get the h3 cell for a complete data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c42af-bb1c-4a51-8d7d-13d69812a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h3\n",
    "from shapely.geometry import Point\n",
    "import h3\n",
    "\n",
    "import h3\n",
    "import pandas as pd\n",
    "\n",
    "# Optimized function to get H3 cells for a GeoDataFrame and return both the set and modified dataframe\n",
    "def get_h3_cells_set_and_dataframe(dataframe, resolution):\n",
    "    # Calculate the H3 cells for all rows in one go\n",
    "    dataframe['h3_cell'] = [\n",
    "        h3.latlng_to_cell(lat, lng, resolution) \n",
    "        for lat, lng in zip(dataframe['lat'], dataframe['lng'])\n",
    "    ]\n",
    "    \n",
    "    # Create the set of unique H3 cells directly from the 'h3_cell' column\n",
    "    h3_cells = set(dataframe['h3_cell'])\n",
    "    \n",
    "    return h3_cells, dataframe\n",
    "\n",
    "# Function to get H3 cells for a GeoDataFrame\n",
    "def get_h3_cells_for_dataframe(dataframe, resolution):\n",
    "    h3_cells = set()\n",
    "    for index, row in dataframe.iterrows():\n",
    "        point = Point(row['lng'], row['lat'])\n",
    "        h3_cells.add(h3.latlng_to_cell(point.y, point.x, resolution))\n",
    "    return h3_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66c6907-3652-437c-a44b-fc4e583fac9a",
   "metadata": {},
   "source": [
    "### Function to add columns with h3_cell and neighbors to a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d56f8-dafc-4b80-a4c4-afaeb7fb46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get H3 cells and their neighbors\n",
    "def add_h3_and_neighbors_to_dataframe(dataframe, resolution):\n",
    "    # Calculate H3 cells for each row (assuming 'lat' and 'lng' columns are present)\n",
    "    dataframe['h3_cell'] = [\n",
    "        h3.latlng_to_cell(lat, lng, resolution) \n",
    "        for lat, lng in zip(dataframe['lat'], dataframe['lng'])\n",
    "    ]\n",
    "    \n",
    "    # Function to get the neighbors for each H3 cell\n",
    "    def get_neighbors(h3_cell):\n",
    "        return list(h3.grid_disk(h3_cell, 1))  # 1 indicates the 1-ring neighbors\n",
    "\n",
    "    # Add a column for neighbors\n",
    "    dataframe['h3_neighbors'] = dataframe['h3_cell'].apply(get_neighbors)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# Example usage with AL_MRIP_station_points_df and a resolution of 9\n",
    "TEST_df= pd.DataFrame({\n",
    "    'lat': [37.7749, 34.0522, 40.7128],  # Example latitudes\n",
    "    'lng': [-122.4194, -118.2437, -74.0060]  # Example longitudes\n",
    "})\n",
    "\n",
    "# Add H3 cell and neighbors to the dataframe\n",
    "TEST_df = add_h3_and_neighbors_to_dataframe(TEST_df, resolution)\n",
    "\n",
    "# Print the dataframe with the new columns\n",
    "# print(TEST_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badbfbda-26ca-406f-8fc1-e6d1bfcd5941",
   "metadata": {},
   "source": [
    "### Function that compares columns in two data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75738bef-d4d1-4856-94d6-904775a3775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompareColumsInTwoDataFrames(df1, df2):\n",
    "    # Get the current frame\n",
    "    frame = inspect.currentframe()\n",
    "    # Get the arguments from the caller's frame\n",
    "    args, _, _, values = inspect.getargvalues(frame.f_back)\n",
    "\n",
    "    # Extract the names of the arguments\n",
    "    df1_name = [name for name in values if values[name] is df1][0]\n",
    "    df2_name = [name for name in values if values[name] is df2][0]\n",
    "\n",
    "    columns_only_in_1 = list(set(df1.columns) - set(df2.columns))\n",
    "    columns_only_in_1 = sorted(columns_only_in_1)\n",
    "\n",
    "    # Get the list of columns in Batch01_merged_df that are not in indicators_df\n",
    "    columns_only_in_2 = list(set(df2.columns) - set(df1.columns))\n",
    "    columns_only_in_2 = sorted(columns_only_in_2)\n",
    "\n",
    "    common_in_both  = list(set(df1.columns).intersection(set(df2.columns)))\n",
    "    common_in_both = sorted(common_in_both)\n",
    "    \n",
    "    if len(columns_only_in_1)>0:\n",
    "        print(\" \")\n",
    "        print(\"Columns in \", df1_name, \"that aren't in\", df2_name,\":\")\n",
    "        print(columns_only_in_1)\n",
    "    else:\n",
    "        print(\"There are no columns in \", df1_name, \"that aren't in\", df2_name)\n",
    "    print(\" \")\n",
    "        \n",
    "\n",
    "    if len(columns_only_in_2)>0:\n",
    "        print(\" \")\n",
    "        print(\"Columns in \", df2_name, \"that aren't in\", df1_name,\":\")\n",
    "        print(columns_only_in_2)\n",
    "    else:\n",
    "        print(\"There are no columns in \", df2_name, \"that aren't in\", df1_name)\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"Columns in both\", df1_name, \"and\", df2_name, \":\", common_in_both)\n",
    "\n",
    "# CompareColumsInTwoDataFrames(Batch01_ind1_df, indicators_df)\n",
    "# indicators_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a746f9c-b444-458a-be30-26f5a6f04059",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Errant pings code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68824ef-3059-43c3-952c-364bd6ed4adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2, to_radians=True, earth_radius=6371):\n",
    "    if to_radians:\n",
    "        lat1, lon1, lat2, lon2 = np.radians([lat1, lon1, lat2, lon2])\n",
    "\n",
    "    a = np.sin((lat2-lat1)/2.0)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin((lon2-lon1)/2.0)**2\n",
    "\n",
    "    return earth_radius * 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def EliminateErrantPingsSpeed(pings_df, mph_limit):\n",
    "    km_per_min_limit = mph_limit*(0.0268224)\n",
    "    # Calculate speed moving forward e.g., row 0 is time since previous trip\n",
    "\n",
    "    pings_df.sort_values(by='event_timestamp', inplace=True)\n",
    "    pings_df = pings_df.drop_duplicates()\n",
    "\n",
    "\n",
    "    # create (or recreate) the time difference variables\n",
    "    pings_df['time_diff_minutes_from_previous'] = pings_df[\"event_timestamp\"].diff()/60.0\n",
    "    pings_df['time_diff_minutes_from_previous'].fillna(value=0, inplace=True)\n",
    "\n",
    "    pings_df['time_diff_minutes_to_next'] = pings_df[\"event_timestamp\"].diff(-1)/60.0\n",
    "    pings_df['time_diff_minutes_to_next'].fillna(value=99999, inplace=True)\n",
    "\n",
    "    \n",
    "    pings_df_shifted_down = pings_df.shift(1)\n",
    "    pings_df['dist_fwd'] = haversine(pings_df['lat'], pings_df['lng'], pings_df_shifted_down['lat'], pings_df_shifted_down['lng'])\n",
    "    pings_df['ping_speed_fwd'] = abs(pings_df['dist_fwd']/(0.00001+pings_df['time_diff_minutes_from_previous']))\n",
    "    pings_df['ping_speed_fwd'].fillna(value=0, inplace=True)\n",
    "\n",
    "    # Calculate speed moving backward e.g., first row is the speed to the next ping\n",
    "    pings_df_shifted_up = pings_df.shift(-1)\n",
    "    pings_df['dist_bkwd'] = haversine(pings_df['lat'], pings_df['lng'], pings_df_shifted_up['lat'], pings_df_shifted_up['lng'])\n",
    "    pings_df['ping_speed_bkwd'] = abs(pings_df['dist_bkwd']/(0.00001+pings_df['time_diff_minutes_to_next']))\n",
    "    pings_df['ping_speed_bkwd'].fillna(value=0, inplace=True)\n",
    "\n",
    "    pings_df['Avg_ping_speed'] = (pings_df['ping_speed_fwd'] + pings_df['ping_speed_bkwd']) / 2\n",
    "    pings_df['row_index'] = pings_df.reset_index().index\n",
    "\n",
    "    # Step 2: Check if the maximum of ping_speed > km_per_min_limit\n",
    "    iIteration=0\n",
    "    while len(pings_df) > 2 and pings_df['Avg_ping_speed'].max() > km_per_min_limit:\n",
    "        iIteration=iIteration+1\n",
    "\n",
    "        max_index = pings_df['Avg_ping_speed'].idxmax()\n",
    "    \n",
    "        # Step 4: Recalculate ping_speed_fwd for the row after the row that was dropped\n",
    "        if max_index + 1 < len(pings_df) and max_index - 1 >= 0:\n",
    "            lat_after = pings_df.iloc[max_index+1]['lat']\n",
    "            lon_after = pings_df.iloc[max_index+1]['lng']\n",
    "            lat_before = pings_df.iloc[max_index - 1]['lat']\n",
    "            lon_before = pings_df.iloc[max_index - 1]['lng']\n",
    "            distance = haversine(lat_before, lon_before, lat_after, lon_after)\n",
    "            time_diff = pings_df.iloc[max_index+1]['event_timestamp']-pings_df.iloc[max_index-1]['event_timestamp']\n",
    "            new_speed = distance /time_diff\n",
    "            \n",
    "            # Calculate new fwd speed for the row before\n",
    "            index_before = max_index - 1\n",
    "            index_after = max_index + 1\n",
    "            \n",
    "            # Update the value using .loc[] or .iloc[] with a single call\n",
    "            pings_df.loc[index_before, 'ping_speed_fwd'] = new_speed\n",
    "            pings_df.loc[index_after, 'ping_speed_bkwd'] = new_speed\n",
    "\n",
    "            pings_df = pings_df[pings_df['event_timestamp'].notna() & (pings_df['event_timestamp'] != '')]\n",
    "\n",
    "        \n",
    "        ################ Debugging ###############\n",
    "        pings_df = pings_df.drop(max_index)\n",
    "        pings_df['Avg_ping_speed'] = (pings_df['ping_speed_fwd'] + pings_df['ping_speed_bkwd']) / 2\n",
    "\n",
    "        # Reset index (I don't know if this is really necessary)\n",
    "        pings_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return pings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424eb0b4-3ab0-476f-a40b-29d16e495c21",
   "metadata": {},
   "source": [
    "### Function to merge Trip DFs\n",
    "Merges two data frames based on cuebiq_id and Trip_number. If the list of IDs is not identical, this returns an empty data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f0126e-fac2-4c0b-9106-21ffb24dca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_Trip_dfs(df1, df2):\n",
    "    # Check if the lists of values of cuebiq_id are the same in both data frames\n",
    "    if set(df1['cuebiq_id']) != set(df2['cuebiq_id']):\n",
    "        print(\"The two data frames do not have the same values of cuebiq_id\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Identify common columns, excluding 'cuebiq_id' and 'Trip_number'\n",
    "    common_cols = [col for col in df1.columns if col in df2.columns and col not in ['cuebiq_id', 'Trip_number']]\n",
    "    \n",
    "    # Drop common columns from df2\n",
    "    df2 = df2.drop(columns=common_cols)\n",
    "    \n",
    "    # Merge the data frames on 'cuebiq_id' and 'Trip_number'\n",
    "    merged_df = pd.merge(df1, df2, on=['cuebiq_id', 'Trip_number'], how='inner')\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f13305-ba8a-4a1d-b950-abe1d51260bb",
   "metadata": {},
   "source": [
    "### Function that gets H3 neighbors, and the neighbors of the neighbors -- a double ring around the original points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284cdec-c7af-4a84-ad2b-869ada76a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h3_cells_and_neighbors_two_levels(dataframe, resolution):\n",
    "    h3_cells_and_neighbors = set()\n",
    "    for index, row in dataframe.iterrows():\n",
    "        point = Point(row['lng'], row['lat'])\n",
    "        h3_cell = h3.latlng_to_cell(point.y, point.x, resolution)\n",
    "        neighbors = h3.grid_disk(h3_cell, 1)\n",
    "        \n",
    "        # Add the original H3 cell and its neighbors at the current resolution\n",
    "        h3_cells_and_neighbors.add(h3_cell)\n",
    "        h3_cells_and_neighbors.update(neighbors)\n",
    "        \n",
    "        # Add neighbors of neighbors at the same resolution\n",
    "        for neighbor in neighbors:\n",
    "            second_layer_neighbors = h3.grid_disk(neighbor, 1)\n",
    "            h3_cells_and_neighbors.update(second_layer_neighbors)\n",
    "\n",
    "    return h3_cells_and_neighbors\n",
    "\n",
    "def get_h3_cells_and_neighbors(dataframe, resolution):\n",
    "    h3_cells_and_neighbors = set()\n",
    "    for index, row in dataframe.iterrows():\n",
    "        point = Point(row['lng'], row['lat'])\n",
    "        h3_cell = h3.latlng_to_cell(point.y, point.x, resolution)\n",
    "        neighbors = h3.grid_disk(h3_cell, 1)\n",
    "\n",
    "        # Add both the original H3 cell and its neighbors at lower resolution to the set\n",
    "        h3_cells_and_neighbors.add(h3_cell)\n",
    "        h3_cells_and_neighbors.update(neighbors)\n",
    "\n",
    "    return h3_cells_and_neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19386eec-0cf4-476d-9c46-1b4c7cabdac0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Auxiliary Files -- polygons, stations, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec6e82-4438-4766-85f3-c3ae713de67b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load stations (MRIP & TPWD) and find H3 cells and neighbors to those cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a929f4-89ca-4b20-8ec0-005a245ada95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataframes\n",
    "\n",
    "# Texas -- Data provided Mark Fisher <Mark.Fisher@tpwd.texas.gov> on 6/3/2022\n",
    "TX_station_points_df = pd.read_csv('../../uploaded_files/TPWD Stations.csv', encoding='latin-1')\n",
    "TX_station_points_df.rename(columns={'y': 'lat', 'x': 'lng'}, inplace=True)\n",
    "TX_station_points_df.dropna(subset=['lat'], inplace=True)  # Drop empty rows\n",
    "\n",
    "# MRIP MS, AL & FL -- downloaded from the site NOAA Site Register\n",
    "MRIP_station_points_df = pd.read_csv('../../uploaded_files/MRIP-sites-LA-AL-MS.csv')\n",
    "MRIP_station_points_df.rename(columns={'SITE_LAT': 'lat', 'SITE_LONG': 'lng'}, inplace=True)\n",
    "\n",
    "AL_MRIP_station_points_df = MRIP_station_points_df[MRIP_station_points_df['STATE_CODE'] == 1].copy()\n",
    "MS_MRIP_station_points_df = MRIP_station_points_df[MRIP_station_points_df['STATE_CODE'] == 28].copy()\n",
    "AL_MS_MRIP_station_points_df = MRIP_station_points_df[(MRIP_station_points_df['STATE_CODE'] == 28) |\n",
    "                                                      (MRIP_station_points_df['STATE_CODE'] == 1)].copy()\n",
    "\n",
    "\n",
    "FL_MRIP_station_points_df = pd.read_csv('../../uploaded_files/FL Sites -- MRIP Site Registry Escambia and Santa Rosa Counties.csv')\n",
    "FL_MRIP_station_points_df.rename(columns={'SITE_LAT': 'lat', 'SITE_LONG': 'lng'}, inplace=True)\n",
    "FL_MRIP_station_points_df = FL_MRIP_station_points_df[FL_MRIP_station_points_df['STATUS'] == \"Active\"].copy()\n",
    "\n",
    "# LA Creel Stations -- personal communication from Nicole Smith (WLF) <nsmith@wlf.la.gov> on 12/13/23\n",
    "LA_CREEL_station_points_df = pd.read_csv('../../uploaded_files/LA creel sites-1.csv')\n",
    "LA_CREEL_station_points_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lng'}, inplace=True)\n",
    "LA_CREEL_station_points_df = LA_CREEL_station_points_df[LA_CREEL_station_points_df['Active_Flg'] == 1].copy()\n",
    "LA_CREEL_station_points_df = LA_CREEL_station_points_df[LA_CREEL_station_points_df['lat'] >0].copy()\n",
    "\n",
    "# All Ports in GOM Gathered from Marine Traffic Website\n",
    "MarineTrafficPorts_df = pd.read_csv('../../uploaded_files/MarineTrafficPorts.csv')\n",
    "MarineTrafficPorts_df.rename(columns={'lon': 'lng'}, inplace=True)\n",
    "\n",
    "LargePorts_Marine_traffic_df = MarineTrafficPorts_df[MarineTrafficPorts_df['MarineTrafficPortType_num'] == 3].copy()\n",
    "Medium_Anchorage_Marine_traffic_df = MarineTrafficPorts_df[MarineTrafficPorts_df['MarineTrafficPortType_num'] == 4].copy()\n",
    "Medium_Marina_Marine_traffic_df = MarineTrafficPorts_df[MarineTrafficPorts_df['MarineTrafficPortType_num'] == 5].copy()\n",
    "Medium_Port_Marine_traffic_df = MarineTrafficPorts_df[MarineTrafficPorts_df['MarineTrafficPortType_num'] == 7].copy()\n",
    "Small_Marina_Marine_traffic_df = MarineTrafficPorts_df[MarineTrafficPorts_df['MarineTrafficPortType_num'] == 9].copy()\n",
    "Small_Port_Marine_traffic_df = MarineTrafficPorts_df[MarineTrafficPorts_df['MarineTrafficPortType_num'] == 10].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1385e39-0413-4cc1-88e3-a60227eaa13d",
   "metadata": {},
   "source": [
    "### Create H3 cells around station points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab91d55e-e7c6-4c22-81fc-756ba1dc0f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the resolution before proceedin\n",
    "print(\"This cell creates the df's and lists of h3 cells for use elsewhere\")\n",
    "confirmation = input(f\"The resolution is set at {resolution}. Are you sure you want to proceed? Type 'Y' to confirm: \").strip().upper()\n",
    "\n",
    "if confirmation == 'Y':\n",
    "    print(\"Creating dataframes and lists\")\n",
    "else:\n",
    "    print(\"Operation cancelled.\")\n",
    "\n",
    "# The maps of stations\n",
    "state_station_points_df = pd.read_csv('../../uploaded_files/LA_TX_Union_Station_WGS84.csv')\n",
    "\n",
    "# MRIP_station_points_df = pd.read_csv('../uploaded_files/MRIP_stations.csv')\n",
    "MRIP_station_points_df = pd.read_csv('../../uploaded_files/MRIP-sites-LA-AL-MS.csv')\n",
    "\n",
    "MRIP_station_points_df.rename(columns={'SITE_LAT': 'lat', 'SITE_LONG': 'lng'}, inplace=True)\n",
    "\n",
    "# Create sets of H3 cells and their neighbors for each dataframe\n",
    "state_station_h3_cells_alone = get_h3_cells_for_dataframe(state_station_points_df, resolution)\n",
    "\n",
    "mrip_station_h3_cells_alone = get_h3_cells_for_dataframe(MRIP_station_points_df, resolution)\n",
    "\n",
    "\n",
    "state_station_h3_cells = get_h3_cells_and_neighbors(state_station_points_df, resolution)\n",
    "mrip_station_h3_cells = get_h3_cells_and_neighbors(MRIP_station_points_df, resolution)\n",
    "\n",
    "TX_station_points_h3_cells = get_h3_cells_and_neighbors(TX_station_points_df, resolution)\n",
    "AL_MRIP_station_points_h3_cells= get_h3_cells_and_neighbors(AL_MRIP_station_points_df, resolution)\n",
    "MS_MRIP_station_points_h3_cells= get_h3_cells_and_neighbors(MS_MRIP_station_points_df, resolution)\n",
    "FL_MRIP_station_points_h3_cells = get_h3_cells_and_neighbors(FL_MRIP_station_points_df, resolution)\n",
    "LA_CREEL_station_points_h3_cells= get_h3_cells_and_neighbors(LA_CREEL_station_points_df, resolution)\n",
    "LargePorts_Marine_traffic_h3_cells= get_h3_cells_and_neighbors(LargePorts_Marine_traffic_df, resolution)\n",
    "Medium_Anchorage_Marine_traffic_h3_cells= get_h3_cells_and_neighbors(Medium_Anchorage_Marine_traffic_df, resolution)\n",
    "Medium_Marina_Marine_traffic_h3_cells= get_h3_cells_and_neighbors(Medium_Marina_Marine_traffic_df, resolution)\n",
    "Medium_Port_Marine_traffic_h3_cells= get_h3_cells_and_neighbors(Medium_Port_Marine_traffic_df, resolution)\n",
    "Small_Marina_Marine_traffic_h3_cells= get_h3_cells_and_neighbors(Small_Marina_Marine_traffic_df, resolution)\n",
    "Small_Port_Marine_traffic_h3_cells= get_h3_cells_and_neighbors(Small_Port_Marine_traffic_df, resolution)\n",
    "\n",
    "# Combine all the h3 cells into a single set\n",
    "combined_h3_cells = state_station_h3_cells | mrip_station_h3_cells\n",
    "combined_h3_cells_alone = state_station_h3_cells_alone | mrip_station_h3_cells_alone\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d58b8-dde2-485a-a5ff-deafcff2115b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### State polygons for identifying state of starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac918895-cdb5-450d-9a2b-d03209be6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.wkt import loads\n",
    "\n",
    "## WKT strings from <script src=\"https://gist.github.com/JoshuaCarroll/49630cbeeb254a49986e939a26672e9c.js\"></script>\n",
    "\n",
    "# # Test on a point on a rectangular state\n",
    "# wkt_string = \"POLYGON((-109.0448 37.0004,-102.0424 36.9949,-102.0534 41.0006,-109.0489 40.9996,-109.0448 37.0004,-109.0448 37.0004))\"\n",
    "# colorado_polygon = loads(wkt_string)\n",
    "# min_lat,min_lng,max_lat,max_lng=colorado_polygon.bounds\n",
    "\n",
    "# avg_lat = .5*(min_lat+max_lat)\n",
    "# avg_lng =  .5*(min_lng+max_lng)\n",
    "# avg_point = Point(avg_lat,avg_lng)\n",
    "\n",
    "# is_within_polygon = avg_point.within(colorado_polygon)\n",
    "# print(avg_point,\"is_within_COLORADO\",is_within_polygon)\n",
    "\n",
    "\n",
    "# Define the WKT string for the TX polygon\n",
    "TX_wkt_string = \"POLYGON((-106.5715 31.8659,-106.5042 31.7504,-106.3092 31.6242,-106.2103 31.4638,-106.0181 31.3912,-105.7874 31.1846,-105.5663 31.0012,-105.4015 30.8456,-105.0032 30.6462,-104.8521 30.3847,-104.7437 30.2591,-104.6915 30.0738,-104.6777 29.9169,-104.5679 29.7644,-104.5280 29.6475,-104.4044 29.5603,-104.2067 29.4719,-104.1559 29.3834,-103.9774 29.2948,-103.9128 29.2804,-103.8208 29.2481,-103.5640 29.1378,-103.4692 29.0682,-103.3154 29.0105,-103.1616 28.9601,-103.0957 29.0177,-103.0298 29.1330,-102.8677 29.2157,-102.8979 29.2565,-102.8375 29.3570,-102.8004 29.4898,-102.7002 29.6881,-102.5134 29.7691,-102.3843 29.7596,-102.3047 29.8788,-102.1509 29.7834,-101.7004 29.7572,-101.4917 29.7644,-101.2939 29.6308,-101.2582 29.5269,-101.0056 29.3642,-100.9204 29.3056,-100.7707 29.1642,-100.7007 29.0946,-100.6306 28.9012,-100.4974 28.6593,-100.3601 28.4675,-100.2969 28.2778,-100.1733 28.1882,-100.0195 28.0526,-99.9344 27.9435,-99.8438 27.7638,-99.7119 27.6641,-99.4812 27.4839,-99.5375 27.3059,-99.4290 27.1948,-99.4455 27.0175,-99.3164 26.8829,-99.2065 26.6867,-99.0967 26.4116,-98.8138 26.3574,-98.6668 26.2257,-98.5474 26.2343,-98.3276 26.1357,-98.1697 26.0457,-97.9143 26.0518,-97.6643 26.0050,-97.4020 25.8419,-97.3526 25.9074,-97.0148 25.9679,-97.0697 26.1789,-97.2249 26.8253,-97.0752 27.4230,-96.6096 28.0599,-95.9285 28.4228,-95.3036 28.7568,-94.7296 29.0742,-94.3355 29.3810,-93.8205 29.6021,-93.9317 29.8013,-93.8136 29.9157,-93.7230 30.0489,-93.6996 30.1214,-93.7216 30.2021,-93.7038 30.2792,-93.7628 30.3278,-93.7587 30.3835,-93.7010 30.4380,-93.7024 30.5079,-93.7299 30.5362,-93.6694 30.6296,-93.6090 30.7466,-93.5527 30.8114,-93.5747 30.8834,-93.5307 30.9376,-93.5074 31.0318,-93.5266 31.0812,-93.5335 31.1787,-93.5980 31.1670,-93.6832 31.3055,-93.6708 31.3830,-93.6887 31.4369,-93.7202 31.5107,-93.8315 31.5820,-93.8123 31.6440,-93.8232 31.7188,-93.8342 31.7936,-93.8782 31.8309,-93.9221 31.8869,-93.9661 31.9335,-94.0430 32.0081,-94.0430 33.4681,-94.0430 33.5414,-94.1528 33.5689,-94.1968 33.5872,-94.2627 33.5872,-94.3176 33.5689,-94.3945 33.5597,-94.4275 33.5780,-94.4275 33.6055,-94.4495 33.6421,-94.4879 33.6329,-94.5236 33.6421,-94.6637 33.6695,-94.7461 33.7061,-94.8999 33.7791,-95.0757 33.8818,-95.1526 33.9251,-95.2254 33.9604,-95.2858 33.8750,-95.5399 33.8841,-95.7568 33.8887,-95.8420 33.8408,-96.0274 33.8556,-96.3528 33.6901,-96.6179 33.8442,-96.5836 33.8898,-96.6673 33.8955,-96.7538 33.8179,-96.8335 33.8613,-96.8774 33.8613,-96.9159 33.9388,-97.0917 33.7392,-97.1645 33.7449,-97.2180 33.8978,-97.3746 33.8225,-97.4611 33.8305,-97.4460 33.8761,-97.6945 33.9798,-97.8648 33.8476,-97.9651 33.8978,-98.0983 34.0299,-98.1752 34.1141,-98.3743 34.1425,-98.4773 34.0640,-98.5529 34.1209,-98.7520 34.1232,-98.9539 34.2095,-99.0637 34.2073,-99.1832 34.2141,-99.2505 34.3593,-99.3823 34.4613,-99.4318 34.3774,-99.5718 34.4160,-99.6158 34.3706,-99.8094 34.4726,-99.9934 34.5631,-100.0017 36.4975,-103.0408 36.5008,-103.0655 32.0011,-106.6168 32.0023,-106.5715 31.8659))\"\n",
    "TX_polygon = loads(TX_wkt_string)\n",
    "\n",
    "# Define the WKT string for the LA polygon\n",
    "LA_wkt_string = \"POLYGON((-94.0430 33.0225,-93.0048 33.0179,-91.1646 33.0087,-91.2209 32.9269,-91.1220 32.8773,-91.1481 32.8358,-91.1412 32.7642,-91.1536 32.6382,-91.1069 32.5804,-91.0080 32.6093,-91.0904 32.4588,-91.0355 32.4379,-91.0286 32.3742,-90.9064 32.3150,-90.9723 32.2616,-91.0464 32.1942,-91.0739 32.1198,-91.0464 32.0593,-91.1014 31.9918,-91.1865 31.9498,-91.3101 31.8262,-91.3527 31.7947,-91.3925 31.6230,-91.5134 31.6218,-91.4310 31.5668,-91.5161 31.5130,-91.5244 31.3701,-91.5477 31.2598,-91.6425 31.2692,-91.6603 31.2328,-91.5848 31.1917,-91.6287 31.1047,-91.5614 31.0318,-91.6397 30.9988,-89.7336 31.0012,-89.8517 30.6686,-89.7858 30.5386,-89.6347 30.3148,-89.5688 30.1807,-89.4960 30.1582,-89.1843 30.2140,-89.0373 30.1463,-88.8354 30.0905,-88.7421 29.8383,-88.8712 29.5758,-88.9371 29.1833,-89.0359 28.9649,-89.2282 28.8832,-89.4754 28.9048,-89.7418 29.1210,-90.1126 28.9529,-90.6619 28.9120,-91.0355 28.9553,-91.3211 29.1210,-91.9061 29.2864,-92.7452 29.4360,-93.8177 29.6009,-93.8631 29.6749,-93.8933 29.7370,-93.9304 29.7930,-93.9276 29.8216,-93.8370 29.8883,-93.7985 29.9811,-93.7601 30.0144,-93.7106 30.0691,-93.7354 30.0929,-93.6996 30.1166,-93.7271 30.1997,-93.7106 30.2899,-93.7656 30.3350,-93.7601 30.3871,-93.6914 30.4416,-93.7106 30.5102,-93.7463 30.5433,-93.7106 30.5954,-93.6914 30.5906,-93.6859 30.6545,-93.6365 30.6781,-93.6200 30.7513,-93.5925 30.7890,-93.5513 30.8150,-93.5623 30.8645,-93.5788 30.8881,-93.5541 30.9187,-93.5294 30.9423,-93.5760 31.0082,-93.5101 31.0318,-93.5596 31.0906,-93.5321 31.1211,-93.5349 31.1799,-93.5953 31.1658,-93.6282 31.2292,-93.6118 31.2668,-93.6859 31.3044,-93.6694 31.3888,-93.7051 31.4240,-93.6859 31.4427,-93.7573 31.4755,-93.7189 31.5083,-93.8040 31.5411,-93.8425 31.6113,-93.8205 31.6581,-93.7985 31.7071,-93.8480 31.8029,-93.9029 31.8892,-93.9606 31.9149,-94.0430 32.0081,-94.0430 32.7041,-94.0430 33.0225,-94.0430 33.0225))\"\n",
    "LA_polygon = loads(LA_wkt_string)\n",
    "\n",
    "# Define the WKT string for the MS polygon\n",
    "MS_wkt_string = \"POLYGON((-90.3049 35.0041,-88.1955 35.0075,-88.0994 34.8882,-88.1241 34.7044,-88.2573 33.6661,-88.4756 31.8939,-88.4180 30.8657,-88.3850 30.1594,-88.8327 30.0905,-89.1870 30.2104,-89.4919 30.1570,-89.5757 30.1796,-89.6457 30.3326,-89.7748 30.5232,-89.8531 30.6663,-89.7377 30.9988,-91.6287 30.9988,-91.5601 31.0341,-91.6273 31.1106,-91.5916 31.1658,-91.6589 31.2304,-91.6452 31.2656,-91.5436 31.2609,-91.5271 31.3724,-91.5161 31.4099,-91.5120 31.5071,-91.4502 31.5692,-91.5147 31.6230,-91.3966 31.6253,-91.3513 31.7936,-91.2744 31.8589,-91.1673 31.9755,-91.0767 32.0267,-91.0767 32.1198,-91.0437 32.1942,-91.0107 32.2221,-90.9132 32.3150,-91.0313 32.3742,-91.0217 32.4263,-91.0986 32.4634,-91.0080 32.6070,-91.1096 32.5746,-91.1536 32.6394,-91.1426 32.7226,-91.1426 32.7873,-91.1536 32.8519,-91.1206 32.8796,-91.2195 32.9257,-91.2085 32.9995,-91.2016 33.0444,-91.2016 33.1192,-91.1041 33.1835,-91.1536 33.3397,-91.1646 33.4223,-91.2291 33.4337,-91.2524 33.5414,-91.1838 33.6135,-91.2524 33.6878,-91.1261 33.6969,-91.1426 33.7883,-91.0437 33.7700,-91.0327 33.8339,-91.0657 33.8795,-91.0876 33.9434,-90.9998 33.9889,-90.9229 34.0253,-90.9009 34.0891,-90.9668 34.1345,-90.9119 34.1709,-90.8501 34.1345,-90.9338 34.2277,-90.8267 34.2833,-90.6921 34.3434,-90.6509 34.3774,-90.6152 34.3978,-90.5589 34.4432,-90.5740 34.5179,-90.5823 34.5880,-90.5356 34.7506,-90.5136 34.7913,-90.4532 34.8780,-90.3543 34.8476,-90.2911 34.8702,-90.3062 35.0041,-90.3049 35.0041))\"\n",
    "MS_polygon = loads(MS_wkt_string)\n",
    "\n",
    "# Define the WKT string for the AL polygon\n",
    "AL_wkt_string = \"POLYGON((-88.1955 35.0041,-85.6068 34.9918,-85.1756 32.8404,-84.8927 32.2593,-85.0342 32.1535,-85.1358 31.7947,-85.0438 31.5200,-85.0836 31.3384,-85.1070 31.2093,-84.9944 31.0023,-87.6009 30.9953,-87.5926 30.9423,-87.6256 30.8539,-87.4072 30.6745,-87.3688 30.4404,-87.5240 30.1463,-88.3864 30.1546,-88.4743 31.8939,-88.1021 34.8938,-88.1721 34.9479,-88.1461 34.9107,-88.1955 35.0041))\"\n",
    "AL_polygon = loads(AL_wkt_string)\n",
    "\n",
    "# Define the WKT string for the AL polygon\n",
    "FL_wkt_string = \"POLYGON((-87.6050 30.9988,-86.5613 30.9964,-85.5313 31.0035,-85.1193 31.0012,-85.0012 31.0023,-84.9847 30.9364,-84.9367 30.8845,-84.9271 30.8409,-84.9257 30.7902,-84.9147 30.7489,-84.8611 30.6993,-84.4272 30.6911,-83.5991 30.6509,-82.5595 30.5895,-82.2134 30.5682,-82.2134 30.5315,-82.1997 30.3883,-82.1544 30.3598,-82.0638 30.3598,-82.0226 30.4877,-82.0473 30.6308,-82.0514 30.6757,-82.0377 30.7111,-82.0514 30.7371,-82.0102 30.7678,-82.0322 30.7914,-81.9717 30.7997,-81.9608 30.8244,-81.8893 30.8056,-81.8372 30.7914,-81.7960 30.7796,-81.6696 30.7536,-81.6051 30.7289,-81.5666 30.7324,-81.5295 30.7229,-81.4856 30.7253,-81.4609 30.7111,-81.4169 30.7088,-81.2274 30.7064,-81.2357 30.4345,-81.1725 30.3160,-81.0379 29.7763,-80.5861 28.8603,-80.3650 28.4771,-80.3815 28.1882,-79.9255 27.1789,-79.8198 26.8425,-79.9118 26.1394,-79.9997 25.5115,-80.3815 24.8802,-80.8704 24.5384,-81.9250 24.3959,-82.2066 24.4496,-82.3137 24.5484,-82.1997 24.6982,-81.3977 25.2112,-81.4622 25.6019,-81.9456 25.9235,-82.2876 26.3439,-82.5307 26.9098,-82.8342 27.3315,-83.0182 27.7565,-83.0017 28.0574,-82.8548 28.6098,-83.0264 28.9697,-83.2050 29.0478,-83.5318 29.4157,-83.9767 29.9133,-84.1072 29.8930,-84.4409 29.6940,-85.0465 29.4551,-85.3610 29.4946,-85.5807 29.7262,-86.1946 30.1594,-86.8510 30.2175,-87.5171 30.1499,-87.4429 30.3006,-87.3750 30.4256,-87.3743 30.4830,-87.3907 30.5658,-87.4004 30.6344,-87.4141 30.6763,-87.5253 30.7702,-87.6256 30.8527,-87.5912 30.9470,-87.5912 30.9682,-87.6050 30.9964,-87.6050 30.9988))\"\n",
    "FL_polygon = loads(FL_wkt_string)\n",
    "\n",
    "# Find the min and max bounds for each state\n",
    "# TX_min_lat,TX_min_lng,TX_max_lat,TX_max_lng=TX_polygon.bounds\n",
    "# LA_min_lat,LA_min_lng,LA_max_lat,LA_max_lng=LA_polygon.bounds\n",
    "# MS_min_lat,MS_min_lng,MS_max_lat,MS_max_lng=MS_polygon.bounds\n",
    "# AL_min_lat,AL_min_lng,AL_max_lat,AL_max_lng=AL_polygon.bounds\n",
    "# FL_min_lat,FL_min_lng,AL_max_lat,FL_max_lng=FL_polygon.bounds\n",
    "\n",
    "TX_min_lng,TX_min_lat,TX_max_lng,TX_max_lat=TX_polygon.bounds\n",
    "LA_min_lng,LA_min_lat,LA_max_lng,LA_max_lat=LA_polygon.bounds\n",
    "MS_min_lng,MS_min_lat,MS_max_lng,MS_max_lat=MS_polygon.bounds\n",
    "AL_min_lng,AL_min_lat,AL_max_lng,AL_max_lat=AL_polygon.bounds\n",
    "FL_min_lng,FL_min_lat,FL_max_lng,FL_max_lat=FL_polygon.bounds\n",
    "\n",
    "\n",
    "avg_lat = .5*(TX_min_lat+TX_max_lat)\n",
    "avg_lng = .5*(TX_min_lng+TX_max_lng)\n",
    "avg_point = Point(avg_lng,avg_lat)\n",
    "\n",
    "TX_within_polygon = avg_point.within(TX_polygon)\n",
    "LA_within_polygon = avg_point.within(LA_polygon)\n",
    "MS_within_polygon = avg_point.within(MS_polygon)\n",
    "AL_within_polygon = avg_point.within(AL_polygon)\n",
    "FL_within_polygon = avg_point.within(FL_polygon)\n",
    "\n",
    "# # Initialize variables\n",
    "TX = LA = MS = AL = FL = 0\n",
    "if TX_within_polygon:\n",
    "    TX = 1\n",
    "elif LA_within_polygon:\n",
    "    LA = 1\n",
    "elif MS_within_polygon:\n",
    "    MS = 1\n",
    "elif AL_within_polygon:\n",
    "    AL = 1\n",
    "elif FL_within_polygon:\n",
    "    FL = 1\n",
    "    \n",
    "print(avg_point,\"is_within_TX\",TX_within_polygon)\n",
    "print(avg_point,\"is_within_LA\",LA_within_polygon)\n",
    "print(avg_point,\"is_within_MS\",MS_within_polygon)\n",
    "print(avg_point,\"is_within_AL\",AL_within_polygon)\n",
    "print(avg_point,\"is_within_FL\",FL_within_polygon)\n",
    "\n",
    "print(TX,LA,MS,AL,FL)\n",
    "print(avg_lat,avg_lng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83a3152-0116-4bd2-a70c-d03aea9cd3fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Prepare for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7615286-7ed2-4025-a9ee-2364f938dbdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load csv files from into data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8120e5a5-0718-4555-a3a1-d5cf5b047bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input files\n",
    "# # Input files\n",
    "disappear_df = pd.read_csv(DisappearanceIndicators_filename)\n",
    "indicators_df = pd.read_csv(Rec_indicators_with_V3_filename)\n",
    "\n",
    "rec_indicators_df = pd.merge(\n",
    "    disappear_df,\n",
    "    indicators_df,\n",
    "    on=['cuebiq_id', 'Trip_number'],\n",
    "    how='outer',  # Use 'outer' to keep all rows from both DataFrames\n",
    ")\n",
    "\n",
    "print(len(rec_indicators_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5e8b0-8f12-4b55-bbb0-f4977e4173ed",
   "metadata": {},
   "source": [
    "### Load ping files -- commented out unless absolutely needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b61e727-89dd-4717-9f8f-6a7b2bd6ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ping Files\n",
    "# # Origins data created above\n",
    "# V3_Pings_df = pd.read_csv(Combined_Pings_V3_Before_After_filename)\n",
    "# Pings_OurTable_Gulf_df= pd.read_csv(Combined_Pings_OurTable_Gulf_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a51e73-82f5-46d6-8d44-1e3877af2e9d",
   "metadata": {},
   "source": [
    "#### Load the indicators for all of the trips from the original 300K cuebiq_id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38608f9-1c89-44d6-9a88-f731bfa5ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Indicators_df = pd.read_csv(Indicators_Classified_filename)\n",
    "print(\"There were \", len(All_Indicators_df), \"trips for which indicators were developed\")\n",
    "Non_371_indicators_df = All_Indicators_df[All_Indicators_df['Predicted_Class'] != 371]\n",
    "print(\"Of these, \", len(Non_371_indicators_df), \"trips not identified by ML as a pleasure boat trip (371)\")\n",
    "km_dist_threshhold = 1\n",
    "Non_371_indicators_df=Non_371_indicators_df[Non_371_indicators_df['Max_distance_traveled_origin_t']>km_dist_threshhold]\n",
    "print(\"Of these, \", len(Non_371_indicators_df), \"trips went at least \", km_dist_threshhold, \" km from the origin and are included for validation analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca95898-1e4d-4c04-86ef-1b72d731d938",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compare identified recreational trips with the site pressure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3668db-85c8-4820-bf1a-fae05e436158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stations_with_stops_filename\n",
    "# V3_Pings_df = pd.read_csv(Combined_Pings_V3_Before_After_filename)\n",
    "# rec_indicators_df = pd.read_csv(Combined_indicators_with_disappearance_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd3a69a-6b62-4279-a59d-a5fa4a4edcfe",
   "metadata": {},
   "source": [
    "## Redo the stops analysis recovering the stations where stops occurred\n",
    "The station that is closest in time to the Gulf trip is identified as the station of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556119e-e908-4aa7-858e-3cb80a77230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ii = 0\n",
    "# Load the indicators file that shows whether a trip was stopped near a station\n",
    "stops_indicators_df = pd.read_csv(Station_NonStationAnalysis_filename)\n",
    "\n",
    "# Limit the data frame to trips with stops near a station in Alabama or Mississippi\n",
    "stops_indicators_df = stops_indicators_df[\n",
    "    (stops_indicators_df['AL_MRIP_station_points_0_1']==1) |\n",
    "    (stops_indicators_df['MS_MRIP_station_points_0_1']==1) |\n",
    "    (stops_indicators_df['LA_CREEL_station_points_0_1']==1) |\n",
    "    (stops_indicators_df['TX_station_points_0_1']==1) \n",
    "    ]\n",
    "\n",
    "# Now take the main rec indicators and pull out only those with identified stops in MS & AL\n",
    "filtered_rec_indicators_df = rec_indicators_df.merge(\n",
    "    stops_indicators_df[['cuebiq_id', 'Trip_number']],\n",
    "    on=['cuebiq_id', 'Trip_number'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Drop rows from filtered_rec_indicators_df where cuebiq_id and Trip_number match\n",
    "\n",
    "print(\"len(filtered_rec_indicators_df)\", len(filtered_rec_indicators_df))\n",
    "\n",
    "Stations_with_stops_filename = ValidationAnalysis_sites_With_Stops_filename\n",
    "if os.path.isfile(Stations_with_stops_filename):\n",
    "    Stations_with_stops_df = pd.read_csv(Stations_with_stops_filename)\n",
    "    Stations_with_stops_df['cuebiq_id'] = Stations_with_stops_df['cuebiq_id'].astype(int)\n",
    "    filtered_rec_indicators_df = filtered_rec_indicators_df[\n",
    "        ~filtered_rec_indicators_df[['cuebiq_id', 'Trip_number']].apply(tuple, axis=1).isin(\n",
    "            Stations_with_stops_df[['cuebiq_id', 'Trip_number']].apply(tuple, axis=1)\n",
    "        )\n",
    "]\n",
    "\n",
    "Eight_hours = 8 * 60 * 60\n",
    "One_mph = 0.0268224   # Speeds are calculated in km/min. 1 mph = 0.0268224 km/min\n",
    "\n",
    "#############################################################################################################################\n",
    "AL_MS_MRIP_station_points_df = pd.concat([AL_MRIP_station_points_df, MS_MRIP_station_points_df], ignore_index=True, join='outer')\n",
    "AL_MS_MRIP_station_points_df = add_h3_and_neighbors_to_dataframe(AL_MS_MRIP_station_points_df, resolution)\n",
    "\n",
    "AL_MS_MRIP_station_points_df['Site_id'] = AL_MS_MRIP_station_points_df['STATE_CODE']*10^7 +  \\\n",
    "                    AL_MS_MRIP_station_points_df['COUNTY_CODE']*10^4 + AL_MS_MRIP_station_points_df['SITE_EXTERNAL_ID']  ## This is Jesse's newsiteID\n",
    "columns_to_retain = [\n",
    "    'Site_id', 'SITE_EXTERNAL_ID', 'SITE_NAME', 'SITE_ADDRESS', 'SITE_CITY', 'SITE_ZIP',\n",
    "    'COUNTY_CODE', 'COUNTY', 'STATE_CODE', 'STATE', 'lat', 'lng', 'h3_cell', 'h3_neighbors'\n",
    "]\n",
    "\n",
    "# columns_to_retain = ['Site_id', 'SITE_NAME','STATE_CODE', 'lat', 'lng']\n",
    "AL_MS_MRIP_station_points_df = AL_MS_MRIP_station_points_df[columns_to_retain]\n",
    "AL_MS_MRIP_station_points_df.rename(columns={'SITE_NAME':  'Site_Name', 'STATE_CODE': 'State_Code'}, inplace=True)\n",
    "# Add h3 cells and neighors to the MRIP data frames\n",
    "columns_to_retain = ['Site_id', 'Site_Name','State_Code', 'lat', 'lng']\n",
    "AL_MS_MRIP_station_points_df = AL_MS_MRIP_station_points_df[columns_to_retain]\n",
    "AL_MS_MRIP_station_points_df = add_h3_and_neighbors_to_dataframe(AL_MS_MRIP_station_points_df, resolution)\n",
    "\n",
    "#############################################################################################################################\n",
    "# TX_station_points_df\n",
    "\n",
    "columns_to_retain = ['major_area_code', 'station_code', 'Site_name', 'lat', 'lng']\n",
    "TX_station_points1_df = TX_station_points_df[columns_to_retain]\n",
    "# TX_station_points1_df = add_h3_and_neighbors_to_dataframe(TX_station_points1_df, resolution)\n",
    "\n",
    "\n",
    "TX_station_points1_df.rename(columns={'Site_name':  'Site_Name'}, inplace=True)\n",
    "TX_station_points1_df['Site_id']=TX_station_points_df['major_area_code']*1000+ TX_station_points_df['station_code']\n",
    "TX_station_points1_df['State_Code'] = 48\n",
    "columns_to_retain = ['Site_id', 'Site_Name','State_Code', 'lat', 'lng']\n",
    "\n",
    "TX_station_points1_df = TX_station_points1_df[columns_to_retain]\n",
    "TX_station_points1_df = add_h3_and_neighbors_to_dataframe(TX_station_points1_df, resolution)\n",
    "\n",
    "\n",
    "#############################################################################################################################\n",
    "# LA_CREEL_station_points_df\n",
    "columns_to_retain = ['Site_Id', 'Site_Name', 'lat', 'lng']\n",
    "LA_CREEL_station_points1_df = LA_CREEL_station_points_df[columns_to_retain]\n",
    "LA_CREEL_station_points1_df.rename(columns={'Site_Id':  'Site_id'}, inplace=True)\n",
    "LA_CREEL_station_points1_df['State_Code'] = 22\n",
    "columns_to_retain = ['Site_id', 'Site_Name','State_Code', 'lat', 'lng']\n",
    "LA_CREEL_station_points1_df = LA_CREEL_station_points1_df[columns_to_retain]\n",
    "LA_CREEL_station_points1_df = add_h3_and_neighbors_to_dataframe(LA_CREEL_station_points1_df, resolution)\n",
    "\n",
    "#############################################################################################################################\n",
    "for index, row in filtered_rec_indicators_df.iterrows():\n",
    "    print(\"len(filtered_rec_indicators_df)\", len(filtered_rec_indicators_df))\n",
    "    assert ii == 2200000\n",
    "    #############\n",
    "    ii = ii +1/500\n",
    "    if abs(int(ii) - ii) < 1/500:\n",
    "        print(ii*500)\n",
    "    #############\n",
    "    # print(\"irow\", irow)\n",
    "    # intersection_results_df = []\n",
    "    cuebiq_id = row['cuebiq_id']\n",
    "    Trip_number = row['Trip_number']\n",
    "    trip_start_epochtime = row['timestamp_start_t']\n",
    "    trip_end_epochtime = row['timestamp_end_t']\n",
    "        \n",
    "    # Get pings before trip\n",
    "    this_trip_before_df = V3_Pings_df[\n",
    "        (V3_Pings_df['cuebiq_id'] == cuebiq_id) &\n",
    "        (V3_Pings_df['event_timestamp'] >= (trip_start_epochtime-Eight_hours)) &\n",
    "        (V3_Pings_df['event_timestamp'] <= trip_start_epochtime) ]\n",
    "    this_trip_before_df=EliminateErrantPingsSpeed(this_trip_before_df, 90)\n",
    "\n",
    "    # Get pings after trip\n",
    "    this_trip_after_df = V3_Pings_df[\n",
    "        (V3_Pings_df['cuebiq_id'] == cuebiq_id) &\n",
    "        (V3_Pings_df['event_timestamp'] <= (trip_end_epochtime+Eight_hours)) &\n",
    "        (V3_Pings_df['event_timestamp'] >= trip_end_epochtime)     ]\n",
    "    this_trip_after_df=EliminateErrantPingsSpeed(this_trip_after_df, 90)\n",
    "\n",
    "    before_and_after_df = pd.concat([this_trip_before_df, this_trip_after_df], ignore_index=True)\n",
    "    before_and_after_df = before_and_after_df[(before_and_after_df['Avg_ping_speed'] <= One_mph)]\n",
    "    \n",
    "    # Get h3 cells of all points where the device is \"stopped\"\n",
    "    before_and_after_h3_cells, before_and_after_df = get_h3_cells_set_and_dataframe(before_and_after_df, resolution)\n",
    "\n",
    "    inTX = inLA = inMS = inAL = False\n",
    "\n",
    "    AL_MRIP_station_points_intersection = list(set(before_and_after_h3_cells).intersection(AL_MRIP_station_points_h3_cells))\n",
    "    if len(AL_MRIP_station_points_intersection) >0:\n",
    "        inAL = True\n",
    "    MS_MRIP_station_points_intersection = list(set(before_and_after_h3_cells).intersection(MS_MRIP_station_points_h3_cells))\n",
    "    if len(MS_MRIP_station_points_intersection) >0:\n",
    "        inMS = True\n",
    "    LA_station_points_intersection = list(set(before_and_after_h3_cells).intersection(LA_CREEL_station_points_h3_cells))\n",
    "    if len(LA_station_points_intersection) >0:\n",
    "        inLA = True\n",
    "    TX_station_points_intersection = list(set(before_and_after_h3_cells).intersection(TX_station_points_h3_cells))\n",
    "    if len(TX_station_points_intersection) >0:\n",
    "        inTX = True\n",
    "\n",
    "    filtered_before_and_after_df = before_and_after_df[\n",
    "        before_and_after_df['h3_cell'].isin(AL_MRIP_station_points_intersection) |\n",
    "        before_and_after_df['h3_cell'].isin(LA_station_points_intersection) |\n",
    "        before_and_after_df['h3_cell'].isin(TX_station_points_intersection) |\n",
    "        before_and_after_df['h3_cell'].isin(MS_MRIP_station_points_intersection)]\n",
    "\n",
    "   \n",
    "    filtered_before_and_after_df['time_before_after'] = np.maximum(\n",
    "        filtered_before_and_after_df['event_timestamp'] - trip_end_epochtime,\n",
    "        trip_start_epochtime - filtered_before_and_after_df['event_timestamp']\n",
    "    )        \n",
    "    specific_h3_cell = filtered_before_and_after_df.loc[filtered_before_and_after_df['time_before_after'].idxmin(), 'h3_cell']\n",
    "\n",
    "    if inAL or inMS:\n",
    "        matching_rows = AL_MS_MRIP_station_points_df[\n",
    "            (AL_MS_MRIP_station_points_df['h3_cell'] == specific_h3_cell) | \n",
    "            (AL_MS_MRIP_station_points_df['h3_neighbors'].apply(lambda x: specific_h3_cell in x))\n",
    "        ]\n",
    "\n",
    "    if inLA:\n",
    "        matching_rows = LA_CREEL_station_points1_df[\n",
    "            (LA_CREEL_station_points1_df['h3_cell'] == specific_h3_cell) | \n",
    "            (LA_CREEL_station_points1_df['h3_neighbors'].apply(lambda x: specific_h3_cell in x))\n",
    "        ]\n",
    "\n",
    "        \n",
    "    if inTX:\n",
    "        matching_rows = TX_station_points1_df[\n",
    "            (TX_station_points1_df['h3_cell'] == specific_h3_cell) | \n",
    "            (TX_station_points1_df['h3_neighbors'].apply(lambda x: specific_h3_cell in x))\n",
    "        ]\n",
    "\n",
    "    \n",
    "    # Select only the first row in the event that there are 2 stations very close to each other\n",
    "    matching_rows = matching_rows.iloc[0]\n",
    "\n",
    "    ######################## Now combine the necessary data for final output to a csv file\n",
    "    stop_row = stops_indicators_df[\n",
    "        (stops_indicators_df['cuebiq_id'] == cuebiq_id) & \n",
    "        (stops_indicators_df['Trip_number'] == Trip_number)\n",
    "    ]\n",
    "    if not stop_row.empty:  # Check if matching row is found\n",
    "        stop_row = stop_row.iloc[0]  # Get the first (and likely only) matching row\n",
    "\n",
    "    # Step 3: Combine columns into a single row (dictionary or list)\n",
    "    combined_row = {\n",
    "        'cuebiq_id': cuebiq_id,\n",
    "        'Trip_number': Trip_number,  \n",
    "        'trip_start_epochtime': row['timestamp_start_t'],\n",
    "        'trip_end_epochtime': row['timestamp_end_t'],\n",
    "        # Columns from stops_indicators_df\n",
    "        'Interruption_01': stop_row['Interruption_01'],\n",
    "        'nPings': stop_row['nPings'],\n",
    "        'nStops': stop_row['nStops'],\n",
    "        'MS': stop_row['MS'],\n",
    "        'AL': stop_row['AL'],\n",
    "        'FL': stop_row['FL'],\n",
    "        'TX': stop_row['TX'],\n",
    "        'LA': stop_row['LA'],\n",
    "        'AL_MRIP_station_points_0_1': stop_row['AL_MRIP_station_points_0_1'],\n",
    "        'MS_MRIP_station_points_0_1': stop_row['MS_MRIP_station_points_0_1'],\n",
    "        'LA_CREEL_station_points_0_1': stop_row['LA_CREEL_station_points_0_1'],\n",
    "        'TX_station_points_0_1':stop_row['TX_station_points_0_1'],\n",
    "    }\n",
    "   # Add the matching_row values (the other columns from matching_rows)\n",
    "    for col in matching_rows.index:\n",
    "        if col not in combined_row:  # Avoid duplicates\n",
    "            combined_row[col] = matching_rows[col]\n",
    "\n",
    "    combined_row_df = pd.DataFrame([combined_row])\n",
    "    combined_row_df.to_csv(Stations_with_stops_filename, index=False, mode='a', \n",
    "                           header=not os.path.isfile(Stations_with_stops_filename))\n",
    "print(\"Done\")\n",
    "print(ii*500,\" of \", len(filtered_rec_indicators_df), \"remaining trips completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d31facc-94c2-416b-ac86-432554022270",
   "metadata": {},
   "source": [
    "### Load and prepare the station visits based on stops analysis (TX, AL & MS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221de824-1d8a-4126-9c04-a332e71b99ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stations_with_stops_filename = ValidationAnalysis_sites_With_Stops_filename\n",
    "\n",
    "#####################################################################\n",
    "# Load Station with Stops data used in station-nonstation analysis\n",
    "Stations_with_stops_df=pd.read_csv(Stations_with_stops_filename)\n",
    "\n",
    "Stations_with_stops_df['datetime'] = pd.to_datetime(Stations_with_stops_df['trip_start_epochtime'], unit='s')\n",
    "\n",
    "# Create new columns\n",
    "Stations_with_stops_df['date'] = Stations_with_stops_df['datetime'].dt.date\n",
    "Stations_with_stops_df['year'] = Stations_with_stops_df['datetime'].dt.year\n",
    "Stations_with_stops_df['month'] = Stations_with_stops_df['datetime'].dt.month\n",
    "Stations_with_stops_df['day_of_week'] = Stations_with_stops_df['datetime'].dt.weekday  # 0 = Monday, 6 = Sunday\n",
    "Stations_with_stops_df['year_month'] = (Stations_with_stops_df['year'])*100 + Stations_with_stops_df['month']\n",
    "\n",
    "Stations_with_stops_df['New_Site_ID'] = 10000*Stations_with_stops_df['State_Code']+Stations_with_stops_df['Site_id']\n",
    "Stations_with_stops_df['WAVE'] = ((Stations_with_stops_df['month'] - 1) // 2) + 1\n",
    "\n",
    "Stations_with_stops_df['year_wave'] = Stations_with_stops_df['year']*100 + Stations_with_stops_df['WAVE']\n",
    "Stations_with_stops_df['Year_month'] = Stations_with_stops_df['year']*100 + Stations_with_stops_df['month']\n",
    "\n",
    "###################################################################################################\n",
    "# Select list for two regions\n",
    "Stations_with_stops_TX_df = Stations_with_stops_df[Stations_with_stops_df['TX_station_points_0_1']==1]\n",
    "Stations_with_stops_AL_MS_df = Stations_with_stops_df[(Stations_with_stops_df['AL_MRIP_station_points_0_1']==1) |\n",
    "                                                      (Stations_with_stops_df['MS_MRIP_station_points_0_1']==1) ]\n",
    "\n",
    "###################################################################################################\n",
    "# Total station trips for each region by site & month or wave\n",
    "Total_mobility_visits_by_site_TX = Stations_with_stops_TX_df.groupby('Site_id').size().reset_index(name='Total_mobility_visits')\n",
    "Total_mobility_visits_by_month_TX = Stations_with_stops_TX_df.groupby('year_month').size().reset_index(name='Total_mobility_visits')\n",
    "\n",
    "Total_mobility_visits_by_site_AL_MS = Stations_with_stops_AL_MS_df.groupby('New_Site_ID').size().reset_index(name='Total_mobility_visits')\n",
    "Total_mobility_visits_by_year_wave_AL_MS = Stations_with_stops_AL_MS_df.groupby('year_wave').size().reset_index(name='Total_mobility_visits')\n",
    "\n",
    "###################################################################################################\n",
    "# Calculate Comple_visits (number of rows per Site_id where Interruption_01 == 1)\n",
    "complete_visits_TX = Stations_with_stops_TX_df[Stations_with_stops_TX_df['Interruption_01'] == 1] \\\n",
    "    .groupby('Site_id').size().reset_index(name='complete_visits')\n",
    "\n",
    "# Calculate Comple_visits (number of rows per Site_id where Interruption_01 == 1)\n",
    "complete_visits_by_month_TX = Stations_with_stops_TX_df[Stations_with_stops_TX_df['Interruption_01'] == 1] \\\n",
    "    .groupby('year_month').size().reset_index(name='complete_visits')\n",
    "\n",
    "# Calculate Comple_visits (number of rows per Site_id where Interruption_01 == 1)\n",
    "complete_visits_AL_MS = Stations_with_stops_AL_MS_df[Stations_with_stops_AL_MS_df['Interruption_01'] == 1] \\\n",
    "    .groupby('New_Site_ID').size().reset_index(name='complete_visits')\n",
    "\n",
    "# Calculate Comple_visits (number of rows per Site_id where Interruption_01 == 1)\n",
    "complete_visits_by_year_wave_AL_MS = Stations_with_stops_AL_MS_df[Stations_with_stops_AL_MS_df['Interruption_01'] == 1] \\\n",
    "    .groupby('year_wave').size().reset_index(name='complete_visits')\n",
    "\n",
    "###################################################################################################\n",
    "# Merge the two DataFrames on 'Site_id'\n",
    "Mobility_Visits_TX_df = pd.merge(Total_mobility_visits_by_site_TX, complete_visits_TX, on='Site_id', how='left')\n",
    "Mobility_Visits_TX_df['complete_visits'] = Mobility_Visits_TX_df['complete_visits'].fillna(0).astype(int)\n",
    "\n",
    "Mobility_Visits_TX_by_month_df = pd.merge(Total_mobility_visits_by_month_TX, complete_visits_by_month_TX, on='year_month', how='left')\n",
    "Mobility_Visits_TX_by_month_df['complete_visits'] = Mobility_Visits_TX_by_month_df['complete_visits'].fillna(0).astype(int)\n",
    "\n",
    "# Merge the two DataFrames on 'Site_id'\n",
    "Mobility_Visits_AL_MS_by_site_df = pd.merge(Total_mobility_visits_by_site_AL_MS, complete_visits_AL_MS, on='New_Site_ID', how='left')\n",
    "Mobility_Visits_AL_MS_by_site_df['complete_visits'] = Mobility_Visits_AL_MS_by_site_df['complete_visits'].fillna(0).astype(int)\n",
    "\n",
    "Mobility_Visits_AL_MS_by_year_wave_df = pd.merge(Total_mobility_visits_by_year_wave_AL_MS, complete_visits_by_year_wave_AL_MS, on='year_wave', how='left')\n",
    "Mobility_Visits_AL_MS_by_year_wave_df['complete_visits'] = Mobility_Visits_AL_MS_by_year_wave_df['complete_visits'].fillna(0).astype(int)\n",
    "\n",
    "# Extract unique Site_id with their lat and lng\n",
    "site_coordinates_TX = Stations_with_stops_TX_df[['Site_id', 'lat', 'lng']].drop_duplicates()\n",
    "site_coordinates_AL_MS = Stations_with_stops_AL_MS_df[['New_Site_ID', 'lat', 'lng']].drop_duplicates()\n",
    "\n",
    "# Merge site coordinates with Mobility_Visits_TX_df\n",
    "Mobility_Visits_TX_df = pd.merge(Mobility_Visits_TX_df, site_coordinates_TX, on='Site_id', how='left')\n",
    "Mobility_Visits_AL_MS_by_site_df = pd.merge(Mobility_Visits_AL_MS_by_site_df, site_coordinates_AL_MS, on='New_Site_ID', how='left')\n",
    "\n",
    "# Mobility_Visits_TX_by_month_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858172b6-4707-435d-9444-ad9fe64cf700",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load and prepare TX Roving Counts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad016b-a080-4485-ae9c-29cb717641f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texas Roving Counts data were provided by Mark Fisher (TPWD) on 5/31/2022\n",
    "\n",
    "TX_Roving_Counts_df = pd.read_csv(TX_Roving_Counts_filename)\n",
    "\n",
    "########################################### Clean up TX_Roving_Counts ###############################\n",
    "TX_Roving_Counts_df['roving_date'] = pd.to_datetime(\n",
    "    TX_Roving_Counts_df['COMPLETION_DTTM'], format='%d%b%Y:%H:%M:%S'\n",
    ")\n",
    "# limit to counts in 2019 and beyond\n",
    "TX_Roving_Counts_df = TX_Roving_Counts_df[TX_Roving_Counts_df['roving_date'].dt.year >= 2019]\n",
    "# Complete Site_id\n",
    "TX_Roving_Counts_df['Site_id'] = TX_Roving_Counts_df['MAJOR_AREA_CODE']*1000 + \\\n",
    "                                TX_Roving_Counts_df['STATION_CODE']\n",
    "\n",
    "TX_Roving_Counts_df = TX_Roving_Counts_df.drop(columns=['MAJOR_AREA_CODE', 'COMPLETION_DTTM', 'STATION_CODE', 'COUNT_TIME_NUM'])\n",
    "TX_Roving_Counts_df = TX_Roving_Counts_df.rename(columns={'TOTAL_COUNT_NUM': 'BoatCt'})\n",
    "\n",
    "TX_Roving_Counts_df['Site_id_count'] = TX_Roving_Counts_df.groupby('Site_id')['Site_id'].transform('count')\n",
    "########################################### Aggregation by Site_id ###############################\n",
    "TX_Roving_Average_by_site_df = TX_Roving_Counts_df.groupby('Site_id', as_index=False).agg(\n",
    "    Avg_Roving_Ct=('BoatCt', 'mean'),\n",
    "    Total_Roving_Ct=('BoatCt', 'sum'),\n",
    "    Count_of_Roving_Ct=('BoatCt', 'count')\n",
    ")\n",
    "\n",
    "########################################### Aggregation by year_month ################################\n",
    "TX_Roving_Counts_df['year'] = TX_Roving_Counts_df['roving_date'].dt.year\n",
    "TX_Roving_Counts_df['month'] = TX_Roving_Counts_df['roving_date'].dt.month\n",
    "# day_of_week ==> 0 = Monday, 6 = Sunday\n",
    "TX_Roving_Counts_df['day_of_week'] = TX_Roving_Counts_df['roving_date'].dt.weekday  \n",
    "TX_Roving_Counts_df['year_month'] = (TX_Roving_Counts_df['year'])*100 + TX_Roving_Counts_df['month']\n",
    "\n",
    "TX_Roving_Average_by_month_df = TX_Roving_Counts_df.groupby('year_month', as_index=False).agg(\n",
    "    Avg_BoatCt=('BoatCt', 'mean'),\n",
    "    Total_BoatCt=('BoatCt', 'sum'),\n",
    "    Count_of_Roving_Ct=('BoatCt', 'count')\n",
    ")\n",
    "TX_Roving_Average_by_month_df.rename(columns={'Avg_Roving_Ct': 'FullWeek_Avg', \n",
    "                                              'Total_Roving_Ct': 'FullWeek_Tot', \n",
    "                                              'Count_of_Roving_Ct': 'FullWeek_CountObservations',\n",
    "                                             }, inplace=True)\n",
    "######################################################################\n",
    "# Weekend Counts\n",
    "TX_Weekend_df = TX_Roving_Counts_df[\n",
    "    (TX_Roving_Counts_df['day_of_week'] >= 5)]\n",
    "\n",
    "# Calculate the average for the filtered rows grouped by 'year_month'\n",
    "Weekend_avg = TX_Weekend_df.groupby('year_month', as_index=False).agg(\n",
    "    Weekend_Avg_Ct=('BoatCt', 'mean'),\n",
    "    Weekend_Tot_Ct=('BoatCt', 'sum'),\n",
    ")\n",
    "\n",
    "# Merge the filtered averages into the aggregated DataFrame\n",
    "TX_Roving_Average_by_month_df = TX_Roving_Average_by_month_df.merge(\n",
    "    Weekend_avg, on='year_month', how='left'\n",
    ")\n",
    "\n",
    "######################################################################\n",
    "# Weekend for consistently active sites\n",
    "site_ct_min = 44 # 90% of the sites have 44 observations or more with a maximum of 50\n",
    "TX_Weekend_df = TX_Weekend_df[\n",
    "    (TX_Weekend_df['Site_id_count'] >= site_ct_min)]\n",
    "\n",
    "# Calculate the average for the filtered rows grouped by 'year_month'\n",
    "Weekend_avg = TX_Weekend_df.groupby('year_month', as_index=False).agg(\n",
    "    Weekend_Avg_Ct_Active=('BoatCt', 'mean'),\n",
    "    Weekend_Tot_Ct_Active=('BoatCt', 'sum'),\n",
    ")\n",
    "\n",
    "# Merge the filtered averages into the aggregated DataFrame\n",
    "TX_Roving_Average_by_month_df = TX_Roving_Average_by_month_df.merge(\n",
    "    Weekend_avg, on='year_month', how='left'\n",
    ")\n",
    "\n",
    "######################################################################\n",
    "# Weekday Counts\n",
    "TX_Weekday_df = TX_Roving_Counts_df[\n",
    "    (TX_Roving_Counts_df['day_of_week'] < 5)]\n",
    "\n",
    "# Calculate the average for the filtered rows grouped by 'year_month'\n",
    "TX_Weekday_avg = TX_Weekday_df.groupby('year_month', as_index=False).agg(\n",
    "    TX_Weekday_avg_Ct=('BoatCt', 'mean'),\n",
    "    Weekday_Tot_Ct=('BoatCt', 'sum'),\n",
    ")\n",
    "\n",
    "# Merge the filtered averages into the aggregated DataFrame\n",
    "TX_Roving_Average_by_month_df = TX_Roving_Average_by_month_df.merge(\n",
    "    TX_Weekday_avg, on='year_month', how='left'\n",
    ")\n",
    "\n",
    "######################################################################\n",
    "# Weekday for consistently active sites\n",
    "site_ct_min = 44 # 90% of the sites have 44 observations or more with a maximum of 50\n",
    "\n",
    "TX_Weekday_df = TX_Weekday_df[\n",
    "    (TX_Weekday_df['Site_id_count'] >= site_ct_min)]\n",
    "\n",
    "# Calculate the average for the filtered rows grouped by 'year_month'\n",
    "TX_Weekday_avg = TX_Weekday_df.groupby('year_month', as_index=False).agg(\n",
    "    TX_Weekday_avg_Ct_Active=('BoatCt', 'mean'),\n",
    "    Weekday_Tot_Ct_Active=('BoatCt', 'sum'),\n",
    ")\n",
    "\n",
    "# Merge the filtered averages into the aggregated DataFrame\n",
    "TX_Roving_Average_by_month_df = TX_Roving_Average_by_month_df.merge(\n",
    "    TX_Weekday_avg, on='year_month', how='left'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "TX_Roving_Average_by_month_df.head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59253483-36df-434f-9a3e-b2d101f043cd",
   "metadata": {},
   "source": [
    "### Load and prepare AL & MS MRIP Trips data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96653dc-4c08-4439-9254-66ef99524762",
   "metadata": {},
   "source": [
    "Create unifying site ID identifier, New_Site_ID for other MRIP data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13566e6-457f-4bdf-b517-226fb09888c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create New_Site_ID for other MRIP data frames\n",
    "Stations_with_stops_AL_MS_df['New_Site_ID'] = 10000*Stations_with_stops_AL_MS_df['State_Code']+Stations_with_stops_AL_MS_df['Site_id']\n",
    "MRIP_station_points_df['New_Site_ID'] = 10000*MRIP_station_points_df['STATE_CODE']+MRIP_station_points_df['SITE_EXTERNAL_ID']\n",
    "# MRIP_station_points_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd91cd3-3a97-49ed-b2fb-9b286ca83362",
   "metadata": {},
   "source": [
    "Create MRIP data set for calibration -- this is trips data for charter and private-rental boats in Alabama and Mississippi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c318808-e8c4-4e31-8222-f201aad6c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRIP data can be \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Base directory\n",
    "base_path = Path(r\"S:\\Database\\MRIP\")\n",
    "\n",
    "# Columns to keep\n",
    "cols_to_keep = [\n",
    "    \"strat_id\", \"psu_id\", \"YEAR\", \"ST\", \"CNTY\", \"INTSITE\", \"MODE_FX\",\n",
    "    \"AREA_X\", \"PARTY\", \"WAVE\", \"kod\", \"month\", \"TIME\", \"DIST\",\n",
    "    \"ART_REEF\", \"wp_int\"\n",
    "]\n",
    "\n",
    "# List of filenames (without .csv)\n",
    "file_stems = [\n",
    "    \"trip_20191\", \"trip_20192\", \"trip_20193\", \"trip_20194\", \"trip_20195\", \"trip_20196\",\n",
    "    \"trip_20201\", \"trip_20202\", \"trip_20203\", \"trip_20204\", \"trip_20205\", \"trip_20206\",\n",
    "    \"trip_20211\", \"trip_20212\", \"trip_20213\", \"trip_20214\", \"trip_20215\", \"trip_20216\",\n",
    "    \"trip_20221\", \"trip_20222\", \"trip_20223\", \"trip_20224\", \"trip_20225\", \"trip_20226\"\n",
    "]\n",
    "\n",
    "# Initialize empty list for storing filtered DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop over all files\n",
    "for stem in file_stems:\n",
    "    file_path = base_path / f\"{stem}.csv\"\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    df = pd.read_csv(file_path, usecols=cols_to_keep)\n",
    "\n",
    "    # Apply filters\n",
    "    df = df[df[\"MODE_FX\"].isin([5, 7])]\n",
    "    df = df[df[\"ST\"].isin([1, 28])]\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all filtered DataFrames\n",
    "AL_MS_MRIP_Trips_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Optional: save combined dataset\n",
    "AL_MS_MRIP_Trips_df.to_csv(MRIP_trips_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e8ffb1-52c3-4b93-bfd2-7268ebbabc4c",
   "metadata": {},
   "source": [
    "Load MRIP data and create unifying site ID identifier, New_Site_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda0bbc-6096-48dd-885c-d5b964424d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the MRIP data is already processed, it can be loaded directly\n",
    "AL_MS_MRIP_Trips_df = pd.read_csv(MRIP_trips_filename)\n",
    "print(MRIP_trips_filename)\n",
    "AL_MS_MRIP_Trips_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f34d4-80d5-4af5-baa2-a2b44ba24db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_MS_MRIP_Trips_df = pd.read_csv(MRIP_trips_filename)\n",
    "\n",
    "# Clean up and create new variables\n",
    "AL_MS_MRIP_Trips_df['New_Site_ID'] = 10000*AL_MS_MRIP_Trips_df['ST']+AL_MS_MRIP_Trips_df['INTSITE']\n",
    "\n",
    "AL_MS_MRIP_Trips_df['year_wave'] = (AL_MS_MRIP_Trips_df['YEAR'])*100 + AL_MS_MRIP_Trips_df['WAVE']\n",
    "AL_MS_MRIP_Trips_df = AL_MS_MRIP_Trips_df[AL_MS_MRIP_Trips_df['year_wave']<=202202]\n",
    "                                          \n",
    "AL_MS_MRIP_Trips_df = AL_MS_MRIP_Trips_df.drop(columns=['strat_id', 'psu_id'])\n",
    "\n",
    "# Group by 'New_Site_ID' and count the unique values of 'year_wave'\n",
    "Site_visits_df = AL_MS_MRIP_Trips_df.groupby('New_Site_ID')['year_wave'].nunique().reset_index()\n",
    "Site_visits_df.columns = ['New_Site_ID', 'Visits_by_enumerators']\n",
    "\n",
    "# Average values per interview -- all trips\n",
    "AL_MS_MRIP_by_site_df = AL_MS_MRIP_Trips_df.groupby('New_Site_ID', as_index=False).agg(\n",
    "    TripCount_All=(\"wp_int\", \"size\"),  # Count the number of trips\n",
    "    WeightedTrips_All=(\"wp_int\", \"sum\"),  # Sum the 'wp_int' values for each group\n",
    "    Interviews=(\"WAVE\", \"nunique\")  # Count the number of unique 'WAVE' values\n",
    ")\n",
    "AL_MS_MRIP_by_site_df['TripCount_per_visit'] = AL_MS_MRIP_by_site_df['TripCount_All']/AL_MS_MRIP_by_site_df['Interviews']\n",
    "AL_MS_MRIP_by_site_df['WeightedTrips_per_visit'] = AL_MS_MRIP_by_site_df['WeightedTrips_All']/AL_MS_MRIP_by_site_df['Interviews']\n",
    "\n",
    "# Average values per interview -- offshore trips only (DIST == 2 => > 3 miles )\n",
    "AL_MS_MRIP_Offshore_Trips_df=AL_MS_MRIP_Trips_df[AL_MS_MRIP_Trips_df['DIST']==2]\n",
    "\n",
    "AL_MS_MRIP_Offshore_by_site_df = AL_MS_MRIP_Offshore_Trips_df.groupby('New_Site_ID', as_index=False).agg(\n",
    "    TripCount_All=(\"wp_int\", \"size\"),  # Count the number of trips\n",
    "    WeightedTrips_All=(\"wp_int\", \"sum\"),  # Sum the 'wp_int' values for each group\n",
    "    Interviews=(\"WAVE\", \"nunique\")  # Count the number of unique 'WAVE' values\n",
    ")\n",
    "AL_MS_MRIP_Offshore_by_site_df['TripCount_per_visit'] = AL_MS_MRIP_Offshore_by_site_df['TripCount_All']/AL_MS_MRIP_Offshore_by_site_df['Interviews']\n",
    "AL_MS_MRIP_Offshore_by_site_df['WeightedTrips_per_visit'] = AL_MS_MRIP_Offshore_by_site_df['WeightedTrips_All']/AL_MS_MRIP_Offshore_by_site_df['Interviews']\n",
    "\n",
    "AL_MS_MRIP_Offshore_by_site_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812eadb-d995-47a9-ace0-8dba4eb78954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of roving count data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Drop duplicates to include only one value per Site_id\n",
    "unique_site_counts_TX = TX_Roving_Counts_df.drop_duplicates(subset='Site_id')['Site_id_count']\n",
    "\n",
    "# Sort the unique Site_id_count values\n",
    "sorted_counts = unique_site_counts_TX.sort_values()\n",
    "\n",
    "# Calculate the CDF\n",
    "cdf = sorted_counts.rank(method='average', pct=True)\n",
    "\n",
    "# Plot the CDF\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(sorted_counts, cdf, marker='.', linestyle='none')\n",
    "\n",
    "# Add horizontal grid lines every 5%\n",
    "plt.yticks(ticks=[i / 20 for i in range(21)])  # 0, 0.05, 0.10, ..., 1.00\n",
    "plt.grid(axis='y', linestyle='--', color='gray', alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Cumulative Distribution Function of Site_id_count')\n",
    "plt.xlabel('Site_id_count')\n",
    "plt.ylabel('CDF')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8807739e-efa8-4e66-9f2a-132927af6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the data\n",
    "# data = AL_MS_MRIP_Offshore_by_site_df['TripCount_per_visit']\n",
    "data = AL_MS_MRIP_by_site_df['WeightedTrips_per_visit']\n",
    "\n",
    "# Sort the data\n",
    "sorted_data = np.sort(data)\n",
    "\n",
    "# Calculate the ECDF\n",
    "ecdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "\n",
    "# Plot the ECDF\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.step(sorted_data, ecdf, where=\"post\", color='blue', label='ECDF of TripCount_per_visit')\n",
    "plt.xlabel('TripCount_per_visit')\n",
    "plt.ylabel('ECDF')\n",
    "plt.title('Empirical Cumulative Distribution Function (ECDF)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bdf5a5-c480-4e2d-b0c4-b450b5469de8",
   "metadata": {},
   "source": [
    "### TX: Create Clusters for Relevant Site_id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac04358-56f2-434e-a3b5-74a052783cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load TX_station_points_df and reduce to only those rows that appear in either the roving counts or mobility data df's\n",
    "TX_station_points_df['Site_id'] = (TX_station_points_df['major_area_code']*1000 + TX_station_points_df['station_code']).astype(int)\n",
    "TX_relevant_site_ids = set(TX_Roving_Counts_df['Site_id']).union(Mobility_Visits_TX_df['Site_id'])\n",
    "Relevant_TX_stations_df = TX_station_points_df[TX_station_points_df['Site_id'].isin(TX_relevant_site_ids)]\n",
    "\n",
    "# Prepare the data: Extract latitude and longitude\n",
    "coords = Relevant_TX_stations_df[['lat', 'lng']].values  # lat, lng columns\n",
    "\n",
    "# Convert degrees to radians (required for haversine distance)\n",
    "coords = np.radians(coords)\n",
    "\n",
    "# Set eps for 1 km (0.0001569 radians corresponds to 1 km)\n",
    "km_boundary = 1\n",
    "eps_value = 0.0001569*km_boundary\n",
    "\n",
    "# Apply DBSCAN with Haversine distance (metric='haversine' works with radians)\n",
    "db = DBSCAN(eps=eps_value, min_samples=1, metric='haversine')  # Adjust eps and min_samples\n",
    "\n",
    "# Fit DBSCAN\n",
    "Relevant_TX_stations_df['cluster'] = db.fit_predict(coords)\n",
    "\n",
    "Station_Clusters_df = Relevant_TX_stations_df[['Site_id', 'cluster', 'lat', 'lng']]\n",
    "\n",
    "# Print the number of sites in each cluster\n",
    "cluster_counts = Station_Clusters_df['cluster'].value_counts()\n",
    "Station_Clusters_df['sites_by_Cluster'] = Station_Clusters_df['cluster'].map(cluster_counts)\n",
    "\n",
    "# Print the cluster counts (excluding noise points labeled as -1)\n",
    "print(\"There are \", len(cluster_counts), \"for the \", len(Station_Clusters_df), \"unique stations\")\n",
    "# Station_Clusters_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602979ed-b11b-46a4-8732-69e7350a472b",
   "metadata": {},
   "source": [
    "### TX: Add clusters to Roving counts and create df by cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f5f0b-d1c1-430e-a245-bd4e2b8c29c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cluster to TX_Roving_Counts_df and Mobility_Visits_TX_df\n",
    "TX_Roving_Average_by_site_df2 = TX_Roving_Average_by_site_df.merge(\n",
    "    Station_Clusters_df[['Site_id', 'cluster', 'lat', 'lng']],  # Columns to merge\n",
    "    on='Site_id',                                # Merge key\n",
    "    how='left'                                   # Retain all rows in TX_Roving_Counts_df\n",
    ")\n",
    "# Columns in TX_Roving_Average_by_site_df\n",
    "##              Avg_Roving_Ct\tTotal_Roving_Ct\tCount_of_Roving_Ct\n",
    "\n",
    "# Group by 'cluster' and apply aggregation functions\n",
    "TX_Roving_Counts_by_cluster_df = TX_Roving_Average_by_site_df2.groupby('cluster', as_index=False).agg({\n",
    "    'lat': 'mean',                    # Average latitude\n",
    "    'lng': 'mean',                    # Average longitude\n",
    "    'Avg_Roving_Ct': 'sum',         # Sum of TOTAL_COUNT_NUM\n",
    "    'Count_of_Roving_Ct': 'sum',                # Count of rows in each cluster\n",
    "    'Total_Roving_Ct': 'sum'                # Sum of all \n",
    "}).rename(columns={\n",
    "    'Avg_Roving_Ct': 'ClusterTotal_Avg_Roving_Ct',  # Rename TOTAL_COUNT_NUM to Roving_Ct_Sum\n",
    "    'Count_of_Roving_Ct': 'Num_of_Counts',             # Rename cluster (count) to Num_of_Cts\n",
    "    'Total_Roving_Ct': 'All_Boats_Counted'             # Rename cluster (count) to Num_of_Cts\n",
    "})\n",
    "         \n",
    "unique_clusters_count = TX_Roving_Counts_by_cluster_df['cluster'].nunique()\n",
    "print(f\"The number of unique clusters is: {unique_clusters_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f555fb-9260-4296-bc4a-c55d9443aa00",
   "metadata": {},
   "source": [
    "### TX: Add clusters to visits and create df by cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f15e2a-7a8f-4350-a292-9d6dcdb9d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mobility_Visits_TX_df.head(6)\n",
    "# Add the cluster to TX_Roving_Counts_df and Mobility_Visits_TX_df\n",
    "Mobility_Visits_TX_df2 = Mobility_Visits_TX_df.merge(\n",
    "    Station_Clusters_df[['Site_id', 'cluster']],  # Columns to merge\n",
    "    on='Site_id',                                # Merge key\n",
    "    how='left'                                   # Retain all rows in TX_Roving_Counts_df\n",
    ")\n",
    "\n",
    "Mobility_Visits_by_cluster_df = Mobility_Visits_TX_df2.groupby('cluster', as_index=False).agg({\n",
    "    'lat': 'mean',                    # Average latitude\n",
    "    'lng': 'mean',                    # Average longitude\n",
    "    'Total_mobility_visits': 'sum',            # Sum of Total_mobility_visits\n",
    "    'complete_visits': 'sum',         # Sum of complete_visits\n",
    "    'Site_id': 'count'                # Count of Site_id (for example, count of rows per cluster)\n",
    "}).rename(columns={\n",
    "    'Total_mobility_visits': 'Total_mobility_visits_by_cluster',  # Rename to descriptive names\n",
    "    'complete_visits': 'complete_visits_by_cluster',\n",
    "    'Site_id': 'sites_by_Cluster'               # Rename to 'sites_by_Cluster'\n",
    "})\n",
    "\n",
    "unique_clusters_count = Mobility_Visits_by_cluster_df['cluster'].nunique()\n",
    "print(f\"The number of unique clusters is: {unique_clusters_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3f5be-7dad-4a2d-af7a-150aefabf1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique 'cluster' values from both DataFrames\n",
    "clusters_mobility = set(Mobility_Visits_by_cluster_df['cluster'])\n",
    "clusters_roving = set(TX_Roving_Counts_by_cluster_df['cluster'])\n",
    "\n",
    "# Find the intersection (clusters that appear in both DataFrames)\n",
    "common_clusters = clusters_mobility & clusters_roving\n",
    "\n",
    "# Find the clusters in Mobility_Visits_by_cluster_df but not in TX_Roving_Counts_by_cluster_df\n",
    "mobility_only_clusters = clusters_mobility - clusters_roving\n",
    "\n",
    "# Find the clusters in TX_Roving_Counts_by_cluster_df but not in Mobility_Visits_by_cluster_df\n",
    "roving_only_clusters = clusters_roving - clusters_mobility\n",
    "\n",
    "# Print the counts\n",
    "print(\"with km_boundary = \", km_boundary)\n",
    "print(f\"Number of clusters in both Mobility_Visits_by_cluster_df and TX_Roving_Counts_by_cluster_df: {len(common_clusters)}\")\n",
    "print(f\"Number of clusters in Mobility_Visits_by_cluster_df but not in TX_Roving_Counts_by_cluster_df: {len(mobility_only_clusters)}\")\n",
    "print(f\"Number of clusters in TX_Roving_Counts_by_cluster_df but not in Mobility_Visits_by_cluster_df: {len(roving_only_clusters)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9acd02f-6b0e-4d79-a29d-0792bdcb8478",
   "metadata": {},
   "source": [
    "## Clusters for MRIP data in AL and MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9c336d-1590-4cf7-b3ff-bbe627804a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Lat & lng to site lists\n",
    "\n",
    "# Perform the merge operation\n",
    "AL_MS_MRIP_by_site_df = AL_MS_MRIP_by_site_df.drop(columns=['lat', 'lng'], errors='ignore')\n",
    "AL_MS_MRIP_by_site_df = AL_MS_MRIP_by_site_df.merge(\n",
    "    MRIP_station_points_df[['New_Site_ID', 'lat', 'lng']],  # Select only relevant columns\n",
    "    on='New_Site_ID',  # Merge on the 'New_Site_ID' column\n",
    "    how='left'  # Perform a left join to keep all rows from AL_MS_MRIP_Offshore_by_site_df\n",
    ")\n",
    "# MS site # 32 does not appear in any other data sets. I suspect this is an error in the underlying data\n",
    "AL_MS_MRIP_by_site_df = AL_MS_MRIP_by_site_df.dropna(subset=['lat'])\n",
    "\n",
    "\n",
    "# Perform the merge operation\n",
    "AL_MS_MRIP_Offshore_by_site_df = AL_MS_MRIP_Offshore_by_site_df.drop(columns=['lat', 'lng'], errors='ignore')\n",
    "AL_MS_MRIP_Offshore_by_site_df = AL_MS_MRIP_Offshore_by_site_df.merge(\n",
    "    MRIP_station_points_df[['New_Site_ID', 'lat', 'lng']],  # Select only relevant columns\n",
    "    on='New_Site_ID',  # Merge on the 'New_Site_ID' column\n",
    "    how='left'  # Perform a left join to keep all rows from AL_MS_MRIP_Offshore_by_site_df\n",
    ")\n",
    "# MS site # 32 does not appear in any other data sets. I suspect this is an error in the underlying data\n",
    "AL_MS_MRIP_Offshore_by_site_df = AL_MS_MRIP_Offshore_by_site_df.dropna(subset=['lat'])\n",
    "AL_MS_MRIP_Offshore_by_site_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0299d492-2346-4592-8ffb-9cd5d317d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Prepare the data: Extract latitude and longitude\n",
    "coords = AL_MS_MRIP_by_site_df[['lat', 'lng']].values  # lat, lng columns\n",
    "\n",
    "# Convert degrees to radians (required for haversine distance)\n",
    "coords = np.radians(coords)\n",
    "\n",
    "# Set eps for 1 km (0.0001569 radians corresponds to 1 km)\n",
    "km_boundary = 1\n",
    "eps_value = 0.0001569*km_boundary\n",
    "\n",
    "# Apply DBSCAN with Haversine distance (metric='haversine' works with radians)\n",
    "db = DBSCAN(eps=eps_value, min_samples=1, metric='haversine')  # Adjust eps and min_samples\n",
    "\n",
    "# Fit DBSCAN\n",
    "AL_MS_MRIP_by_site_df['cluster'] = db.fit_predict(coords)\n",
    "AL_MS_MRIP_by_site_df\n",
    "AL_MS_MRIP_Clusters_df = AL_MS_MRIP_by_site_df[['New_Site_ID', 'cluster', 'lat', 'lng']]\n",
    "\n",
    "# Print the number of sites in each cluster\n",
    "cluster_counts = AL_MS_MRIP_Clusters_df['cluster'].value_counts()\n",
    "AL_MS_MRIP_Clusters_df['sites_by_Cluster'] = AL_MS_MRIP_Clusters_df['cluster'].map(cluster_counts)\n",
    "\n",
    "## Add in the average lat and lgn for each cluster\n",
    "cluster_avg = AL_MS_MRIP_Clusters_df.groupby('cluster')[['lat', 'lng']].mean().reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "cluster_avg.rename(columns={'lat': 'cluster_lat', 'lng': 'cluster_lng'}, inplace=True)\n",
    "\n",
    "# Merge the averaged values back into the original DataFrame\n",
    "AL_MS_MRIP_Clusters_df = AL_MS_MRIP_Clusters_df.merge(cluster_avg, on='cluster', how='left')\n",
    "\n",
    "\n",
    "# Print the cluster counts (excluding noise points labeled as -1)\n",
    "print(\"There are \", len(cluster_counts), \"for the \", len(AL_MS_MRIP_Clusters_df), \"unique stations\")\n",
    "AL_MS_MRIP_Clusters_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab21ce1-ae2a-47c0-9864-1ef2bc3982b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Clusters to the MRIP trips data\n",
    "AL_MS_MRIP_Trips_df = AL_MS_MRIP_Trips_df.drop(columns=['cluster', 'lat', 'lng', 'cluster_lat', 'cluster_lng'], errors='ignore')\n",
    "\n",
    "AL_MS_MRIP_Trips_df = AL_MS_MRIP_Trips_df.merge(\n",
    "    AL_MS_MRIP_Clusters_df[['New_Site_ID', 'cluster', 'lat', 'lng', 'cluster_lat', 'cluster_lng']],  # Select only relevant columns\n",
    "    on='New_Site_ID',  # Merge on the 'New_Site_ID' column\n",
    "    how='left'  # Perform a left join to keep all rows from AL_MS_MRIP_Offshore_by_site_df\n",
    ")\n",
    "\n",
    "##################################################################################################################\n",
    "# Group by 'New_Site_ID' and count the unique values of 'year_wave'\n",
    "Cluster_visits_df = AL_MS_MRIP_Trips_df.groupby('cluster')['year_wave'].nunique().reset_index()\n",
    "Cluster_visits_df.columns = ['cluster', 'Visits_by_enumerators']\n",
    "\n",
    "##################################################################################################################\n",
    "# Average values per interview -- all trips\n",
    "AL_MS_MRIP_by_cluster_df = AL_MS_MRIP_Trips_df.groupby('cluster', as_index=False).agg(\n",
    "    TripCount_All=(\"wp_int\", \"size\"),  # Count the number of trips\n",
    "    WeightedTrips_All=(\"wp_int\", \"sum\"),  # Sum the 'wp_int' values for each group\n",
    "    Interviews=(\"WAVE\", \"nunique\")  # Count the number of unique 'WAVE' values\n",
    ")\n",
    "AL_MS_MRIP_by_cluster_df['TripCount_per_visit'] = AL_MS_MRIP_by_cluster_df['TripCount_All']/AL_MS_MRIP_by_cluster_df['Interviews']\n",
    "AL_MS_MRIP_by_cluster_df['WeightedTrips_per_visit'] = AL_MS_MRIP_by_cluster_df['WeightedTrips_All']/AL_MS_MRIP_by_cluster_df['Interviews']\n",
    "\n",
    "##################################################################################################################\n",
    "# Average values per interview -- grouped by cluster and year_wave\n",
    "AL_MS_MRIP_by_year_wave_df = AL_MS_MRIP_Trips_df.groupby(['year_wave'], as_index=False).agg(\n",
    "    TripCount_All=(\"wp_int\", \"size\"),  # Count the number of trips\n",
    "    WeightedTrips_All=(\"wp_int\", \"sum\"),  # Sum the 'wp_int' values for each group\n",
    "    Interviews=(\"year_wave\", \"nunique\")  # Count the number of unique 'WAVE' values\n",
    ")\n",
    "\n",
    "# Calculate per-visit averages\n",
    "AL_MS_MRIP_by_year_wave_df['TripCount_per_visit'] = (\n",
    "    AL_MS_MRIP_by_year_wave_df['TripCount_All'] / AL_MS_MRIP_by_year_wave_df['Interviews']\n",
    ")\n",
    "AL_MS_MRIP_by_year_wave_df['WeightedTrips_per_visit'] = (\n",
    "    AL_MS_MRIP_by_year_wave_df['WeightedTrips_All'] / AL_MS_MRIP_by_year_wave_df['Interviews']\n",
    ")\n",
    "\n",
    "##################################################################################################################\n",
    "##################### Repeat for offshore trips ###############################\n",
    "# Average values per interview -- offshore trips only (DIST == 2 => > 3 miles )\n",
    "AL_MS_MRIP_Offshore_Trips_df=AL_MS_MRIP_Trips_df[AL_MS_MRIP_Trips_df['DIST']==2]\n",
    "\n",
    "AL_MS_MRIP_Offshore_by_cluster_df = AL_MS_MRIP_Offshore_Trips_df.groupby('cluster', as_index=False).agg(\n",
    "    TripCount_All=(\"wp_int\", \"size\"),  # Count the number of trips\n",
    "    WeightedTrips_All=(\"wp_int\", \"sum\"),  # Sum the 'wp_int' values for each group\n",
    "    Interviews=(\"WAVE\", \"nunique\")  # Count the number of unique 'WAVE' values\n",
    ")\n",
    "AL_MS_MRIP_Offshore_by_cluster_df['TripCount_per_visit'] = AL_MS_MRIP_Offshore_by_cluster_df['TripCount_All']/AL_MS_MRIP_Offshore_by_cluster_df['Interviews']\n",
    "AL_MS_MRIP_Offshore_by_cluster_df['WeightedTrips_per_visit'] = AL_MS_MRIP_Offshore_by_cluster_df['WeightedTrips_All']/AL_MS_MRIP_Offshore_by_cluster_df['Interviews']\n",
    "\n",
    "##################################################################################################################\n",
    "# Average values per interview -- grouped by cluster and year_wave\n",
    "AL_MS_MRIP_Offshore_by_year_wave_df = AL_MS_MRIP_Offshore_Trips_df.groupby(['year_wave'], as_index=False).agg(\n",
    "    TripCount_All=(\"wp_int\", \"size\"),  # Count the number of trips\n",
    "    WeightedTrips_All=(\"wp_int\", \"sum\"),  # Sum the 'wp_int' values for each group\n",
    "    Interviews=(\"year_wave\", \"nunique\")  # Count the number of unique 'WAVE' values\n",
    ")\n",
    "\n",
    "# Calculate per-visit averages\n",
    "AL_MS_MRIP_Offshore_by_year_wave_df['TripCount_per_visit'] = (\n",
    "    AL_MS_MRIP_Offshore_by_year_wave_df['TripCount_All'] / AL_MS_MRIP_Offshore_by_year_wave_df['Interviews']\n",
    ")\n",
    "AL_MS_MRIP_Offshore_by_year_wave_df['WeightedTrips_per_visit'] = (\n",
    "    AL_MS_MRIP_Offshore_by_year_wave_df['WeightedTrips_All'] / AL_MS_MRIP_Offshore_by_year_wave_df['Interviews']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "AL_MS_MRIP_Offshore_by_cluster_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7737b0-7a70-4b78-97bf-7ece8bafa713",
   "metadata": {},
   "source": [
    "# MRIP Validation\n",
    "### AL & MS Add clusters to mobility data visits and create df by cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5001a-7e37-4b3d-bf42-ad09e066cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Mobility data with cluster\n",
    "if 'Mobility_Visits_AL_MS_by_site_df2' in globals():\n",
    "    del Mobility_Visits_AL_MS_by_site_df2\n",
    "\n",
    "# Merge the DataFrames while ensuring existing columns are replaced\n",
    "Mobility_Visits_AL_MS_by_site_df2 = Mobility_Visits_AL_MS_by_site_df.merge(\n",
    "    AL_MS_MRIP_Clusters_df[['New_Site_ID', 'cluster', 'cluster_lat', 'cluster_lng']],  \n",
    "    on='New_Site_ID',  \n",
    "    how='left'  \n",
    ")\n",
    "# print(Mobility_Visits_AL_MS_by_site_df2.columns)\n",
    "\n",
    "Mobility_Visits_AL_MS_by_cluster_df = Mobility_Visits_AL_MS_by_site_df2.groupby('cluster', as_index=False).agg({\n",
    "    'cluster_lat': 'mean',                    # Average latitude\n",
    "    'cluster_lng': 'mean',                    # Average longitude\n",
    "    'Total_mobility_visits': 'sum',            # Sum of Total_mobility_visits\n",
    "    'complete_visits': 'sum',         # Sum of complete_visits\n",
    "    'New_Site_ID': 'count'                # Count of Site_id (for example, count of rows per cluster)\n",
    "}).rename(columns={\n",
    "    'Total_mobility_visits': 'Total_mobility_visits_by_cluster',  # Rename to descriptive names\n",
    "    'complete_visits': 'complete_mobility_visits_by_cluster',\n",
    "    'New_Site_ID': 'sites_per_Cluster'               # Rename to 'sites_by_Cluster'\n",
    "})\n",
    "\n",
    "# unique_clusters_count = Mobility_Visits_by_cluster_df['cluster'].nunique()\n",
    "# print(f\"The number of unique clusters is: {unique_clusters_count}\")\n",
    "Mobility_Visits_AL_MS_by_cluster_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d465138c-9ea0-4302-8d27-51b538d56984",
   "metadata": {},
   "source": [
    "## MRIP Validation over space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b146015-c3ea-4c14-a5fb-23cf1b911790",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## MRIP Validation over space\")\n",
    "print(\"\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "### RUN OPTIONS\n",
    "# # ALL TRIPS UNWEIGHTED\n",
    "MRIP_df = AL_MS_MRIP_by_cluster_df\n",
    "x_col = 'TripCount_per_visit' #'WeightedTrips_per_visit'  # Change this to use a different X variable\n",
    "x_title = 'MRIP access point surveys'\n",
    "print(x_title, len(MRIP_df))\n",
    "\n",
    "# ALL TRIPS WEIGHTED\n",
    "# MRIP_df = AL_MS_MRIP_by_cluster_df\n",
    "# x_col = 'WeightedTrips_per_visit' #''  # Change this to use a different X variable\n",
    "# x_title = 'MRIP weighted  access point surveys per cluster'\n",
    "# print(x_title, len(MRIP_df))\n",
    "\n",
    "# # OFFSHORE TRIPS UNWEIGHTED\n",
    "# MRIP_df = AL_MS_MRIP_Offshore_by_cluster_df\n",
    "# x_col = 'TripCount_per_visit' #'WeightedTrips_per_visit'  # Change this to use a different X variable\n",
    "# x_title = 'Offshore trips recorded in MRIP access point surveys per cluster'\n",
    "# print(x_title, len(MRIP_df))\n",
    "\n",
    "# # OFFSHORE TRIPS WEIGHTED\n",
    "# MRIP_df = AL_MS_MRIP_Offshore_by_cluster_df\n",
    "# x_col = 'WeightedTrips_per_visit' #''  # Change this to use a different X variable\n",
    "# x_title = 'MRIP Weighted offshore trips access point surveys per cluster'\n",
    "# print(x_title, len(MRIP_df))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "y_col = 'Total_mobility_visits_by_cluster'  # Change this to use a different Y variable\n",
    "y_title = 'mobility data visits'  # Change this to use a different Y variable\n",
    "\n",
    "\n",
    "\n",
    "# # MERGING WITH ALL MRIP TRIPS\n",
    "# merged_df = Mobility_Visits_AL_MS_by_cluster_df.merge(\n",
    "#     MRIP_df[['cluster', 'WeightedTrips_per_visit', 'TripCount_per_visit']],\n",
    "#     on='cluster',\n",
    "#     how='inner'  # Use 'inner' to keep only matching clusters\n",
    "# )\n",
    "\n",
    "# MERGING WITH OFFSHORE MRIP TRIPS\n",
    "merged_df = Mobility_Visits_AL_MS_by_cluster_df.merge(\n",
    "    MRIP_df[['cluster', 'WeightedTrips_per_visit', 'TripCount_per_visit']],\n",
    "    on='cluster',\n",
    "    how='inner'  # Use 'inner' to keep only matching clusters\n",
    ")\n",
    "\n",
    "\n",
    "# print(merged_df.columns)\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Take log10 of selected columns\n",
    "x = np.log10(merged_df[x_col])\n",
    "y = np.log10(merged_df[y_col])\n",
    "correlation = np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "# Calculate Pearson and Spearman correlations and p-values\n",
    "r, p_val = pearsonr(x, y)\n",
    "print(f\"Pearson correlation: {r:.3f} (p = {p_val:.3g})\")\n",
    "spearman_corr, spearman_pval = spearmanr(x, y)\n",
    "print(f\"Spearman correlation: {spearman_corr:.4f} (p-value: {spearman_pval:.4g})\")\n",
    "\n",
    "# Fit the linear regression model\n",
    "x_with_const = sm.add_constant(x)  # Adds an intercept term\n",
    "model = sm.OLS(y, x_with_const)\n",
    "results = model.fit()\n",
    "\n",
    "# Extract regression results\n",
    "coefficients = results.params\n",
    "standard_errors = results.bse\n",
    "trendline = results.predict(x_with_const)\n",
    "\n",
    "slope = coefficients[x_col]\n",
    "slope_se = standard_errors[x_col]\n",
    "\n",
    "\n",
    "# Increase font sizes for the plot\n",
    "font_size = 18\n",
    "plt.rcParams.update({'font.size': font_size})\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x, y, color='blue', alpha=0.7, label='Data Points')\n",
    "\n",
    "\n",
    "# Plot the trend line\n",
    "plt.plot(x, trendline, color='red', linestyle='--', label='Trend Line')\n",
    "\n",
    "# Add labels and title dynamically\n",
    "plt.xlabel(f'Log (10) of {x_title}', fontsize=font_size)\n",
    "plt.ylabel(f'Log (10) of {y_title}', fontsize=font_size)\n",
    "\n",
    "###################\n",
    "y_min, y_max = plt.ylim()  # Get the y-axis limits\n",
    "x_min, x_max = plt.xlim()  # Get the x-axis limits\n",
    "\n",
    "plt.text(\n",
    "    x_max,                       # x coordinate at the rightmost point\n",
    "    y_min + 0.07*(y_max - y_min),  # keep your desired vertical offset\n",
    "    f\"Correlation coefficient: {correlation:.2f}\",\n",
    "    color='black',\n",
    "    fontsize=font_size,\n",
    "    ha='right'                   # anchor text to its right edge\n",
    ")\n",
    "##################\n",
    "\n",
    "plt.legend()\n",
    "# Show the grid for better readability\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###################### Conduct a Chi-Squared contingency test ############\n",
    "\n",
    "# Define columns dynamically\n",
    "\n",
    "# Create a contingency table using selected columns\n",
    "contingency_table = pd.DataFrame({\n",
    "    'Visits_data': merged_df[x_col],\n",
    "    'Counts_data': merged_df[y_col]\n",
    "})\n",
    "\n",
    "# Perform the chi-squared test\n",
    "from scipy.stats import chi2_contingency\n",
    "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Print the coefficients of the fitted line\n",
    "print(\"Coefficients of fitted line\", coefficients)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nResults of the Chi-Squared contingency test\")\n",
    "print(\"Chi-squared Statistic:\", chi2_stat)\n",
    "print(\"P-value:\", p_value)\n",
    "print(\"Degrees of Freedom:\", dof)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.01  # Significance level\n",
    "if p_value < alpha:\n",
    "    print(f\"At the {alpha*100:.0f}% level, reject the null hypothesis. The data are likely not generated by the same process.\")\n",
    "else:\n",
    "    print(f\"At the {alpha*100:.0f}% level, fail to reject the null hypothesis. The data may come from the same process.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf0ec8d-ad84-44b6-90ac-3b346c2e2ffa",
   "metadata": {},
   "source": [
    "## Validation graph -- MRIP variation over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195adf5-88cb-47ce-a72f-41858b3b4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## Validation graph -- MRIP variation over time\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### RUN OPTIONS\n",
    "GraphName = \"ALL TRIPS UNWEIGHTED\"\n",
    "MRIP_df = AL_MS_MRIP_by_year_wave_df\n",
    "x_col = 'TripCount_per_visit' #'WeightedTrips_per_visit'  # Change this to use a different X variable\n",
    "x_title = 'MRIP access point surveys'\n",
    "\n",
    "# GraphName = \"ALL TRIPS WEIGHTED\"\n",
    "# MRIP_df = AL_MS_MRIP_by_year_wave_df\n",
    "# x_col = 'WeightedTrips_per_visit' #''  # Change this to use a different X variable\n",
    "# x_title = 'weighted MRIP access point surveys'\n",
    "\n",
    "# GraphName = \"OFFSHORE TRIPS UNWEIGHTED\"\n",
    "# MRIP_df = AL_MS_MRIP_Offshore_by_year_wave_df\n",
    "# x_col = 'TripCount_per_visit' #'WeightedTrips_per_visit'  # Change this to use a different X variable\n",
    "# x_title = 'offshore MRIP access point surveys'\n",
    "\n",
    "# GraphName = \"OFFSHORE TRIPS WEIGHTED\"\n",
    "# MRIP_df = AL_MS_MRIP_Offshore_by_year_wave_df\n",
    "# x_col = 'WeightedTrips_per_visit' #''  # Change this to use a different X variable\n",
    "# x_title = 'offshore weighted MRIP access point surveys'\n",
    "\n",
    "merged_df = Mobility_Visits_AL_MS_by_year_wave_df.merge(\n",
    "    MRIP_df[['year_wave', 'WeightedTrips_per_visit', 'TripCount_per_visit']],\n",
    "    on='year_wave',\n",
    "    how='inner'  # Use 'inner' to keep only matching clusters\n",
    ")\n",
    "# print(merged_df.columns)\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose columns dynamically\n",
    "y_col = 'Total_mobility_visits'  # Change this to use a different Y variable\n",
    "y_title = 'mobility data visits'  # Change this to use a different Y variable\n",
    "\n",
    "# Take log10 of selected columns\n",
    "x = np.log10(merged_df[x_col])\n",
    "# print(merged_df[y_col].min())\n",
    "y = np.log10(merged_df[y_col])\n",
    "correlation = np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "# Calculate Pearson and Spearman correlations and p-values\n",
    "r, p_val = pearsonr(x, y)\n",
    "print(f\"Pearson correlation: {r:.3f} (p = {p_val:.3g})\")\n",
    "spearman_corr, spearman_pval = spearmanr(x, y)\n",
    "print(f\"Spearman correlation: {spearman_corr:.4f} (p-value: {spearman_pval:.4g})\")\n",
    "\n",
    "\n",
    "# Fit the linear regression model\n",
    "x_with_const = sm.add_constant(x)  # Adds an intercept term\n",
    "model = sm.OLS(y, x_with_const)\n",
    "results = model.fit()\n",
    "\n",
    "# Extract regression results\n",
    "coefficients = results.params\n",
    "standard_errors = results.bse\n",
    "trendline = results.predict(x_with_const)\n",
    "\n",
    "slope = coefficients[x_col]\n",
    "slope_se = standard_errors[x_col]\n",
    "\n",
    "# Increase font sizes for the plot\n",
    "font_size = 18\n",
    "plt.rcParams.update({'font.size': font_size})\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x, y, color='blue', alpha=0.7, label='Data Points')\n",
    "\n",
    "# Plot the trend line\n",
    "plt.plot(x, trendline, color='red', linestyle='--', label='Trend Line')\n",
    "\n",
    "# print (GraphName)\n",
    "# Add labels and title dynamically\n",
    "plt.xlabel(f'Log (10) of {x_title}', fontsize=font_size)\n",
    "plt.ylabel(f'Log (10) of {y_title}', fontsize=font_size)\n",
    "###################\n",
    "# y_min, y_max = plt.ylim()  # Get the y-axis limits\n",
    "# x_min, x_max = plt.xlim()  # Get the x-axis limits\n",
    "\n",
    "# plt.text((x_min + x_max) / 1.75, y_max * 0.25, \n",
    "#          f\"Coefficient (slope): {slope:.2f}\\nStandard Error: {slope_se:.2f}\", \n",
    "#          color='black', fontsize=12, ha='center')\n",
    "y_min, y_max = plt.ylim()  # Get the y-axis limits\n",
    "x_min, x_max = plt.xlim()  # Get the x-axis limits\n",
    "\n",
    "# plt.text(x_min+(x_max-x_min)*.8, y_min+(y_max-y_min)*.1, \n",
    "#          f\"Correlation coefficient: {correlation:.2f}\", \n",
    "#          color='black', fontsize=12, ha='center')\n",
    "\n",
    "plt.text(\n",
    "    x_max,                       # x coordinate at the rightmost point\n",
    "    y_min + 0.07*(y_max - y_min),  # keep your desired vertical offset\n",
    "    f\"Correlation coefficient: {correlation:.2f}\",\n",
    "    color='black',\n",
    "    fontsize=font_size,\n",
    "    ha='right'                   # anchor text to its right edge\n",
    ")\n",
    "##################\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###################### Conduct a Chi-Squared contingency test ############\n",
    "\n",
    "# Define columns dynamically\n",
    "\n",
    "# Create a contingency table using selected columns\n",
    "contingency_table = pd.DataFrame({\n",
    "    'Visits_data': merged_df[x_col],\n",
    "    'Counts_data': merged_df[y_col]\n",
    "})\n",
    "\n",
    "# Perform the chi-squared test\n",
    "from scipy.stats import chi2_contingency\n",
    "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Print the coefficients of the fitted line\n",
    "print(\"Coefficients of fitted line\", coefficients)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nResults of the Chi-Squared contingency test\")\n",
    "print(\"Chi-squared Statistic:\", chi2_stat)\n",
    "print(\"P-value:\", p_value)\n",
    "print(\"Degrees of Freedom:\", dof)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.01  # Significance level\n",
    "if p_value < alpha:\n",
    "    print(f\"At the {alpha*100:.0f}% level, reject the null hypothesis. The data are likely not generated by the same process.\")\n",
    "else:\n",
    "    print(f\"At the {alpha*100:.0f}% level, fail to reject the null hypothesis. The data may come from the same process.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346ed6f-e6e5-4325-88bb-e969338f8831",
   "metadata": {},
   "source": [
    "# TXRC Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c07a7-f589-43e7-93dd-907d99d0df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TX ANALYSIS\n",
    "# Get the unique 'cluster' values from both DataFrames\n",
    "clusters_mobility = set(Mobility_Visits_by_cluster_df['cluster'])\n",
    "clusters_roving = set(TX_Roving_Counts_by_cluster_df['cluster'])\n",
    "\n",
    "# Find the intersection (clusters that appear in both DataFrames)\n",
    "common_clusters = clusters_mobility & clusters_roving\n",
    "\n",
    "# Find the clusters in Mobility_Visits_by_cluster_df but not in TX_Roving_Counts_by_cluster_df\n",
    "mobility_only_clusters = clusters_mobility - clusters_roving\n",
    "\n",
    "# Find the clusters in TX_Roving_Counts_by_cluster_df but not in Mobility_Visits_by_cluster_df\n",
    "roving_only_clusters = clusters_roving - clusters_mobility\n",
    "\n",
    "# Print the counts\n",
    "print(\"with km_boundary = \", km_boundary)\n",
    "print(f\"Number of clusters in both Mobility_Visits_by_cluster_df and TX_Roving_Counts_by_cluster_df: {len(common_clusters)}\")\n",
    "print(f\"Number of clusters in Mobility_Visits_by_cluster_df but not in TX_Roving_Counts_by_cluster_df: {len(mobility_only_clusters)}\")\n",
    "print(f\"Number of clusters in TX_Roving_Counts_by_cluster_df but not in Mobility_Visits_by_cluster_df: {len(roving_only_clusters)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a7894b-f4d6-4000-a299-6e7b0565d916",
   "metadata": {},
   "source": [
    "### TX: Create single df with one row per cluster, including only clusters with non-zero counts and visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f40f5b-7678-4d80-8ca6-6b8b30f92916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the merge\n",
    "Visits_and_Counts_by_cluster_df = pd.merge(\n",
    "    Mobility_Visits_by_cluster_df, \n",
    "    TX_Roving_Counts_by_cluster_df, \n",
    "    on='cluster', \n",
    "    how='outer',  # 'outer' merge to retain all rows from both DataFrames\n",
    "    suffixes=('', '_drop')  # Use a suffix to identify duplicate columns from the second DataFrame\n",
    ")\n",
    "\n",
    "# Replace NaNs in 'lat' and 'lng' columns with values from '_drop' columns (if they exist)\n",
    "Visits_and_Counts_by_cluster_df['lat'] = Visits_and_Counts_by_cluster_df['lat'].combine_first(\n",
    "    Visits_and_Counts_by_cluster_df['lat_drop']\n",
    ")\n",
    "Visits_and_Counts_by_cluster_df['lng'] = Visits_and_Counts_by_cluster_df['lng'].combine_first(\n",
    "    Visits_and_Counts_by_cluster_df['lng_drop']\n",
    ")\n",
    "\n",
    "# Drop duplicate columns (from TX_Roving_Counts_by_cluster_df)\n",
    "Visits_and_Counts_by_cluster_df = Visits_and_Counts_by_cluster_df.loc[:, ~Visits_and_Counts_by_cluster_df.columns.str.endswith('_drop')]\n",
    "\n",
    "# Replace missing values (NaNs) with 0\n",
    "Visits_and_Counts_by_cluster_df.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "Visits_and_Counts_non_zero_by_cluster_df = Visits_and_Counts_by_cluster_df[Visits_and_Counts_by_cluster_df['Total_mobility_visits_by_cluster']>0]\n",
    "Visits_and_Counts_non_zero_by_cluster_df = Visits_and_Counts_non_zero_by_cluster_df[Visits_and_Counts_non_zero_by_cluster_df['ClusterTotal_Avg_Roving_Ct']>0]\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(\"Of a total of \", len(Visits_and_Counts_by_cluster_df), \"clusters, \", len(Visits_and_Counts_non_zero_by_cluster_df), \"have positive visits and counts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fe08d5-c6e8-46fb-b032-92bce91e75f7",
   "metadata": {},
   "source": [
    "#### Show clusters where counts are found but no visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebc28d5-cae8-4b14-ada0-5a013bc116d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Create a base map centered at the average location in the dataset\n",
    "center_lat = Visits_and_Counts_by_cluster_df['lat'].mean()\n",
    "center_lng = Visits_and_Counts_by_cluster_df['lng'].mean()\n",
    "map_clusters = folium.Map(location=[center_lat, center_lng], zoom_start=8)\n",
    "\n",
    "# Add points to the map\n",
    "for _, row in Visits_and_Counts_by_cluster_df.iterrows():\n",
    "    # Determine marker color based on Total_mobility_visits_by_cluster\n",
    "    color = 'blue'\n",
    "    color = 'red' if row['Total_mobility_visits_by_cluster'] == 0 else color\n",
    "    color = 'green' if row['ClusterTotal_Avg_Roving_Ct'] == 0 else color\n",
    "    \n",
    "    # Add a marker\n",
    "    folium.CircleMarker(\n",
    "        location=(row['lat'], row['lng']),\n",
    "        radius=3,  # Adjust point size if needed\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_opacity=0.8,\n",
    "        tooltip = f\"Cluster: {int(row['cluster'])} \\\n",
    "                    \\nTotal Visits: {int(row['Total_mobility_visits_by_cluster'])} \\\n",
    "                    \\nAvg Ct: {int(row['ClusterTotal_Avg_Roving_Ct'])}\"\n",
    "    ).add_to(map_clusters)\n",
    "\n",
    "# Display the map\n",
    "map_clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9214851-1891-4383-bc32-ea6cd9d858d5",
   "metadata": {},
   "source": [
    "## TXRC Validation graph over space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b788bc6b-a441-4e4b-9a30-1563d0d43cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTED X AND Y AXES\n",
    "\n",
    "print(\"## TXRC Validation graph over space\")\n",
    "print(\"\")\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose columns dynamically\n",
    "y_col = 'Total_mobility_visits_by_cluster'  # Change this to use a different Y variable\n",
    "y_title = 'mobility data visits'  # Change this to use a different Y variable\n",
    "x_col = 'ClusterTotal_Avg_Roving_Ct' #'WeightedTrips_per_visit'  # Change this to use a different X variable\n",
    "x_title = 'Texas roving count per site'\n",
    "\n",
    "## copy      x_col =                                             'ClusterTotal_Avg_Roving_Ct' #'WeightedTrips_per_visit'  # Change this to use a different X variable\n",
    "## one above x = np.log(Visits_and_Counts_non_zero_by_cluster_df['ClusterTotal_Avg_Roving_Ct'])\n",
    "\n",
    "# Extract x and y values\n",
    "# x = np.log10(Visits_and_Counts_non_zero_by_cluster_df['Total_mobility_visits_by_cluster'])\n",
    "# y = np.log10(Visits_and_Counts_non_zero_by_cluster_df['ClusterTotal_Avg_Roving_Ct'])\n",
    "\n",
    "x = np.log10(Visits_and_Counts_non_zero_by_cluster_df[x_col])\n",
    "y = np.log10(Visits_and_Counts_non_zero_by_cluster_df[y_col])\n",
    "correlation = np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "# Calculate Pearson and Spearman correlations and p-values\n",
    "r, p_val = pearsonr(x, y)\n",
    "print(f\"Pearson correlation: {r:.3f} (p = {p_val:.3g})\")\n",
    "spearman_corr, spearman_pval = spearmanr(x, y)\n",
    "print(f\"Spearman correlation: {spearman_corr:.4f} (p-value: {spearman_pval:.4g})\")\n",
    "\n",
    "\n",
    "# x = sm.add_constant(x)  # Adds a constant (intercept) term to the independent variable\n",
    "# Fit the linear regression model\n",
    "x_with_const = sm.add_constant(x)\n",
    "model = sm.OLS(y, x_with_const)  # OLS = Ordinary Least Squares\n",
    "results = model.fit()  # Fit the model\n",
    "\n",
    "coefficients = results.params  # [intercept, slope]\n",
    "# print(coefficients)\n",
    "standard_errors = results.bse  # [standard error of intercept, standard error of slope]\n",
    "trendline = results.predict(x_with_const)\n",
    "\n",
    "slope = coefficients[x_col]  # Use the appropriate name\n",
    "slope_se = standard_errors[x_col]  # Use the appropriate name\n",
    "\n",
    "# Increase font sizes for the plot\n",
    "font_size = 18\n",
    "plt.rcParams.update({'font.size': font_size})\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x, y, color='blue', alpha=0.7, label='Data Points')\n",
    "\n",
    "# print(\"Coefficient (slope): {coefficients[1]:.2f}\\nStandard Error: {standard_errors[1]:.2f}\")\n",
    "\n",
    "# Plot the trend line\n",
    "plt.plot(x, trendline, color='red', linestyle='--', label='Trend Line')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel(f'Log (10) of {x_title}', fontsize=font_size)\n",
    "plt.ylabel(f'Log (10) of {y_title}', fontsize=font_size)\n",
    "# plt.xlabel('Log (10) of Texas roving counts', fontsize=font_size)\n",
    "# plt.ylabel('Log (10) of mobility data visits', fontsize=font_size)\n",
    "# plt.title('Predicted trips vs. roving counts per cluster', fontsize=14)\n",
    "\n",
    "# Annotate the plot with the coefficient and standard error\n",
    "# plt.text(0.95, 0.05, f\"Coefficient (slope): {slope:.2f}\\nStandard Error: {slope_se:.2f}\", \n",
    "#          color='black', fontsize=12, ha='right', va='bottom', transform=plt.gca().transAxes)\n",
    "y_min, y_max = plt.ylim()  # Get the y-axis limits\n",
    "x_min, x_max = plt.xlim()  # Get the x-axis limits\n",
    "\n",
    "# plt.text(x_min+(x_max-x_min)*.8, y_min+(y_max-y_min)*.1, \n",
    "#          f\"Correlation coefficient: {correlation:.2f}\", \n",
    "#          color='black', fontsize=12, ha='center')\n",
    "\n",
    "plt.text(\n",
    "    x_max,                       # x coordinate at the rightmost point\n",
    "    y_min + 0.07*(y_max - y_min),  # keep your desired vertical offset\n",
    "    f\"Correlation coefficient: {correlation:.2f}\",\n",
    "    color='black',\n",
    "    fontsize=font_size,\n",
    "    ha='right'                   # anchor text to its right edge\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Show the grid for better readability\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "###################### Conduct a Chi-Squared contingency test ############\n",
    "# Create a contingency table\n",
    "contingency_table = pd.DataFrame({\n",
    "    'Visits_data': Visits_and_Counts_non_zero_by_cluster_df[x_col],\n",
    "    'Counts_data': Visits_and_Counts_non_zero_by_cluster_df[y_col]\n",
    "})\n",
    "\n",
    "# Perform the chi-squared test\n",
    "from scipy.stats import chi2_contingency\n",
    "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Print the coefficients of the fitted line\n",
    "print(\"Coefficients of fitted line\", coefficients)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nResults of the Chi-Squared contingency test\")\n",
    "print(\"Chi-squared Statistic:\", chi2_stat)\n",
    "print(\"P-value:\", p_value)\n",
    "print(\"Degrees of Freedom:\", dof)\n",
    "# Interpret the results\n",
    "alpha = 0.01  # Significance level\n",
    "if p_value < alpha:\n",
    "    print(f\"At the {alpha*100:.0f}% level, reject the null hypothesis. The data are likely not generated by the same process.\")\n",
    "else:\n",
    "    print(f\"At the {alpha*100:.0f}% level, fail to reject the null hypothesis. The data may be generated by the same process.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e42caa7-4a7b-431c-b7a8-921dfb2236d5",
   "metadata": {},
   "source": [
    "### TX: Create single df with one row per year-month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4cbbb-38c6-4ae0-9dd1-8d9e7e8a0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the merge\n",
    "Visits_and_Counts_by_year_month_df = pd.merge(\n",
    "    Mobility_Visits_TX_by_month_df, \n",
    "    TX_Roving_Average_by_month_df, \n",
    "    on='year_month', \n",
    "    how='outer',  # 'outer' merge to retain all rows from both DataFrames\n",
    "    suffixes=('', '_drop')  # Use a suffix to identify duplicate columns from the second DataFrame\n",
    ")\n",
    "\n",
    "# Drop duplicate columns (from TX_Roving_Counts_by_cluster_df)\n",
    "Visits_and_Counts_by_year_month_df = Visits_and_Counts_by_year_month_df.loc[:, ~Visits_and_Counts_by_year_month_df.columns.str.endswith('_drop')]\n",
    "\n",
    "# Replace missing values (NaNs) with 0\n",
    "Visits_and_Counts_by_year_month_df.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# print(\"Of a total of \", len(Visits_and_Counts_by_year_month_df), \"year_month combinations, \")\n",
    "Visits_and_Counts_by_year_month_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5060347-b5a4-4dab-a862-a1422bfe88b7",
   "metadata": {},
   "source": [
    "## TXRC Validation graphs over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06fab04-b8d5-4d1b-a309-8586b2e8b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"## TXRC Validation graphs over time\")\n",
    "print(\"\")\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# BOAT COUNT OPTIONS\n",
    "Ct_variable = 'Avg_BoatCt'\n",
    "Ct_Title = \"Texas roving counts\"\n",
    "\n",
    "# Ct_variable = 'Total_BoatCt'  \n",
    "# Ct_Title = \"sum of all roving counts\"\n",
    "\n",
    "# Ct_variable = 'Weekend_Avg_Ct'  \n",
    "# Ct_Title = \"average weekend roving count at all sites\"\n",
    "\n",
    "# Ct_variable = 'Weekend_Tot_Ct'  \n",
    "# Ct_Title = \"sum of all weekend roving counts\"\n",
    "\n",
    "# Ct_variable = 'Weekend_Avg_Ct_Active'\n",
    "# Ct_Title = \"average weekend roving roving count at active sites\"\n",
    "\n",
    "# Ct_variable = 'Weekend_Tot_Ct_Active' \n",
    "# Ct_Title = \"sum of all weekend roving counts at active sites\"\n",
    "\n",
    "# Ct_variable = 'TX_Weekday_avg_Ct'\n",
    "# Ct_Title = \"average weekday roving count at all sites\"\n",
    "\n",
    "# Ct_variable = 'Weekday_Tot_Ct' \n",
    "# Ct_Title = \"sum of all weekday roving counts at all sites\"\n",
    "\n",
    "# Ct_variable = 'TX_Weekday_avg_Ct_Active'\n",
    "# Ct_Title = \"average weekday roving count at active sites\"\n",
    "\n",
    "# Ct_variable = 'Weekday_Tot_Ct_Active' \n",
    "# Ct_Title = \"sum of all weekday roving counts at active sites\"\n",
    "\n",
    "y_col = 'Total_mobility_visits'  # Change this to use a different Y variable\n",
    "y_title = 'mobility data visits'  # Change this to use a different Y variable\n",
    "x_col = 'Avg_BoatCt' #'WeightedTrips_per_visit'  # Change this to use a different X variable\n",
    "x_title = 'Texas roving counts'\n",
    "\n",
    "# Create data for regression\n",
    "# Log specification\n",
    "Visits_and_Counts_by_year_month_df1=Visits_and_Counts_by_year_month_df[\n",
    "    (Visits_and_Counts_by_year_month_df[y_col]>0) &\n",
    "    (Visits_and_Counts_by_year_month_df[x_col]>0)]\n",
    "\n",
    "x = np.log10(Visits_and_Counts_by_year_month_df1[x_col])\n",
    "y = np.log10( Visits_and_Counts_by_year_month_df1[y_col])\n",
    "correlation = np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "# Calculate Pearson and Spearman correlations and p-values\n",
    "r, p_val = pearsonr(x, y)\n",
    "print(f\"Pearson correlation: {r:.3f} (p = {p_val:.3g})\")\n",
    "spearman_corr, spearman_pval = spearmanr(x, y)\n",
    "print(f\"Spearman correlation: {spearman_corr:.4f} (p-value: {spearman_pval:.4g})\")\n",
    "\n",
    "visits_title='Log (10) of mobility data visits'\n",
    "RovingCt_title = 'Log (10) of ' + Ct_Title\n",
    "\n",
    "# # Non-transformed specification\n",
    "# x = (Visits_and_Counts_by_year_month_df['Total_mobility_visits'])\n",
    "# y = ( Visits_and_Counts_by_year_month_df['Total_BoatCt'])\n",
    "# visits_title='Mobiltiy Data Visits'\n",
    "# RovingCt_title ='Roving Ct'\n",
    "\n",
    "# x = sm.add_constant(x)  # Adds a constant (intercept) term to the independent variable\n",
    "# Fit the linear regression model\n",
    "x_with_const = sm.add_constant(x)\n",
    "model = sm.OLS(y, x_with_const)  # OLS = Ordinary Least Squares\n",
    "results = model.fit()  # Fit the model\n",
    "\n",
    "coefficients = results.params  # [intercept, slope]\n",
    "# print(coefficients)\n",
    "standard_errors = results.bse  # [standard error of intercept, standard error of slope]\n",
    "trendline = results.predict(x_with_const)\n",
    "\n",
    "slope = coefficients[x_col]  # Use the appropriate name\n",
    "slope_se = standard_errors[x_col]  # Use the appropriate name\n",
    "\n",
    "# Increase font sizes for the plot\n",
    "font_size = 18\n",
    "plt.rcParams.update({'font.size': font_size})\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x, y, color='blue', alpha=0.7, label='Data Points')\n",
    "\n",
    "# print(\"Coefficient (slope): {coefficients[1]:.2f}\\nStandard Error: {standard_errors[1]:.2f}\")\n",
    "\n",
    "# Plot the trend line\n",
    "plt.plot(x, trendline, color='red', linestyle='--', label='Trend Line')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel(RovingCt_title, fontsize=font_size)\n",
    "plt.ylabel(visits_title, fontsize=font_size)\n",
    "# plt.title('Predicted trips vs. roving counts per year-month', fontsize=14)\n",
    "# Annotate the plot with the coefficient and standard error\n",
    "# plt.text(0.95, 0.05, f\"Coefficient (slope): {slope:.2f}\\nStandard Error: {slope_se:.2f}\", \n",
    "#          color='black', fontsize=12, ha='right', va='bottom', transform=plt.gca().transAxes)\n",
    "y_min, y_max = plt.ylim()  # Get the y-axis limits\n",
    "x_min, x_max = plt.xlim()  # Get the x-axis limits\n",
    "\n",
    "plt.text(\n",
    "    x_max,                       # x coordinate at the rightmost point\n",
    "    y_min + 0.07*(y_max - y_min),  # keep your desired vertical offset\n",
    "    f\"Correlation coefficient: {correlation:.2f}\",\n",
    "    color='black',\n",
    "    fontsize=font_size,\n",
    "    ha='right'                   # anchor text to its right edge\n",
    ")\n",
    "plt.legend()\n",
    "\n",
    "# Show the grid for better readability\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "###################### Conduct a Chi-Squared contingency test ############\n",
    "# Create a contingency table\n",
    "contingency_table = pd.DataFrame({\n",
    "    'Visits_Data': Visits_and_Counts_by_year_month_df1['Total_mobility_visits'],\n",
    "    'TXRC_Data': Visits_and_Counts_by_year_month_df1[Ct_variable]\n",
    "})\n",
    "\n",
    "# Perform the chi-squared test\n",
    "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Print the coefficients of the fitted line\n",
    "print(\"Coefficients of fitted line\", coefficients)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nResults of the Chi-Squared contingency test\")\n",
    "print(\"Chi-squared Statistic:\", chi2_stat)\n",
    "print(\"P-value:\", p_value)\n",
    "print(\"Degrees of Freedom:\", dof)\n",
    "# Interpret the results\n",
    "alpha = 0.05  # Significance level\n",
    "if p_value < alpha:\n",
    "    print(f\"At the {alpha*100:.2f}% level, reject the null hypothesis. The data are likely not generated by the same process.\")\n",
    "else:\n",
    "    print(f\"At the {alpha*100:.2f}% level, fail to reject the null hypothesis. The data may be generated by the same process.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
