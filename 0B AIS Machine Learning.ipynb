{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e597115a-b12f-4e54-bbba-a9a09d4cb460",
   "metadata": {},
   "source": [
    "# This program loads features created in \"0A AIS data download, clean & process.ipynb\" to create a machine learning classifier for application to the Cuebiq data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588cf39-b1aa-40d9-9d16-74d371d9367a",
   "metadata": {},
   "source": [
    "## Key inputs: \n",
    "* **Indicators2019_All.C.csv.csv** -- a csv file labeled data with features for AIS trips\n",
    "## Key output: \n",
    "* Random Forest classifier: **rf_model_AIS_2019.pkl**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddb8835-af9d-4d50-a018-514375819919",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384884cd-fbf1-4920-aa75-f503284a6d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime\n",
    "import pytz  \n",
    "import warnings\n",
    "import numpy as np\n",
    "import math\n",
    "local_timezone = pytz.timezone('US/Central')\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "import datetime as dt\n",
    "import geopandas as gpd\n",
    "import heapq\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "\n",
    "from scipy import interpolate\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "from zipfile import ZipFile\n",
    "from shapely.geometry import Point  # Import the Point class from shapely.geometry\n",
    "from datetime import datetime\n",
    "\n",
    "# from pandarallel import pandarallel\n",
    "# pandarallel.initialize()\n",
    "\n",
    "local_timezone = pytz.timezone('US/Central')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3750e07-900a-4044-9448-182713561b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f59a545-9ac9-46aa-b604-791cdc152df2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47442277-b3bb-4446-9cfb-e7c6cfcbda14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE GENERATE THE SEASON BASED ON MONTH\n",
    "\n",
    "# def month_to_season(month_num):\n",
    "    \n",
    "#     monthMar= (month_num>1)*(month_num-1) + (month_num<=1)*(11+month_num)\n",
    "#     season=int((monthMar) / 3)+1\n",
    "    \n",
    "#     return season\n",
    "# month 6 and 1 are wrong\n",
    "\n",
    "def month_to_season(month_num):\n",
    "    if (month_num == 11) | (month_num == 0) | (month_num == 1): # DEC-FEB\n",
    "        season = 4\n",
    "    if (month_num == 2) | (month_num == 3) | (month_num == 4): # MARCH - MAY\n",
    "        season = 1\n",
    "    if (month_num == 5) | (month_num == 6) | (month_num == 7): # JUNE - August \n",
    "        season = 2\n",
    "    if (month_num == 8) | (month_num == 9) | (month_num == 10): # SEP- NOV\n",
    "        season = 3\n",
    "    return season\n",
    "\n",
    "# check season function\n",
    "season= month_to_season(1)\n",
    "season\n",
    "\n",
    "# note 0 equals Jan and 11 equals december \n",
    "def monthofyear(EpochTime):  # January = 0\n",
    "    Base = 1546322400  # Jan. 1, 2019, 12:00 a.m. \n",
    "    Jan12021 = 731\n",
    "    Jan12022 = 1096\n",
    "    Jan12023 = 1461\n",
    "\n",
    "    FebStart = 31\n",
    "    MarStart = 59\n",
    "    AprStart = 90\n",
    "    MayStart = 120\n",
    "    JunStart = 151\n",
    "    JulStart = 181\n",
    "    AugStart = 212\n",
    "    SepStart = 243\n",
    "    OctStart = 273\n",
    "    NovStart = 304\n",
    "    DecStart = 334\n",
    "    leapyear2020 = 1582869600     # Feb 29, 2020\n",
    "\n",
    "    days_since_2019 = epoch_to_days_since_1_1_2019(EpochTime)\n",
    "    leapyearadjust = -1*(EpochTime >leapyear2020)\n",
    "    year = int((days_since_2019+ leapyearadjust)/365)\n",
    "    dayofyear = (days_since_2019) - 365*year +  leapyearadjust\n",
    "    month = 1\n",
    "    month = 1*(dayofyear>=FebStart) + \\\n",
    "            1*(dayofyear>=    MarStart ) + \\\n",
    "            1*(dayofyear>=    AprStart ) + \\\n",
    "            1*(dayofyear>=    MayStart ) + \\\n",
    "            1*(dayofyear>=    JunStart ) + \\\n",
    "            1*(dayofyear>=    JulStart ) + \\\n",
    "            1*(dayofyear>=    AugStart ) + \\\n",
    "            1*(dayofyear>=    SepStart ) + \\\n",
    "            1*(dayofyear>=    OctStart ) + \\\n",
    "            1*(dayofyear>=    NovStart ) + \\\n",
    "            1*(dayofyear>=    DecStart )\n",
    "    return month\n",
    "        \n",
    "\n",
    "def epoch_to_season(EpochTime):\n",
    "    month = monthofyear(EpochTime)\n",
    "    season = month_to_season(month)\n",
    "    return season\n",
    "\n",
    "def epoch_to_DOW(EpochTime):\n",
    "    Base = 1546322400  # Jan. 1, 2019, 12:00 a.m. \n",
    "    BaseDOW = 1\n",
    "    DaySince = (EpochTime - Base) / (24*60*60)\n",
    "    WeeksSinceBase = DaySince / 7\n",
    "    DayOfWeek = BaseDOW + int((WeeksSinceBase - int(WeeksSinceBase)) * 7)\n",
    "    return DayOfWeek\n",
    "\n",
    "def epoch_to_days_since_1_1_2019(EpochTime):\n",
    "    Base = 1546322400  # Jan. 1, 2019, 12:00 a.m. \n",
    "    DaySinceBase = int((EpochTime-Base) / (60*60*24))\n",
    "    return DaySinceBase\n",
    "\n",
    "def AISdate_to_epoch(AISDate):\n",
    "    from datetime import datetime\n",
    "\n",
    "    # date_string = \"2019-06-01T16:14:14\"\n",
    "\n",
    "    # Define the format of the input date string\n",
    "    # date_format = \"%Y-%m-%dT%H:%M:%S\"\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "\n",
    "    # Convert the date string to a datetime object\n",
    "    dt_object = datetime.strptime(AISDate, date_format)\n",
    "\n",
    "    # Convert the datetime object to epoch time\n",
    "    epoch_time = int(dt_object.timestamp())\n",
    "\n",
    "    # print(\"Epoch Time:\", epoch_time)\n",
    "    return epoch_time\n",
    "\n",
    "def epoch_to_hour_of_day(EpochTime):\n",
    "    Base = 1546322400  # Jan. 1, 2019, 12:00 a.m. \n",
    "    # Daylight Savings Time points for US Central Time\n",
    "    start2019 = 1552204800  \n",
    "    end2019 = 1572768000\n",
    "    start2020 = 1583654400\n",
    "    end2020 = 1604217600\n",
    "    start2021 = 1615708800\n",
    "    end2021 = 1636272000\n",
    "    start2022 = 1647158400\n",
    "    end2022 = 1667721600\n",
    "    start2023 = 1678608000\n",
    "    end2023 = 1699171200\n",
    "\n",
    "    \n",
    "    DayLightSavingsAdjust = +1 * (EpochTime > start2019) + \\\n",
    "                            -1 * (EpochTime > end2019) + \\\n",
    "                            +1 * (EpochTime > start2020) + \\\n",
    "                            -1 * (EpochTime > end2020) + \\\n",
    "                            +1 * (EpochTime > start2021) + \\\n",
    "                            -1 * (EpochTime > end2021) + \\\n",
    "                            +1 * (EpochTime > start2022) + \\\n",
    "                            -1 * (EpochTime > end2022) + \\\n",
    "                            +1 * (EpochTime > start2023) + \\\n",
    "                            -1 * (EpochTime > end2023)\n",
    "#    print(DayLightSavingsAdjust)\n",
    "    DaysSince = ((EpochTime-Base) / (60*60*24))\n",
    "    PortionOfDay = DaysSince - int(DaysSince)\n",
    "    HourOfDay = int(PortionOfDay*24) + DayLightSavingsAdjust\n",
    "    return HourOfDay\n",
    "\n",
    "def epoch_to_date(epoch_time):\n",
    "    # Convert epoch time to a datetime object\n",
    "    dt = datetime.fromtimestamp(epoch_time)\n",
    "    \n",
    "    # Format the datetime as 'YYYY-MM-DD'\n",
    "    formatted_date = dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    return formatted_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e1e50e-570b-40ce-b71d-1c73bf1134ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set directories to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf1e387-a4f3-496a-9d00-14a49b04ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Directory for this output\n",
    "OurTable_V3_directory = '~/RecFishing/Analysis with Our Tables and V3/Data Files'\n",
    "# Expand the tilde to the user's home directory\n",
    "OurTable_V3_directory = os.path.expanduser(OurTable_V3_directory)\n",
    "# Check to make sure the directory exist\n",
    "DirExist = os.path.exists(OurTable_V3_directory)\n",
    "print(OurTable_V3_directory, \"exists = \" ,DirExist)\n",
    "\n",
    "#################################################################\n",
    "# Directory some core data fro analysis\n",
    "CoreData_Directory = '~/RecFishing/CoreData'\n",
    "CoreData_Directory = os.path.expanduser(CoreData_Directory)\n",
    "print(CoreData_Directory, \"exists = \", os.path.exists(CoreData_Directory))\n",
    "\n",
    "#################################################################\n",
    "# Directory  with Original_directory material\n",
    "Original_directory = '~/RecFishing/DataflowStudioJobs'\n",
    "Original_directory = os.path.expanduser(Original_directory)\n",
    "DirExist = os.path.exists(Original_directory)\n",
    "print(Original_directory, \"exists = \", DirExist)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory  with Original_directory material\n",
    "Previously_Processed_directory = '~/RecFishing/DataflowStudioJobs/FinalCode - Rec Fishing Identification'\n",
    "Previously_Processed_directory = os.path.expanduser(Previously_Processed_directory)\n",
    "DirExist = os.path.exists(Previously_Processed_directory)\n",
    "print(Previously_Processed_directory, \"exists = \", DirExist)\n",
    "\n",
    "#################################################################\n",
    "# Directory with Travel Cost files\n",
    "Travel_Cost_directory = '~/RecFishing/Travel Costs with Dedicated Table/CSV Files'\n",
    "# Expand the tilde to the user's home directory\n",
    "Travel_Cost_directory = os.path.expanduser(Travel_Cost_directory)\n",
    "DirExist = os.path.exists(Travel_Cost_directory)\n",
    "print(Travel_Cost_directory, \"exists = \", DirExist)\n",
    "\n",
    "#################################################################\n",
    "# Directory with Weather data and related files\n",
    "Weather_Data_directory = '~/RecFishing/uploaded_files/Weather Data'\n",
    "# Expand the tilde to the user's home directory\n",
    "Weather_Data_directory = os.path.expanduser(Weather_Data_directory)\n",
    "print(Weather_Data_directory, \"exists = \", DirExist)\n",
    "\n",
    "####################################################################################\n",
    "####################  AIS Directory #################################################\n",
    "AIS_Directory = '~/RecFishing/AIS Files/Data'\n",
    "AIS_Directory = os.path.expanduser(AIS_Directory)\n",
    "DirExist = os.path.exists(AIS_Directory)\n",
    "print(AIS_Directory, \"exists = \", DirExist)\n",
    "\n",
    "# ID_list_RandomSample from ScheduledExecution5.pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e51283-b07c-402f-a40e-f4d10f33ce6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set input and output files to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f456d45-c947-47b1-b27c-a809d226ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_existence(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Does     Exist {file_path}\")\n",
    "    else:\n",
    "        print(f\"Does NOT Exist {file_path}\")\n",
    "        \n",
    "# Complete list of randomized IDs\n",
    "# PKL_File_With_Random_IDs_Filename  =  os.path.join(Original_directory, 'cuebiq_id_list.pkl')\n",
    "# check_file_existence(PKL_File_With_Random_IDs_Filename)\n",
    "\n",
    "# Complete list of randomized IDs- without bernouli sampling 740k\n",
    "# PKL_File_With_Random_IDs_Filename  =  os.path.join(Original_directory, 'cuebiq_id_list_wo_sampling_740k.pkl')\n",
    "PKL_File_With_Random_IDs_Filename  =  os.path.join(CoreData_Directory, 'cuebiq_id_list_wo_sampling_740k.pkl')\n",
    "check_file_existence(PKL_File_With_Random_IDs_Filename)\n",
    "    \n",
    "# Data gathered and used prior to the NOAA Webinar in February 2024\n",
    "IDs_Used_in_NOAA_Webinar_filename = os.path.join(CoreData_Directory, 'IDs_From_Random_Draw_Prior_to_NOAA_Webinar.csv')\n",
    "check_file_existence(IDs_Used_in_NOAA_Webinar_filename)\n",
    "Ping_Used_in_NOAA_Webinar_filename = os.path.join(CoreData_Directory, 'Pings_From_Random_Draw_Prior_to_NOAA_Webinar.csv')\n",
    "check_file_existence(Ping_Used_in_NOAA_Webinar_filename)\n",
    "                                                     \n",
    "# List of IDs that have been processed for Indicators\n",
    "AlreadyFullyProcessedIDs_Filename  =  os.path.join(OurTable_V3_directory, 'RandomlyChosenCuebiq_ids.List_of_Processed_ids.csv')\n",
    "check_file_existence(AlreadyFullyProcessedIDs_Filename)\n",
    "    \n",
    "######################################################################################################################\n",
    "#########################  ID Checklist with columns for ID, Pings, Indicators Created (TF) & Trips  ################\n",
    "IDs_Pulled_from_Dedicated_Table_filename  =  os.path.join(OurTable_V3_directory, 'IDs_Pulled_From_Dedicated_Table.csv')\n",
    "check_file_existence(IDs_Pulled_from_Dedicated_Table_filename)\n",
    "    \n",
    "# Pings in the OurTable for a single large draw of IDs TEMPORARY FILE\n",
    "Pings_OurTable_temp_filename = os.path.join(OurTable_V3_directory,'Pings_OurTable_temp.csv')\n",
    "check_file_existence(Pings_OurTable_temp_filename)\n",
    "    \n",
    "# Pings from V3 corresponding with the IDs found in the OurTable for a single large draw of IDs TEMPORARY FILE\n",
    "Pings_V3_temp_filename = os.path.join(OurTable_V3_directory,'Pings_V3_temp.csv')\n",
    "check_file_existence(Pings_V3_temp_filename)\n",
    "    \n",
    "# Set output file names\n",
    "Indicators_IDs_checked_filename = os.path.join(OurTable_V3_directory, 'IDs_Checked_Indicators_OurTable.csv')\n",
    "check_file_existence(Indicators_IDs_checked_filename)\n",
    "\n",
    "cuebiq_id_list_and_count_filename= os.path.join(OurTable_V3_directory,'cuebiq_id_list_and_count.csv')\n",
    "check_file_existence(cuebiq_id_list_and_count_filename)\n",
    "\n",
    "\n",
    "# List of IDs and dates for V3 query Pings in the OurTable for a single large draw of IDs TEMPORARY FILE\n",
    "OurTable_IDs_and_Dates_filename = os.path.join(OurTable_V3_directory,'OurTable_IDs_and_Dates.csv')\n",
    "check_file_existence(OurTable_IDs_and_Dates_filename)\n",
    "    \n",
    "# Filenames for consolidated data frames\n",
    "Pings_OurTable_Gulf_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Gulf_ALL.csv')\n",
    "check_file_existence(Pings_OurTable_Gulf_filename)\n",
    "\n",
    "Pings_OurTable_Gulf_MT19_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Gulf_MT19.csv')\n",
    "check_file_existence(Pings_OurTable_Gulf_MT19_filename)\n",
    "\n",
    "Pings_OurTable_Coast_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Coast.csv')\n",
    "check_file_existence(Pings_OurTable_Coast_filename)\n",
    "\n",
    "Pings_OurTable_Outside_our_wkts_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Outside_our_wkts.csv')\n",
    "check_file_existence(Pings_OurTable_Outside_our_wkts_filename)\n",
    "    \n",
    "##########################################################################################\n",
    "############################### INDICATORS  ##############################################\n",
    "Indicators_filename = os.path.join(OurTable_V3_directory,'Indicators_OurTable.csv')\n",
    "check_file_existence(Indicators_filename)\n",
    "     \n",
    "AIS_indicators_file_path = os.path.join(AIS_Directory,'Indicators2019_All.C.csv')\n",
    "check_file_existence(AIS_indicators_file_path)\n",
    "  \n",
    "\n",
    "##########################################################################################\n",
    "############################### Random Forest Stuff    ###############################\n",
    "classifier_file_path = os.path.join(AIS_Directory, f'rf_model_AIS_2019.pkl')\n",
    "\n",
    "RF_Results_test = os.path.join(AIS_Directory, f'Test_observations.csv')\n",
    "RF_Results_Train = os.path.join(AIS_Directory, f'Train_observations.csv')\n",
    "RF_Results_filename = os.path.join(AIS_Directory, f'RandomForest_Predictions2019AISData.csv')\n",
    "  \n",
    "##########################################################################################\n",
    "############################### Weather data files ####################################\n",
    "Buoys_file_path  = os.path.join(Weather_Data_directory,'Buoys.csv')\n",
    "check_file_existence(Buoys_file_path)\n",
    "\n",
    "Weather_file_path  = os.path.join(Weather_Data_directory,'DailyWeatherData.csv')\n",
    "check_file_existence(Weather_file_path)\n",
    "    \n",
    "### Dedicate Table Names for reference\n",
    "# Dedicated table with all Pings within the Gulf WKT for 1/12019 - 4/22/2022\n",
    "#  Table Name:  dedicated.ScheduledExecution5.DeviceTable   \n",
    "#  Code used for call:  RecFishing/DataflowStudioJobs/ScheduledEx5-updated.ipynb\n",
    "\n",
    "# Dedicated table with all Pings within the Gulf WKT AND Origin for 1/12019 - 4/22/2022\n",
    "#  Table Name:  dedicated.ScheduledExecution5_parallel_origin.DeviceTable\n",
    "#  Code used for call:  RecFishing/DataflowStudioJobs/ScheduledEx5-origin.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a16b6-50ac-4b3e-9597-e8d28a38ba16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Function to correct the trip duration and related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a997e8e-b2ea-4b9c-bc99-eb9ec9a9afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def df_duration_correction(df):\n",
    "    # Calculate corrected trip duration\n",
    "    df['Trip_Duration_t_Corrected'] = (df['timestamp_end_t'] - df['timestamp_start_t']) / 60\n",
    "    \n",
    "    # Calculate duration correction factor and apply to other columns\n",
    "    df['Duration_Correction'] = df['Trip_Duration_t'] / df['Trip_Duration_t_Corrected']\n",
    "    df['Pings_per_minute_t'] = df['Pings_per_minute_t'] * df['Duration_Correction']\n",
    "    df['pct_time_stopped'] = df['pct_time_stopped'] * df['Duration_Correction']\n",
    "    df['pct_time_trawling'] = df['pct_time_trawling'] * df['Duration_Correction']\n",
    "    df['pct_time_moving'] = df['pct_time_moving'] * df['Duration_Correction']\n",
    "    \n",
    "    # If Trip_Duration_t_Corrected is zero, set all related columns to zero\n",
    "    zero_duration_mask = df['Trip_Duration_t_Corrected'] == 0\n",
    "    df.loc[zero_duration_mask, ['Pings_per_minute_t', 'pct_time_stopped', 'pct_time_trawling', 'pct_time_moving']] = 0\n",
    "    \n",
    "    # Update Trip_Duration_t to corrected value\n",
    "    df['Trip_Duration_t'] = df['Trip_Duration_t_Corrected']\n",
    "    \n",
    "    # Drop intermediate columns\n",
    "    df.drop(columns=['Trip_Duration_t_Corrected', 'Duration_Correction'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def duration_correction(timestamp_end_t, timestamp_start_t, Trip_Duration_t, \n",
    "                        Pings_per_minute_t, pct_time_stopped, pct_time_trawling, \n",
    "                        pct_time_moving):\n",
    "\n",
    "    Trip_Duration_t_Corrected = (timestamp_end_t - timestamp_start_t) / 60\n",
    "    \n",
    "    print(\"Trip_Duration_t, Trip_Duration_t_Corrected\", Trip_Duration_t, Trip_Duration_t_Corrected)\n",
    "    if Trip_Duration_t_Corrected > 0:\n",
    "        Duration_Correction = Trip_Duration_t / (Trip_Duration_t_Corrected)\n",
    "\n",
    "        Pings_per_minute_t = Pings_per_minute_t * Duration_Correction\n",
    "        pct_time_stopped = pct_time_stopped * Duration_Correction\n",
    "        pct_time_trawling = pct_time_trawling * Duration_Correction\n",
    "        pct_time_moving = pct_time_moving * Duration_Correction\n",
    "    else:\n",
    "        Duration_Correction = 0\n",
    "\n",
    "        Pings_per_minute_t = 0\n",
    "        pct_time_stopped = 0\n",
    "        pct_time_trawling = 0\n",
    "        pct_time_moving = 0\n",
    "\n",
    "    Trip_Duration_t =Trip_Duration_t_Corrected\n",
    "    return Trip_Duration_t, Pings_per_minute_t, pct_time_stopped, pct_time_trawling, pct_time_moving\n",
    "\n",
    "# Assuming df is your DataFrame containing the columns mentioned\n",
    "# df['Trip_Duration_t'], df['Pings_per_minute_t'], df['pct_time_stopped'], df['pct_time_trawling'], df['pct_time_moving'] = \\\n",
    "#     duration_correction(df['timestamp_end_t'], df['timestamp_start_t'], df['Trip_Duration_t'], \n",
    "#                         df['Pings_per_minute_t'], df['pct_time_stopped'], df['pct_time_trawling'], \n",
    "#                         df['pct_time_moving'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9022956-83f5-4f79-9103-1b67dae7ed22",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa31020-36e2-44ef-b64f-5441241dd6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import joblib\n",
    "\n",
    "\n",
    "# # Read df -- not needed if run sequentially\n",
    "AISIndicators_df= pd.read_csv(AIS_indicators_file_path)\n",
    "###### Correct the trip durations\n",
    "\n",
    "AISIndicators_df['timestamp_start_t'] = AISIndicators_df['timestamp_start_t'].apply(AISdate_to_epoch)\n",
    "AISIndicators_df['timestamp_end_t'] = AISIndicators_df['timestamp_end_t'].apply(AISdate_to_epoch)\n",
    "AISIndicators_df = df_duration_correction(AISIndicators_df)\n",
    "\n",
    "\n",
    "# Identify columns to be used in RF\n",
    "X = AISIndicators_df[[\n",
    "                        'trips_per_day', \n",
    "                        'break_duration_t',\n",
    "                        'Weekday_trips', \n",
    "                        'Trip_Duration_t',\n",
    "                        'Total_distance_traveled_t', \n",
    "                        'Distance_from_origin_t', \n",
    "                        'first_distance_from_coast_t',\n",
    "                        'last_distance_from_coast_t', \n",
    "                        'Max_distance_traveled_origin_t',\n",
    "                        'Weekend_trip_t', \n",
    "                        'time_stopped_t', \n",
    "                        'pct_time_stopped',\n",
    "                        'number_of_stops_t', \n",
    "                        'longest_stop_t',\n",
    "                        'shortest_stop_t',\n",
    "                        'time_trawling_t',\n",
    "                        'pct_time_trawling',\n",
    "                        'number_of_trawl_t',\n",
    "                        'longest_trawl_t', \n",
    "                        'shortest_trawl_t', \n",
    "                        'distance_trawling_t',\n",
    "                        'time_moving_t', \n",
    "                        'pct_time_moving', \n",
    "                        'number_of_moves_t',\n",
    "                        'longest_move_t', \n",
    "                        'shortest_move_t',\n",
    "                        'distance_moving_t',\n",
    "                        'dist_from_origin_during_stops', \n",
    "                        'Trip_pings_mov_traw_per_time_t',\n",
    "                        'move_efficiency_t', \n",
    "                        'move_speed_t',\n",
    "                        'maxspeed_t', \n",
    "                        'max_distance_from_coast_t',\n",
    "                        'WSPD', \n",
    "                        'GST',\n",
    "                        'WVHT', \n",
    "                        'ATMP'  \n",
    "                        ]]\n",
    "\n",
    "y = AISIndicators_df['vt']\n",
    "\n",
    "identifiers = AISIndicators_df[['mmsi', 'Trip_number', 'vt']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#  NOTE THAT TEST_SIZE  CAN BE MODIFIED -- 20% IS, ACCORDING TO CHAT GPT, THE DEFAULT TEST_SIZE\n",
    "X_train, X_test, y_train, y_test, identifiers_train, identifiers_test = train_test_split(X, y, identifiers, test_size=0.2, random_state=49)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "# rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier, Weight using the balance_subsample option\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced_subsample\")\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "###############  -- RF analysis is done --###############  \n",
    "\n",
    "# Predict vessel types on the testing data\n",
    "y_pred_test = rf_classifier.predict(X_test)\n",
    "\n",
    "# Basic evaluation of model's accuracy on the test data\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict vessel types on the testing data\n",
    "y_pred_train = rf_classifier.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train )\n",
    "print(f\"Accuracy in training: {accuracy_train:.4f}\")\n",
    "\n",
    "# Assuming you have trained rf_classifier and want to save it\n",
    "\n",
    "# Save the trained Random Forest classifier to a file\n",
    "import datetime\n",
    "\n",
    "current_datetime = datetime.datetime.now()\n",
    "date_string = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# classifier_file_path = os.path.join(Indicatorsdir, f'rf_model_{date_string}.pkl')\n",
    "joblib.dump(rf_classifier, classifier_file_path)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b97a2a-258c-443a-862d-42e54c3fda03",
   "metadata": {},
   "source": [
    "# Create a single array with all the data, the predicted class and the probabilities of each class. This uses df's from the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb8ac36-c2de-4109-94da-5f21d0ec6e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# CREATE A DATA FRAME FOR THE TEST OBSERVATIONS FROM ABOVE\n",
    "################################################################\n",
    "# Create a combined data frame for all the test data\n",
    "test_df = identifiers_test\n",
    "# ### print\n",
    "# print(1,test_df.shape)\n",
    "test_df['group'] = \"test\"\n",
    "test_df['vt_pred'] = y_pred_test\n",
    "# ### print\n",
    "# print(2,test_df.shape)\n",
    "\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "predicted_proba_df = []\n",
    "# predicted_proba_df.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "predicted_probabilities = rf_classifier.predict_proba(X_test)\n",
    "predicted_proba_df = pd.DataFrame(predicted_probabilities, columns=rf_classifier.classes_)\n",
    "test_df = pd.concat([test_df, predicted_proba_df], axis=1)\n",
    "test_df = pd.concat([test_df, X_test], axis=1)\n",
    "# ### print\n",
    "# print(3,test_df.shape)\n",
    "# Export test_df to a CSV file\n",
    "test_df.to_csv(RF_Results_test, index=False)\n",
    "\n",
    "################################################################\n",
    "# CREATE A DATA FRAME FOR THE TRAINING OBSERVATIONS FROM ABOVE\n",
    "################################################################\n",
    "# Create a combined data frame for all the train data\n",
    "train_df = identifiers_train\n",
    "# ### print\n",
    "# print(1,train_df.shape)\n",
    "train_df['group'] = \"train\"\n",
    "train_df['vt_pred'] = y_pred_train\n",
    "# ### print\n",
    "# print(2,train_df.shape)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "predicted_proba_df.reset_index(drop=True, inplace=True)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "predicted_probabilities = rf_classifier.predict_proba(X_train)\n",
    "predicted_proba_df = pd.DataFrame(predicted_probabilities, columns=rf_classifier.classes_)\n",
    "train_df = pd.concat([train_df, predicted_proba_df], axis=1)\n",
    "train_df = pd.concat([train_df, X_train], axis=1)\n",
    "# ### print\n",
    "# print(3,train_df.shape)\n",
    "# Export train_df to a CSV file\n",
    "train_df.to_csv(RF_Results_Train, index=False)\n",
    "\n",
    "################################################################\n",
    "# PULL TEST AND TRAIN RESULTS TOGETHER INTO A SINGLE DATA FRAME AND EXPORT\n",
    "################################################################\n",
    "current_datetime = datetime.datetime.now()\n",
    "date_string = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "full_df = pd.concat([train_df, test_df], axis=0)\n",
    "full_df.to_csv(RF_Results_filename, index=False)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
