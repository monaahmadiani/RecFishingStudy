{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84895423-eba5-464c-a922-d86af7554020",
   "metadata": {},
   "source": [
    "# This notebook carries out the station-non-station analysis, identifying recreational fishing trips that appear to have launched from a location where creel data are gathered. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a71ac-d0e6-471d-8ee5-9e32ffd8d102",
   "metadata": {},
   "source": [
    "## Key inputs: \n",
    "* **rec_indicators_with_V3** - features used for classification\n",
    "* **DisappearanceIndicators.csv** - indicators of whether a trip is fully tracked\n",
    "\n",
    "## Key outputs:\n",
    "* **Station_NonStationAnalysis_full.csv** -- includes an indicator of whether the trip stopped at a station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e589575d-3447-47c3-882b-07231184d09c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e3058-d05a-4dda-a61a-f3163dbf7986",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7fd74-d818-4530-b514-722cf774c2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Install modules\n",
    "!pip install tqdm\n",
    "!pip install statsmodels\n",
    "\n",
    "# # Suppress all warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime\n",
    "import pytz  \n",
    "import warnings\n",
    "import numpy as np\n",
    "import math\n",
    "local_timezone = pytz.timezone('US/Central')\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "import datetime as dt\n",
    "import geopandas as gpd\n",
    "import heapq\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "import statsmodels\n",
    "import inspect\n",
    "import folium\n",
    "\n",
    "from scipy import interpolate\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "from zipfile import ZipFile\n",
    "from shapely.geometry import Point  # Import the Point class from shapely.geometry\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "class OperationCancelled(Exception):\n",
    "    pass\n",
    "\n",
    "local_timezone = pytz.timezone('US/Central')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6324afe-3f34-4f73-b562-a26f0d631361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856bd9a2-337b-4ab2-a161-f282e5f74295",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set directories to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6ac76-7613-43f1-9b36-0f37f2ba9ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Directory for this output\n",
    "OurTable_V3_directory = '~/RecFishing/Analysis with Our Tables and V3/Data Files'\n",
    "# Expand the tilde to the user's home directory\n",
    "OurTable_V3_directory = os.path.expanduser(OurTable_V3_directory)\n",
    "# Check to make sure the directory exist\n",
    "DirExist = os.path.exists(OurTable_V3_directory)\n",
    "print(OurTable_V3_directory, \"exists = \" ,DirExist)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory for output from the first draw\n",
    "Batch01_directory = '~/RecFishing/Analysis with Our Tables and V3/Data Files/Batch01'\n",
    "# Expand the tilde to the user's home directory\n",
    "Batch01_directory = os.path.expanduser(Batch01_directory)\n",
    "# Check to make sure the directory exist\n",
    "DirExist = os.path.exists(Batch01_directory)\n",
    "print(Batch01_directory, \"exists = \" ,DirExist)\n",
    "\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory for Groups of V3 Pings\n",
    "V3_Pings_Groups_directory = '~/RecFishing/Analysis with Our Tables and V3/Data Files/V3_Ping_Groups'\n",
    "V3_Pings_Groups_directory = os.path.expanduser(V3_Pings_Groups_directory)\n",
    "print(V3_Pings_Groups_directory, \"exists = \" ,os.path.exists(V3_Pings_Groups_directory))\n",
    "\n",
    "#################################################################\n",
    "# Directory some core data fro analysis\n",
    "CoreData_Directory = '~/RecFishing/CoreData'\n",
    "CoreData_Directory = os.path.expanduser(CoreData_Directory)\n",
    "print(CoreData_Directory, \"exists = \", os.path.exists(CoreData_Directory))\n",
    "\n",
    "#################################################################\n",
    "# Directory  with Original_directory material\n",
    "Original_directory = '~/RecFishing/DataflowStudioJobs'\n",
    "Original_directory = os.path.expanduser(Original_directory)\n",
    "DirExist = os.path.exists(Original_directory)\n",
    "print(Original_directory, \"exists = \", DirExist)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory  with Original_directory material\n",
    "Previously_Processed_directory = '~/RecFishing/DataflowStudioJobs/FinalCode - Rec Fishing Identification'\n",
    "Previously_Processed_directory = os.path.expanduser(Previously_Processed_directory)\n",
    "DirExist = os.path.exists(Previously_Processed_directory)\n",
    "print(Previously_Processed_directory, \"exists = \", DirExist)\n",
    "\n",
    "#################################################################\n",
    "# Directory with Travel Cost files\n",
    "Travel_Cost_directory = '~/RecFishing/Travel Costs with Dedicated Table/CSV Files'\n",
    "# Expand the tilde to the user's home directory\n",
    "Travel_Cost_directory = os.path.expanduser(Travel_Cost_directory)\n",
    "DirExist = os.path.exists(Travel_Cost_directory)\n",
    "print(Travel_Cost_directory, \"exists = \", DirExist)\n",
    "\n",
    "#################################################################\n",
    "# Directory with Weather data and related files\n",
    "Weather_Data_directory = '~/RecFishing/uploaded_files/Weather Data'\n",
    "# Expand the tilde to the user's home directory\n",
    "Weather_Data_directory = os.path.expanduser(Weather_Data_directory)\n",
    "print(Weather_Data_directory, \"exists = \", DirExist)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory with other Uploaded data \n",
    "Uploaded_Data_directory = '~/RecFishing/uploaded_files'\n",
    "# Expand the tilde to the user's home directory\n",
    "Uploaded_Data_directory = os.path.expanduser(Uploaded_Data_directory)\n",
    "\n",
    "#################################################################\n",
    "# Results and Analysis\n",
    "Results_directory = '~/RecFishing/Analysis with Our Tables and V3/Results'\n",
    "Results_directory = os.path.expanduser(Results_directory)\n",
    "\n",
    "Figures_directory = '~/RecFishing/Analysis with Our Tables and V3/Results/Figures'\n",
    "Figures_directory = os.path.expanduser(Figures_directory)\n",
    "\n",
    "TrajectoryMaps_directory = '~/RecFishing/Analysis with Our Tables and V3/Results/Figures/Maps'\n",
    "TrajectoryMaps_directory = os.path.expanduser(TrajectoryMaps_directory)\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "####################  AIS Directory #################################################\n",
    "AIS_Directory = '~/RecFishing/AIS Files/Data'\n",
    "AIS_Directory = os.path.expanduser(AIS_Directory)\n",
    "DirExist = os.path.exists(AIS_Directory)\n",
    "print(AIS_Directory, \"exists = \", DirExist)\n",
    "\n",
    "# ID_list_RandomSample from ScheduledExecution5.pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e583a397-c173-40b8-824c-c2cb81ec0671",
   "metadata": {},
   "source": [
    "## Set input and output files to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf9ed1-3c69-49c7-becc-59326ad11529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_existence(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"{file_path} Does NOT Exist\")\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "#########################  Log File  ################\n",
    "Log_filename  =  os.path.join(OurTable_V3_directory, 'Log.txt')\n",
    "\n",
    "######################################################################################################################\n",
    "########################## Complete list of randomized IDs- without bernouli sampling 740k #########################\n",
    "# PKL_File_With_Random_IDs_Filename  =  os.path.join(Original_directory, 'cuebiq_id_list_wo_sampling_740k.pkl')\n",
    "PKL_File_With_Random_IDs_Filename  =  os.path.join(CoreData_Directory, 'cuebiq_id_list_wo_sampling_740k.pkl')\n",
    "check_file_existence(PKL_File_With_Random_IDs_Filename)\n",
    "    \n",
    "# Data gathered and used prior to the NOAA Webinar in February 2024\n",
    "IDs_Used_in_NOAA_Webinar_filename = os.path.join(CoreData_Directory, 'IDs_From_Random_Draw_Prior_to_NOAA_Webinar.csv')\n",
    "check_file_existence(IDs_Used_in_NOAA_Webinar_filename)\n",
    "Ping_Used_in_NOAA_Webinar_filename = os.path.join(CoreData_Directory, 'Pings_From_Random_Draw_Prior_to_NOAA_Webinar.csv')\n",
    "check_file_existence(Ping_Used_in_NOAA_Webinar_filename)\n",
    "                                                     \n",
    "# List of IDs that have been processed for Indicators\n",
    "AlreadyFullyProcessedIDs_Filename  =  os.path.join(OurTable_V3_directory, 'RandomlyChosenCuebiq_ids.List_of_Processed_ids.csv')\n",
    "check_file_existence(AlreadyFullyProcessedIDs_Filename)\n",
    "    \n",
    "######################################################################################################################\n",
    "#########################  ID Checklist with columns for ID, Pings, Indicators Created (TF) & Trips  ################\n",
    "IDs_Pulled_from_Dedicated_Table_filename  =  os.path.join(OurTable_V3_directory, 'IDs_Pulled_From_Dedicated_Table.csv')\n",
    "check_file_existence(IDs_Pulled_from_Dedicated_Table_filename)\n",
    "    \n",
    "ID_For_V3_Queries_filename  =  os.path.join(OurTable_V3_directory, 'IDs_from_V3.csv')\n",
    "check_file_existence(ID_For_V3_Queries_filename)\n",
    "    \n",
    "RecTripRating_filename =  os.path.join(OurTable_V3_directory, 'RecTripRating.csv')\n",
    "check_file_existence(RecTripRating_filename)\n",
    "    \n",
    "# This file contains information about the rows of Pings_V3_temp_filename that can be used to avoid loading the entire file into a data frame\n",
    "V3_Pings_Index_filename =  os.path.join(OurTable_V3_directory, 'V3_Pings_File_Index.csv')\n",
    "check_file_existence(V3_Pings_Index_filename)\n",
    "    \n",
    "ID_Groups_filename = os.path.join(OurTable_V3_directory,'Cuebiq_ID_Groups.csv')\n",
    "check_file_existence(ID_Groups_filename)\n",
    "\n",
    "######################################################\n",
    "# Pings in the OurTable for a single large draw of IDs TEMPORARY FILE\n",
    "Pings_OurTable_temp_filename = os.path.join(OurTable_V3_directory,'Pings_OurTable_temp.csv')\n",
    "check_file_existence(Pings_OurTable_temp_filename)\n",
    "    \n",
    "# Pings from V3 corresponding with the IDs found in the OurTable \n",
    "Pings_V3_temp_filename = os.path.join(OurTable_V3_directory,'Pings_V3_temp.csv')\n",
    "check_file_existence(Pings_V3_temp_filename)\n",
    "    \n",
    "# Set output file names\n",
    "Indicators_IDs_checked_filename = os.path.join(OurTable_V3_directory, 'IDs_Checked_Indicators_OurTable.csv')\n",
    "check_file_existence(Indicators_IDs_checked_filename)\n",
    "\n",
    "cuebiq_id_list_and_count_filename= os.path.join(OurTable_V3_directory,'cuebiq_id_list_and_count.csv')\n",
    "check_file_existence(cuebiq_id_list_and_count_filename)\n",
    "\n",
    "# List of IDs and dates for V3 query Pings in the OurTable for a single large draw of IDs TEMPORARY FILE\n",
    "OurTable_IDs_and_Dates_filename = os.path.join(OurTable_V3_directory,'OurTable_IDs_and_Dates.csv')\n",
    "check_file_existence(OurTable_IDs_and_Dates_filename)\n",
    "    \n",
    "##########################################################################################\n",
    "############################### PINGS FILES   ##############################################\n",
    "Pings_OurTable_Gulf_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Gulf_ALL.csv')\n",
    "check_file_existence(Pings_OurTable_Gulf_filename)\n",
    "\n",
    "Combined_Pings_OurTable_Gulf_filename= os.path.join(OurTable_V3_directory,'Combined_Pings_OurTable_Gulf_ALL.csv')\n",
    "check_file_existence(Combined_Pings_OurTable_Gulf_filename)\n",
    "\n",
    "Pings_V3_Before_After_filename= os.path.join(OurTable_V3_directory,'Pings_V3_Before_After.csv')\n",
    "check_file_existence(Pings_V3_Before_After_filename)\n",
    "\n",
    "Combined_Pings_V3_Before_After_filename= os.path.join(OurTable_V3_directory,'Combined_Pings_V3_Before_After.csv')\n",
    "check_file_existence(Combined_Pings_V3_Before_After_filename)\n",
    "\n",
    "Pings_OurTable_Gulf_MT19_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Gulf_MT19.csv')\n",
    "check_file_existence(Pings_OurTable_Gulf_MT19_filename)\n",
    "\n",
    "Pings_OurTable_Coast_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Coast.csv')\n",
    "check_file_existence(Pings_OurTable_Coast_filename)\n",
    "\n",
    "Pings_OurTable_Outside_our_wkts_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Outside_our_wkts.csv')\n",
    "check_file_existence(Pings_OurTable_Outside_our_wkts_filename)\n",
    "    \n",
    "##########################################################################################\n",
    "############################### INDICATORS  ##############################################\n",
    "Indicators_filename = os.path.join(OurTable_V3_directory,'Indicators_OurTable.csv')\n",
    "check_file_existence(Indicators_filename)\n",
    "    \n",
    "# cuebiq_id_count_filename= os.path.join(EEZ_V3_directory,'cuebiq_id_count_distribution_EEZ_V3.csv')\n",
    "Indicators_Classified_filename = os.path.join(OurTable_V3_directory,'Indicators_OurTable.Predictions.csv')\n",
    "check_file_existence(Indicators_Classified_filename)\n",
    "\n",
    "Rec_Indicators_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable.csv')\n",
    "check_file_existence(Rec_Indicators_filename)\n",
    "\n",
    "Rec_Indicators_Step1_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable.Step1.csv')\n",
    "check_file_existence(Rec_Indicators_filename)\n",
    "\n",
    "V3_Indicators_filename =  os.path.join(OurTable_V3_directory,'V3_indicators.csv')\n",
    "check_file_existence(V3_Indicators_filename)\n",
    "\n",
    "Rec_indicators_with_V3_filename = os.path.join(OurTable_V3_directory,'rec_indicators_with_V3.csv')\n",
    "check_file_existence(Rec_indicators_with_V3_filename)\n",
    "\n",
    "Indicators_with_V3_indicators_filename= os.path.join(OurTable_V3_directory,'Indicators_with_V3_indicators_indicators.csv')\n",
    "check_file_existence(Indicators_with_V3_indicators_filename)\n",
    "\n",
    "# Rec_Indicators_Selected_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable_Selected.csv')\n",
    "Rec_Indicators_Selected_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable_Selected_May2024.csv')\n",
    "check_file_existence(Rec_Indicators_Selected_filename)\n",
    "\n",
    "Rec_Indicators_Final_All_Exclusions_And_Disappearance_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_Final_All_Exclusions_And_Disappearance.csv')\n",
    "check_file_existence(Rec_Indicators_Final_All_Exclusions_And_Disappearance_filename)\n",
    "\n",
    "# Sorted_Results_file_path = os.path.join(OurTable_V3_directory,'Indicators_EEZ_and_V3.Predictions.sorted.csv')\n",
    "# RecFishing_Results_file_path =  os.path.join(OurTable_V3_directory,'RecFishingBoat Predictions.sorted.csv')\n",
    "\n",
    "DisappearanceIndicators_filename = os.path.join(OurTable_V3_directory,'DisappearanceIndicators.csv')\n",
    "check_file_existence(DisappearanceIndicators_filename)\n",
    "\n",
    "DisappearanceAnalysis_filename = os.path.join(OurTable_V3_directory,'DisappearanceAnalysis.csv')\n",
    "check_file_existence(DisappearanceAnalysis_filename)\n",
    "\n",
    "Stops_Indicators_filename = os.path.join(OurTable_V3_directory,'Stops_Indicators.csv')\n",
    "check_file_existence(Stops_Indicators_filename)\n",
    "\n",
    "Trawls_Indicators_filename = os.path.join(OurTable_V3_directory,'Trawls_Indicators.csv')\n",
    "check_file_existence(Trawls_Indicators_filename)\n",
    "\n",
    "Stop_Trawls_Indicators_filename = os.path.join(OurTable_V3_directory,'Stop_Trawls_Indicators.csv')\n",
    "check_file_existence(Stop_Trawls_Indicators_filename)\n",
    "\n",
    "Combined_Stops_Indicators_filename = os.path.join(OurTable_V3_directory,'Combined_Stops_Indicators.csv')\n",
    "check_file_existence(Combined_Stops_Indicators_filename)\n",
    "\n",
    "Combined_Trawls_Indicators_filename = os.path.join(OurTable_V3_directory,'Combined_Trawls_Indicators.csv')\n",
    "check_file_existence(Combined_Trawls_Indicators_filename)\n",
    "\n",
    "Combined_Stop_Trawls_Indicators_filename = os.path.join(OurTable_V3_directory,'Combined_Stop_Trawls_Indicators.csv')\n",
    "check_file_existence(Combined_Stop_Trawls_Indicators_filename)\n",
    "\n",
    "##########################################################################################\n",
    "############################### Files that COMBINE BATCH01 and newer data ################\n",
    "Combined_indicators_with_disappearance_filename = os.path.join(OurTable_V3_directory,'Combined_indicators_with_disappearance.csv')\n",
    "check_file_existence(Combined_indicators_with_disappearance_filename)\n",
    "\n",
    "#################################################################\n",
    "####################   Results and Analysis #################################################\n",
    "Station_NonStationAnalysis_filename  = os.path.join(Results_directory,'Station_NonStationAnalysis.csv')\n",
    "check_file_existence(Station_NonStationAnalysis_filename)\n",
    "\n",
    "Station_NonStationAnalysis_full_filename  = os.path.join(Results_directory,'Station_NonStationAnalysis_full.csv')\n",
    "check_file_existence(Station_NonStationAnalysis_full_filename)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "############################### WEATHER data files ####################################\n",
    "Buoys_file_path  = os.path.join(Weather_Data_directory,'Buoys.csv')\n",
    "check_file_existence(Buoys_file_path)\n",
    "\n",
    "Weather_file_path  = os.path.join(Weather_Data_directory,'DailyWeatherData.csv')\n",
    "check_file_existence(Weather_file_path)\n",
    "\n",
    "##########################################################################################\n",
    "############################### SUPPLEMENTARY MAP DATA  ############################\n",
    "Industrial_polygons_filename  = os.path.join(Uploaded_Data_directory,'Polygons Around Industrial Sites.wkt')\n",
    "check_file_existence(Industrial_polygons_filename)\n",
    "\n",
    "\n",
    "station_points_filename = os.path.join(Uploaded_Data_directory,'LA_TX_Union_Station_WGS84.csv')\n",
    "check_file_existence(station_points_filename)\n",
    "\n",
    "MRIP_station_points_filename =os.path.join(Uploaded_Data_directory,'MRIP_stations.csv')\n",
    "check_file_existence(MRIP_station_points_filename)\n",
    "\n",
    "##########################################################################################\n",
    "############################### AIS Files INCLUDING CLASSIFIER ############################\n",
    "RF_Classfier_filename = os.path.join(AIS_Directory, 'rf_model_AIS_2019.pkl')\n",
    "check_file_existence(RF_Classfier_filename)\n",
    "\n",
    "RF_Importance_Factors_filename = os.path.join(AIS_Directory, 'rf_classifier_importance_factors.csv')\n",
    "check_file_existence(RF_Importance_Factors_filename)\n",
    "\n",
    "AIS_Predictions_filename = os.path.join(AIS_Directory, 'RandomForest_Predictions2019AISData.csv')\n",
    "check_file_existence(AIS_Predictions_filename)\n",
    "\n",
    "\n",
    "### Dedicate Table Names for reference\n",
    "# Dedicated table with all Pings within the Gulf WKT for 1/12019 - 4/22/2022\n",
    "#  Table Name:  dedicated.ScheduledExecution5.DeviceTable   \n",
    "#  Code used for call:  RecFishing/DataflowStudioJobs/ScheduledEx5-updated.ipynb\n",
    "\n",
    "# Dedicated table with all Pings within the Gulf WKT AND Origin for 1/12019 - 4/22/2022\n",
    "#  Table Name:  dedicated.ScheduledExecution5_parallel_origin.DeviceTable\n",
    "#  Code used for call:  RecFishing/DataflowStudioJobs/ScheduledEx5-origin.ipynb\n",
    "\n",
    "##########################################################################################\n",
    "############################### Results & Analysis ############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b919be9d-511a-4ab1-b071-5394663bdcca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Date and Distance Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec950a-6d79-42d4-9a5e-dec6f63c0ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates_sequence(\n",
    "    date_start, \n",
    "    date_end, \n",
    "    date_format\n",
    "):\n",
    "    return [\n",
    "        (datetime.strptime(date_start, date_format) + timedelta(days=x)).strftime(date_format)\n",
    "        for x in range (\n",
    "        0,\n",
    "        (datetime.strptime(date_end, date_format) - datetime.strptime(date_start, date_format) + timedelta(days=1)).days\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0adbab-1a4a-42b8-b8bb-dea678bf7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format = \"%Y%m%d\"\n",
    "\n",
    "first_date = \"20190101\"\n",
    "last_date_to_compute = \"20220422\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346c10f-1cba-4528-aef9-360ca2ae0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2, to_radians=True, earth_radius=6371):\n",
    "    if to_radians:\n",
    "        lat1, lon1, lat2, lon2 = np.radians([lat1, lon1, lat2, lon2])\n",
    "\n",
    "    a = np.sin((lat2-lat1)/2.0)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin((lon2-lon1)/2.0)**2\n",
    "\n",
    "    return earth_radius * 2 * np.arcsin(np.sqrt(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe2104-d2e3-444f-aaa9-ff5864ac16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_speed(df):\n",
    "    # Sort the DataFrame based on 'event_timestamp'\n",
    "    df.sort_values(by='event_timestamp', inplace=True)\n",
    "    \n",
    "    # Calculate distances and speeds\n",
    "    df['dist_fwd'] = haversine(df['lat'], df['lng'], df['lat'].shift(1), df['lng'].shift(1))\n",
    "    df['time_fwd'] = df['event_timestamp'] - df['event_timestamp'].shift(1)\n",
    "    df['ping_speed_fwd'] = 60 * abs(df['dist_fwd'] / (0.000001 + df['time_fwd']))\n",
    "    df['ping_speed_fwd'].iloc[0] = 0.0  # KM/minute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7105a7-d897-4183-935d-3e704e6d2b14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## H3 Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5311de9b-42b6-4f59-81ac-3921a0384509",
   "metadata": {},
   "source": [
    "#### Default resolution for all H3 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e8904-efed-4f8e-b614-e97390731fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolution 10 has an average edge length of 75.86 meters, meaning, the distance between two opposite vertices is 151.5 meters\n",
    "resolution = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b9e84-51b6-46e7-ab10-d1f993c47005",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Function to get the h3 cell for a complete data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c42af-bb1c-4a51-8d7d-13d69812a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h3\n",
    "from shapely.geometry import Point\n",
    "import h3\n",
    "\n",
    "# Function to get H3 cells for a GeoDataFrame\n",
    "def get_h3_cells_for_dataframe(dataframe, resolution):\n",
    "    h3_cells = set()\n",
    "    for index, row in dataframe.iterrows():\n",
    "        point = Point(row['lng'], row['lat'])\n",
    "        h3_cells.add(h3.latlng_to_cell(point.y, point.x, resolution))\n",
    "    return h3_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3cb6b2-504f-48b9-8eb7-d1b1d9804e20",
   "metadata": {},
   "source": [
    "#### Function to get H3 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ed394-b815-47a9-86ce-db7c9d88a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h3_cells_and_neighbors(dataframe, resolution):\n",
    "    h3_cells_and_neighbors = set()\n",
    "    for index, row in dataframe.iterrows():\n",
    "        point = Point(row['lng'], row['lat'])\n",
    "        h3_cell = h3.latlng_to_cell(point.y, point.x, resolution)\n",
    "        neighbors = h3.grid_disk(h3_cell, 1)\n",
    "\n",
    "        # Add both the original H3 cell and its neighbors at lower resolution to the set\n",
    "        h3_cells_and_neighbors.add(h3_cell)\n",
    "        h3_cells_and_neighbors.update(neighbors)\n",
    "\n",
    "    return h3_cells_and_neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badbfbda-26ca-406f-8fc1-e6d1bfcd5941",
   "metadata": {},
   "source": [
    "### Function that compares columns in two data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75738bef-d4d1-4856-94d6-904775a3775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompareColumsInTwoDataFrames(df1, df2):\n",
    "    # Get the current frame\n",
    "    frame = inspect.currentframe()\n",
    "    # Get the arguments from the caller's frame\n",
    "    args, _, _, values = inspect.getargvalues(frame.f_back)\n",
    "\n",
    "    # Extract the names of the arguments\n",
    "    df1_name = [name for name in values if values[name] is df1][0]\n",
    "    df2_name = [name for name in values if values[name] is df2][0]\n",
    "\n",
    "    columns_only_in_1 = list(set(df1.columns) - set(df2.columns))\n",
    "    columns_only_in_1 = sorted(columns_only_in_1)\n",
    "\n",
    "    # Get the list of columns in Batch01_merged_df that are not in indicators_df\n",
    "    columns_only_in_2 = list(set(df2.columns) - set(df1.columns))\n",
    "    columns_only_in_2 = sorted(columns_only_in_2)\n",
    "\n",
    "    common_in_both  = list(set(df1.columns).intersection(set(df2.columns)))\n",
    "    common_in_both = sorted(common_in_both)\n",
    "    \n",
    "    if len(columns_only_in_1)>0:\n",
    "        print(\" \")\n",
    "        print(\"Columns in \", df1_name, \"that aren't in\", df2_name,\":\")\n",
    "        print(columns_only_in_1)\n",
    "    else:\n",
    "        print(\"There are no columns in \", df1_name, \"that aren't in\", df2_name)\n",
    "    print(\" \")\n",
    "        \n",
    "\n",
    "    if len(columns_only_in_2)>0:\n",
    "        print(\" \")\n",
    "        print(\"Columns in \", df2_name, \"that aren't in\", df1_name,\":\")\n",
    "        print(columns_only_in_2)\n",
    "    else:\n",
    "        print(\"There are no columns in \", df2_name, \"that aren't in\", df1_name)\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"Columns in both\", df1_name, \"and\", df2_name, \":\", common_in_both)\n",
    "\n",
    "# CompareColumsInTwoDataFrames(Batch01_ind1_df, indicators_df)\n",
    "# indicators_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ab9036-3ddb-4b8d-9f72-ec414e7cd5fc",
   "metadata": {},
   "source": [
    "### Function that gets H3 neighbors, and the neighbors of the neighbors -- a double ring around the original points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284cdec-c7af-4a84-ad2b-869ada76a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h3_cells_and_neighbors_two_levels(dataframe, resolution):\n",
    "    h3_cells_and_neighbors = set()\n",
    "    for index, row in dataframe.iterrows():\n",
    "        point = Point(row['lng'], row['lat'])\n",
    "        h3_cell = h3.latlng_to_cell(point.y, point.x, resolution)\n",
    "        neighbors = h3.grid_disk(h3_cell, 1)\n",
    "        \n",
    "        # Add the original H3 cell and its neighbors at the current resolution\n",
    "        h3_cells_and_neighbors.add(h3_cell)\n",
    "        h3_cells_and_neighbors.update(neighbors)\n",
    "        \n",
    "        # Add neighbors of neighbors at the same resolution\n",
    "        for neighbor in neighbors:\n",
    "            second_layer_neighbors = h3.grid_disk(neighbor, 1)\n",
    "            h3_cells_and_neighbors.update(second_layer_neighbors)\n",
    "\n",
    "    return h3_cells_and_neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a746f9c-b444-458a-be30-26f5a6f04059",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Errant pings code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68824ef-3059-43c3-952c-364bd6ed4adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2, to_radians=True, earth_radius=6371):\n",
    "    if to_radians:\n",
    "        lat1, lon1, lat2, lon2 = np.radians([lat1, lon1, lat2, lon2])\n",
    "\n",
    "    a = np.sin((lat2-lat1)/2.0)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin((lon2-lon1)/2.0)**2\n",
    "\n",
    "    return earth_radius * 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def EliminateErrantPingsSpeed(pings_df, mph_limit):\n",
    "    km_per_min_limit = mph_limit*(0.0268224)\n",
    "    # Calculate speed moving forward e.g., row 0 is time since previous trip\n",
    "\n",
    "    pings_df.sort_values(by='event_timestamp', inplace=True)\n",
    "    pings_df = pings_df.drop_duplicates()\n",
    "\n",
    "\n",
    "    # create (or recreate) the time difference variables\n",
    "    pings_df['time_diff_minutes_from_previous'] = pings_df[\"event_timestamp\"].diff()/60.0\n",
    "    pings_df['time_diff_minutes_from_previous'].fillna(value=0, inplace=True)\n",
    "\n",
    "    pings_df['time_diff_minutes_to_next'] = pings_df[\"event_timestamp\"].diff(-1)/60.0\n",
    "    pings_df['time_diff_minutes_to_next'].fillna(value=99999, inplace=True)\n",
    "\n",
    "    \n",
    "    pings_df_shifted_down = pings_df.shift(1)\n",
    "    pings_df['dist_fwd'] = haversine(pings_df['lat'], pings_df['lng'], pings_df_shifted_down['lat'], pings_df_shifted_down['lng'])\n",
    "    pings_df['ping_speed_fwd'] = abs(pings_df['dist_fwd']/(0.00001+pings_df['time_diff_minutes_from_previous']))\n",
    "    pings_df['ping_speed_fwd'].fillna(value=0, inplace=True)\n",
    "\n",
    "    # Calculate speed moving backward e.g., first row is the speed to the next ping\n",
    "    pings_df_shifted_up = pings_df.shift(-1)\n",
    "    pings_df['dist_bkwd'] = haversine(pings_df['lat'], pings_df['lng'], pings_df_shifted_up['lat'], pings_df_shifted_up['lng'])\n",
    "    pings_df['ping_speed_bkwd'] = abs(pings_df['dist_bkwd']/(0.00001+pings_df['time_diff_minutes_to_next']))\n",
    "    pings_df['ping_speed_bkwd'].fillna(value=0, inplace=True)\n",
    "\n",
    "    pings_df['Avg_ping_speed'] = (pings_df['ping_speed_fwd'] + pings_df['ping_speed_bkwd']) / 2\n",
    "    pings_df['row_index'] = pings_df.reset_index().index\n",
    "\n",
    "    # Step 2: Check if the maximum of ping_speed > km_per_min_limit\n",
    "    iIteration=0\n",
    "    while len(pings_df) > 2 and pings_df['Avg_ping_speed'].max() > km_per_min_limit:\n",
    "        iIteration=iIteration+1\n",
    "\n",
    "        max_index = pings_df['Avg_ping_speed'].idxmax()\n",
    "    \n",
    "        # Step 4: Recalculate ping_speed_fwd for the row after the row that was dropped\n",
    "        if max_index + 1 < len(pings_df) and max_index - 1 >= 0:\n",
    "            lat_after = pings_df.iloc[max_index+1]['lat']\n",
    "            lon_after = pings_df.iloc[max_index+1]['lng']\n",
    "            lat_before = pings_df.iloc[max_index - 1]['lat']\n",
    "            lon_before = pings_df.iloc[max_index - 1]['lng']\n",
    "            distance = haversine(lat_before, lon_before, lat_after, lon_after)\n",
    "            time_diff = pings_df.iloc[max_index+1]['event_timestamp']-pings_df.iloc[max_index-1]['event_timestamp']\n",
    "            new_speed = distance /time_diff\n",
    "            \n",
    "            # Calculate new fwd speed for the row before\n",
    "            index_before = max_index - 1\n",
    "            index_after = max_index + 1\n",
    "            \n",
    "            # Update the value using .loc[] or .iloc[] with a single call\n",
    "            pings_df.loc[index_before, 'ping_speed_fwd'] = new_speed\n",
    "            pings_df.loc[index_after, 'ping_speed_bkwd'] = new_speed\n",
    "\n",
    "            pings_df = pings_df[pings_df['event_timestamp'].notna() & (pings_df['event_timestamp'] != '')]\n",
    "\n",
    "        \n",
    "        ################ Debugging ###############\n",
    "        pings_df = pings_df.drop(max_index)\n",
    "        pings_df['Avg_ping_speed'] = (pings_df['ping_speed_fwd'] + pings_df['ping_speed_bkwd']) / 2\n",
    "\n",
    "        # Reset index (I don't know if this is really necessary)\n",
    "        pings_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return pings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424eb0b4-3ab0-476f-a40b-29d16e495c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Merge Trip DFs\n",
    "Merges two data frames based on cuebiq_id and Trip_number. If the list of IDs is not identical, this returns an empty data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f0126e-fac2-4c0b-9106-21ffb24dca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_Trip_dfs(df1, df2):\n",
    "    # Check if the lists of values of cuebiq_id are the same in both data frames\n",
    "    if set(df1['cuebiq_id']) != set(df2['cuebiq_id']):\n",
    "        print(\"The two data frames do not have the same values of cuebiq_id\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Identify common columns, excluding 'cuebiq_id' and 'Trip_number'\n",
    "    common_cols = [col for col in df1.columns if col in df2.columns and col not in ['cuebiq_id', 'Trip_number']]\n",
    "    \n",
    "    # Drop common columns from df2\n",
    "    df2 = df2.drop(columns=common_cols)\n",
    "    \n",
    "    # Merge the data frames on 'cuebiq_id' and 'Trip_number'\n",
    "    merged_df = pd.merge(df1, df2, on=['cuebiq_id', 'Trip_number'], how='inner')\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19386eec-0cf4-476d-9c46-1b4c7cabdac0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Auxiliary Files -- polygons, stations, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec6e82-4438-4766-85f3-c3ae713de67b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load stations (MRIP & TPWD) and find H3 cells and neighbors to those cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a929f4-89ca-4b20-8ec0-005a245ada95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataframes\n",
    "\n",
    "# Texas -- Data provided Mark Fisher <Mark.Fisher@tpwd.texas.gov> on 6/3/2022\n",
    "TX_station_points_df = pd.read_csv('../../uploaded_files/TPWD Stations.csv', encoding='latin-1')\n",
    "TX_station_points_df.rename(columns={'y': 'lat', 'x': 'lng'}, inplace=True)\n",
    "TX_station_points_df.dropna(subset=['lat'], inplace=True)  # Drop empty rows\n",
    "TX_station_points_h3_cells = get_h3_cells_and_neighbors(TX_station_points_df, resolution)\n",
    "\n",
    "# MRIP MS, AL & FL -- downloaded from the site NOAA Site Register\n",
    "MRIP_station_points_df = pd.read_csv('../../uploaded_files/MRIP-sites-LA-AL-MS.csv')\n",
    "MRIP_station_points_df.rename(columns={'SITE_LAT': 'lat', 'SITE_LONG': 'lng'}, inplace=True)\n",
    "\n",
    "AL_MRIP_station_points_df = MRIP_station_points_df[MRIP_station_points_df['STATE_CODE'] == 1].copy()\n",
    "MS_MRIP_station_points_df = MRIP_station_points_df[MRIP_station_points_df['STATE_CODE'] == 28].copy()\n",
    "\n",
    "AL_MRIP_station_points_h3_cells= get_h3_cells_and_neighbors(AL_MRIP_station_points_df, resolution)\n",
    "MS_MRIP_station_points_h3_cells= get_h3_cells_and_neighbors(MS_MRIP_station_points_df, resolution)\n",
    "\n",
    "FL_MRIP_station_points_df = pd.read_csv('../../uploaded_files/FL Sites -- MRIP Site Registry Escambia and Santa Rosa Counties.csv')\n",
    "FL_MRIP_station_points_df.rename(columns={'SITE_LAT': 'lat', 'SITE_LONG': 'lng'}, inplace=True)\n",
    "FL_MRIP_station_points_df = FL_MRIP_station_points_df[FL_MRIP_station_points_df['STATUS'] == \"Active\"].copy()\n",
    "FL_MRIP_station_points_h3_cells = get_h3_cells_and_neighbors(FL_MRIP_station_points_df, resolution)\n",
    "\n",
    "# LA Creel Stations -- personal communication from Nicole Smith (WLF) <nsmith@wlf.la.gov> on 12/13/23\n",
    "LA_CREEL_station_points_df = pd.read_csv('../../uploaded_files/LA creel sites-1.csv')\n",
    "LA_CREEL_station_points_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lng'}, inplace=True)\n",
    "LA_CREEL_station_points_df = LA_CREEL_station_points_df[LA_CREEL_station_points_df['Active_Flg'] == 1].copy()\n",
    "LA_CREEL_station_points_df = LA_CREEL_station_points_df[LA_CREEL_station_points_df['lat'] >0].copy()\n",
    "LA_CREEL_station_points_h3_cells= get_h3_cells_and_neighbors(LA_CREEL_station_points_df, resolution)\n",
    "\n",
    "# All Ports in GOM Gathered from Marine Traffic Website\n",
    "MarineTrafficPorts_df = pd.read_csv('../../uploaded_files/MarineTrafficPorts.csv')\n",
    "MarineTrafficPorts_df.rename(columns={'lon': 'lng'}, inplace=True)\n",
    "\n",
    "LargePorts_Marine_traffic_df = MarineTrafficPorts_df[MarineTrafficPorts_df['MarineTrafficPortType_num'] == 3].copy()\n",
    "Medium_Anchorage_Marine_traffic_df = MarineTrafficPorts_df[MarineTrafficPorts_df['MarineTrafficPortType_num'] == 4].copy()\n",
    "Medium_Marina_Marine_traffic_df = MarineTrafficPorts_df[MarineTrafficPorts_df['MarineTrafficPortType_num'] == 5].copy()\n",
    "Medium_Port_Marine_traffic_df = MarineTrafficPorts_df[MarineTrafficPorts_df['MarineTrafficPortType_num'] == 7].copy()\n",
    "Small_Marina_Marine_traffic_df = MarineTrafficPorts_df[MarineTrafficPorts_df['MarineTrafficPortType_num'] == 9].copy()\n",
    "Small_Port_Marine_traffic_df = MarineTrafficPorts_df[MarineTrafficPorts_df['MarineTrafficPortType_num'] == 10].copy()\n",
    "\n",
    "LargePorts_Marine_traffic_h3_cells= get_h3_cells_and_neighbors(LargePorts_Marine_traffic_df, resolution)\n",
    "Medium_Anchorage_Marine_traffic_h3_cells= get_h3_cells_and_neighbors(Medium_Anchorage_Marine_traffic_df, resolution)\n",
    "Medium_Marina_Marine_traffic_h3_cells= get_h3_cells_and_neighbors(Medium_Marina_Marine_traffic_df, resolution)\n",
    "Medium_Port_Marine_traffic_h3_cells= get_h3_cells_and_neighbors(Medium_Port_Marine_traffic_df, resolution)\n",
    "Small_Marina_Marine_traffic_h3_cells= get_h3_cells_and_neighbors(Small_Marina_Marine_traffic_df, resolution)\n",
    "Small_Port_Marine_traffic_h3_cells= get_h3_cells_and_neighbors(Small_Port_Marine_traffic_df, resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab91d55e-e7c6-4c22-81fc-756ba1dc0f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maps of stations\n",
    "state_station_points_df = pd.read_csv('../../uploaded_files/LA_TX_Union_Station_WGS84.csv')\n",
    "\n",
    "# MRIP_station_points_df = pd.read_csv('../uploaded_files/MRIP_stations.csv')\n",
    "MRIP_station_points_df = pd.read_csv('../../uploaded_files/MRIP-sites-LA-AL-MS.csv')\n",
    "\n",
    "MRIP_station_points_df.rename(columns={'SITE_LAT': 'lat', 'SITE_LONG': 'lng'}, inplace=True)\n",
    "\n",
    "# Create sets of H3 cells and their neighbors for each dataframe\n",
    "state_station_h3_cells = get_h3_cells_and_neighbors(state_station_points_df, resolution)\n",
    "state_station_h3_cells_alone = get_h3_cells_for_dataframe(state_station_points_df, resolution)\n",
    "\n",
    "mrip_station_h3_cells = get_h3_cells_and_neighbors(MRIP_station_points_df, resolution)\n",
    "mrip_station_h3_cells_alone = get_h3_cells_for_dataframe(MRIP_station_points_df, resolution)\n",
    "\n",
    "# Combine all the h3 cells into a single set\n",
    "combined_h3_cells = state_station_h3_cells | mrip_station_h3_cells\n",
    "combined_h3_cells_alone = state_station_h3_cells_alone | mrip_station_h3_cells_alone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d58b8-dde2-485a-a5ff-deafcff2115b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### State polygons for identifying state of starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac918895-cdb5-450d-9a2b-d03209be6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.wkt import loads\n",
    "\n",
    "## WKT strings from <script src=\"https://gist.github.com/JoshuaCarroll/49630cbeeb254a49986e939a26672e9c.js\"></script>\n",
    "\n",
    "# # Test on a point on a rectangular state\n",
    "# wkt_string = \"POLYGON((-109.0448 37.0004,-102.0424 36.9949,-102.0534 41.0006,-109.0489 40.9996,-109.0448 37.0004,-109.0448 37.0004))\"\n",
    "# colorado_polygon = loads(wkt_string)\n",
    "# min_lat,min_lng,max_lat,max_lng=colorado_polygon.bounds\n",
    "\n",
    "# avg_lat = .5*(min_lat+max_lat)\n",
    "# avg_lng =  .5*(min_lng+max_lng)\n",
    "# avg_point = Point(avg_lat,avg_lng)\n",
    "\n",
    "# is_within_polygon = avg_point.within(colorado_polygon)\n",
    "# print(avg_point,\"is_within_COLORADO\",is_within_polygon)\n",
    "\n",
    "\n",
    "# Define the WKT string for the TX polygon\n",
    "TX_wkt_string = \"POLYGON((-106.5715 31.8659,-106.5042 31.7504,-106.3092 31.6242,-106.2103 31.4638,-106.0181 31.3912,-105.7874 31.1846,-105.5663 31.0012,-105.4015 30.8456,-105.0032 30.6462,-104.8521 30.3847,-104.7437 30.2591,-104.6915 30.0738,-104.6777 29.9169,-104.5679 29.7644,-104.5280 29.6475,-104.4044 29.5603,-104.2067 29.4719,-104.1559 29.3834,-103.9774 29.2948,-103.9128 29.2804,-103.8208 29.2481,-103.5640 29.1378,-103.4692 29.0682,-103.3154 29.0105,-103.1616 28.9601,-103.0957 29.0177,-103.0298 29.1330,-102.8677 29.2157,-102.8979 29.2565,-102.8375 29.3570,-102.8004 29.4898,-102.7002 29.6881,-102.5134 29.7691,-102.3843 29.7596,-102.3047 29.8788,-102.1509 29.7834,-101.7004 29.7572,-101.4917 29.7644,-101.2939 29.6308,-101.2582 29.5269,-101.0056 29.3642,-100.9204 29.3056,-100.7707 29.1642,-100.7007 29.0946,-100.6306 28.9012,-100.4974 28.6593,-100.3601 28.4675,-100.2969 28.2778,-100.1733 28.1882,-100.0195 28.0526,-99.9344 27.9435,-99.8438 27.7638,-99.7119 27.6641,-99.4812 27.4839,-99.5375 27.3059,-99.4290 27.1948,-99.4455 27.0175,-99.3164 26.8829,-99.2065 26.6867,-99.0967 26.4116,-98.8138 26.3574,-98.6668 26.2257,-98.5474 26.2343,-98.3276 26.1357,-98.1697 26.0457,-97.9143 26.0518,-97.6643 26.0050,-97.4020 25.8419,-97.3526 25.9074,-97.0148 25.9679,-97.0697 26.1789,-97.2249 26.8253,-97.0752 27.4230,-96.6096 28.0599,-95.9285 28.4228,-95.3036 28.7568,-94.7296 29.0742,-94.3355 29.3810,-93.8205 29.6021,-93.9317 29.8013,-93.8136 29.9157,-93.7230 30.0489,-93.6996 30.1214,-93.7216 30.2021,-93.7038 30.2792,-93.7628 30.3278,-93.7587 30.3835,-93.7010 30.4380,-93.7024 30.5079,-93.7299 30.5362,-93.6694 30.6296,-93.6090 30.7466,-93.5527 30.8114,-93.5747 30.8834,-93.5307 30.9376,-93.5074 31.0318,-93.5266 31.0812,-93.5335 31.1787,-93.5980 31.1670,-93.6832 31.3055,-93.6708 31.3830,-93.6887 31.4369,-93.7202 31.5107,-93.8315 31.5820,-93.8123 31.6440,-93.8232 31.7188,-93.8342 31.7936,-93.8782 31.8309,-93.9221 31.8869,-93.9661 31.9335,-94.0430 32.0081,-94.0430 33.4681,-94.0430 33.5414,-94.1528 33.5689,-94.1968 33.5872,-94.2627 33.5872,-94.3176 33.5689,-94.3945 33.5597,-94.4275 33.5780,-94.4275 33.6055,-94.4495 33.6421,-94.4879 33.6329,-94.5236 33.6421,-94.6637 33.6695,-94.7461 33.7061,-94.8999 33.7791,-95.0757 33.8818,-95.1526 33.9251,-95.2254 33.9604,-95.2858 33.8750,-95.5399 33.8841,-95.7568 33.8887,-95.8420 33.8408,-96.0274 33.8556,-96.3528 33.6901,-96.6179 33.8442,-96.5836 33.8898,-96.6673 33.8955,-96.7538 33.8179,-96.8335 33.8613,-96.8774 33.8613,-96.9159 33.9388,-97.0917 33.7392,-97.1645 33.7449,-97.2180 33.8978,-97.3746 33.8225,-97.4611 33.8305,-97.4460 33.8761,-97.6945 33.9798,-97.8648 33.8476,-97.9651 33.8978,-98.0983 34.0299,-98.1752 34.1141,-98.3743 34.1425,-98.4773 34.0640,-98.5529 34.1209,-98.7520 34.1232,-98.9539 34.2095,-99.0637 34.2073,-99.1832 34.2141,-99.2505 34.3593,-99.3823 34.4613,-99.4318 34.3774,-99.5718 34.4160,-99.6158 34.3706,-99.8094 34.4726,-99.9934 34.5631,-100.0017 36.4975,-103.0408 36.5008,-103.0655 32.0011,-106.6168 32.0023,-106.5715 31.8659))\"\n",
    "TX_polygon = loads(TX_wkt_string)\n",
    "\n",
    "# Define the WKT string for the LA polygon\n",
    "LA_wkt_string = \"POLYGON((-94.0430 33.0225,-93.0048 33.0179,-91.1646 33.0087,-91.2209 32.9269,-91.1220 32.8773,-91.1481 32.8358,-91.1412 32.7642,-91.1536 32.6382,-91.1069 32.5804,-91.0080 32.6093,-91.0904 32.4588,-91.0355 32.4379,-91.0286 32.3742,-90.9064 32.3150,-90.9723 32.2616,-91.0464 32.1942,-91.0739 32.1198,-91.0464 32.0593,-91.1014 31.9918,-91.1865 31.9498,-91.3101 31.8262,-91.3527 31.7947,-91.3925 31.6230,-91.5134 31.6218,-91.4310 31.5668,-91.5161 31.5130,-91.5244 31.3701,-91.5477 31.2598,-91.6425 31.2692,-91.6603 31.2328,-91.5848 31.1917,-91.6287 31.1047,-91.5614 31.0318,-91.6397 30.9988,-89.7336 31.0012,-89.8517 30.6686,-89.7858 30.5386,-89.6347 30.3148,-89.5688 30.1807,-89.4960 30.1582,-89.1843 30.2140,-89.0373 30.1463,-88.8354 30.0905,-88.7421 29.8383,-88.8712 29.5758,-88.9371 29.1833,-89.0359 28.9649,-89.2282 28.8832,-89.4754 28.9048,-89.7418 29.1210,-90.1126 28.9529,-90.6619 28.9120,-91.0355 28.9553,-91.3211 29.1210,-91.9061 29.2864,-92.7452 29.4360,-93.8177 29.6009,-93.8631 29.6749,-93.8933 29.7370,-93.9304 29.7930,-93.9276 29.8216,-93.8370 29.8883,-93.7985 29.9811,-93.7601 30.0144,-93.7106 30.0691,-93.7354 30.0929,-93.6996 30.1166,-93.7271 30.1997,-93.7106 30.2899,-93.7656 30.3350,-93.7601 30.3871,-93.6914 30.4416,-93.7106 30.5102,-93.7463 30.5433,-93.7106 30.5954,-93.6914 30.5906,-93.6859 30.6545,-93.6365 30.6781,-93.6200 30.7513,-93.5925 30.7890,-93.5513 30.8150,-93.5623 30.8645,-93.5788 30.8881,-93.5541 30.9187,-93.5294 30.9423,-93.5760 31.0082,-93.5101 31.0318,-93.5596 31.0906,-93.5321 31.1211,-93.5349 31.1799,-93.5953 31.1658,-93.6282 31.2292,-93.6118 31.2668,-93.6859 31.3044,-93.6694 31.3888,-93.7051 31.4240,-93.6859 31.4427,-93.7573 31.4755,-93.7189 31.5083,-93.8040 31.5411,-93.8425 31.6113,-93.8205 31.6581,-93.7985 31.7071,-93.8480 31.8029,-93.9029 31.8892,-93.9606 31.9149,-94.0430 32.0081,-94.0430 32.7041,-94.0430 33.0225,-94.0430 33.0225))\"\n",
    "LA_polygon = loads(LA_wkt_string)\n",
    "\n",
    "# Define the WKT string for the MS polygon\n",
    "MS_wkt_string = \"POLYGON((-90.3049 35.0041,-88.1955 35.0075,-88.0994 34.8882,-88.1241 34.7044,-88.2573 33.6661,-88.4756 31.8939,-88.4180 30.8657,-88.3850 30.1594,-88.8327 30.0905,-89.1870 30.2104,-89.4919 30.1570,-89.5757 30.1796,-89.6457 30.3326,-89.7748 30.5232,-89.8531 30.6663,-89.7377 30.9988,-91.6287 30.9988,-91.5601 31.0341,-91.6273 31.1106,-91.5916 31.1658,-91.6589 31.2304,-91.6452 31.2656,-91.5436 31.2609,-91.5271 31.3724,-91.5161 31.4099,-91.5120 31.5071,-91.4502 31.5692,-91.5147 31.6230,-91.3966 31.6253,-91.3513 31.7936,-91.2744 31.8589,-91.1673 31.9755,-91.0767 32.0267,-91.0767 32.1198,-91.0437 32.1942,-91.0107 32.2221,-90.9132 32.3150,-91.0313 32.3742,-91.0217 32.4263,-91.0986 32.4634,-91.0080 32.6070,-91.1096 32.5746,-91.1536 32.6394,-91.1426 32.7226,-91.1426 32.7873,-91.1536 32.8519,-91.1206 32.8796,-91.2195 32.9257,-91.2085 32.9995,-91.2016 33.0444,-91.2016 33.1192,-91.1041 33.1835,-91.1536 33.3397,-91.1646 33.4223,-91.2291 33.4337,-91.2524 33.5414,-91.1838 33.6135,-91.2524 33.6878,-91.1261 33.6969,-91.1426 33.7883,-91.0437 33.7700,-91.0327 33.8339,-91.0657 33.8795,-91.0876 33.9434,-90.9998 33.9889,-90.9229 34.0253,-90.9009 34.0891,-90.9668 34.1345,-90.9119 34.1709,-90.8501 34.1345,-90.9338 34.2277,-90.8267 34.2833,-90.6921 34.3434,-90.6509 34.3774,-90.6152 34.3978,-90.5589 34.4432,-90.5740 34.5179,-90.5823 34.5880,-90.5356 34.7506,-90.5136 34.7913,-90.4532 34.8780,-90.3543 34.8476,-90.2911 34.8702,-90.3062 35.0041,-90.3049 35.0041))\"\n",
    "MS_polygon = loads(MS_wkt_string)\n",
    "\n",
    "# Define the WKT string for the AL polygon\n",
    "AL_wkt_string = \"POLYGON((-88.1955 35.0041,-85.6068 34.9918,-85.1756 32.8404,-84.8927 32.2593,-85.0342 32.1535,-85.1358 31.7947,-85.0438 31.5200,-85.0836 31.3384,-85.1070 31.2093,-84.9944 31.0023,-87.6009 30.9953,-87.5926 30.9423,-87.6256 30.8539,-87.4072 30.6745,-87.3688 30.4404,-87.5240 30.1463,-88.3864 30.1546,-88.4743 31.8939,-88.1021 34.8938,-88.1721 34.9479,-88.1461 34.9107,-88.1955 35.0041))\"\n",
    "AL_polygon = loads(AL_wkt_string)\n",
    "\n",
    "# Define the WKT string for the AL polygon\n",
    "FL_wkt_string = \"POLYGON((-87.6050 30.9988,-86.5613 30.9964,-85.5313 31.0035,-85.1193 31.0012,-85.0012 31.0023,-84.9847 30.9364,-84.9367 30.8845,-84.9271 30.8409,-84.9257 30.7902,-84.9147 30.7489,-84.8611 30.6993,-84.4272 30.6911,-83.5991 30.6509,-82.5595 30.5895,-82.2134 30.5682,-82.2134 30.5315,-82.1997 30.3883,-82.1544 30.3598,-82.0638 30.3598,-82.0226 30.4877,-82.0473 30.6308,-82.0514 30.6757,-82.0377 30.7111,-82.0514 30.7371,-82.0102 30.7678,-82.0322 30.7914,-81.9717 30.7997,-81.9608 30.8244,-81.8893 30.8056,-81.8372 30.7914,-81.7960 30.7796,-81.6696 30.7536,-81.6051 30.7289,-81.5666 30.7324,-81.5295 30.7229,-81.4856 30.7253,-81.4609 30.7111,-81.4169 30.7088,-81.2274 30.7064,-81.2357 30.4345,-81.1725 30.3160,-81.0379 29.7763,-80.5861 28.8603,-80.3650 28.4771,-80.3815 28.1882,-79.9255 27.1789,-79.8198 26.8425,-79.9118 26.1394,-79.9997 25.5115,-80.3815 24.8802,-80.8704 24.5384,-81.9250 24.3959,-82.2066 24.4496,-82.3137 24.5484,-82.1997 24.6982,-81.3977 25.2112,-81.4622 25.6019,-81.9456 25.9235,-82.2876 26.3439,-82.5307 26.9098,-82.8342 27.3315,-83.0182 27.7565,-83.0017 28.0574,-82.8548 28.6098,-83.0264 28.9697,-83.2050 29.0478,-83.5318 29.4157,-83.9767 29.9133,-84.1072 29.8930,-84.4409 29.6940,-85.0465 29.4551,-85.3610 29.4946,-85.5807 29.7262,-86.1946 30.1594,-86.8510 30.2175,-87.5171 30.1499,-87.4429 30.3006,-87.3750 30.4256,-87.3743 30.4830,-87.3907 30.5658,-87.4004 30.6344,-87.4141 30.6763,-87.5253 30.7702,-87.6256 30.8527,-87.5912 30.9470,-87.5912 30.9682,-87.6050 30.9964,-87.6050 30.9988))\"\n",
    "FL_polygon = loads(FL_wkt_string)\n",
    "\n",
    "# Find the min and max bounds for each state\n",
    "# TX_min_lat,TX_min_lng,TX_max_lat,TX_max_lng=TX_polygon.bounds\n",
    "# LA_min_lat,LA_min_lng,LA_max_lat,LA_max_lng=LA_polygon.bounds\n",
    "# MS_min_lat,MS_min_lng,MS_max_lat,MS_max_lng=MS_polygon.bounds\n",
    "# AL_min_lat,AL_min_lng,AL_max_lat,AL_max_lng=AL_polygon.bounds\n",
    "# FL_min_lat,FL_min_lng,AL_max_lat,FL_max_lng=FL_polygon.bounds\n",
    "\n",
    "TX_min_lng,TX_min_lat,TX_max_lng,TX_max_lat=TX_polygon.bounds\n",
    "LA_min_lng,LA_min_lat,LA_max_lng,LA_max_lat=LA_polygon.bounds\n",
    "MS_min_lng,MS_min_lat,MS_max_lng,MS_max_lat=MS_polygon.bounds\n",
    "AL_min_lng,AL_min_lat,AL_max_lng,AL_max_lat=AL_polygon.bounds\n",
    "FL_min_lng,FL_min_lat,FL_max_lng,FL_max_lat=FL_polygon.bounds\n",
    "\n",
    "\n",
    "avg_lat = .5*(TX_min_lat+TX_max_lat)\n",
    "avg_lng = .5*(TX_min_lng+TX_max_lng)\n",
    "avg_point = Point(avg_lng,avg_lat)\n",
    "\n",
    "TX_within_polygon = avg_point.within(TX_polygon)\n",
    "LA_within_polygon = avg_point.within(LA_polygon)\n",
    "MS_within_polygon = avg_point.within(MS_polygon)\n",
    "AL_within_polygon = avg_point.within(AL_polygon)\n",
    "FL_within_polygon = avg_point.within(FL_polygon)\n",
    "\n",
    "# # Initialize variables\n",
    "TX = LA = MS = AL = FL = 0\n",
    "if TX_within_polygon:\n",
    "    TX = 1\n",
    "elif LA_within_polygon:\n",
    "    LA = 1\n",
    "elif MS_within_polygon:\n",
    "    MS = 1\n",
    "elif AL_within_polygon:\n",
    "    AL = 1\n",
    "elif FL_within_polygon:\n",
    "    FL = 1\n",
    "    \n",
    "print(avg_point,\"is_within_TX\",TX_within_polygon)\n",
    "print(avg_point,\"is_within_LA\",LA_within_polygon)\n",
    "print(avg_point,\"is_within_MS\",MS_within_polygon)\n",
    "print(avg_point,\"is_within_AL\",AL_within_polygon)\n",
    "print(avg_point,\"is_within_FL\",FL_within_polygon)\n",
    "\n",
    "print(TX,LA,MS,AL,FL)\n",
    "print(avg_lat,avg_lng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83a3152-0116-4bd2-a70c-d03aea9cd3fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Prepare for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7615286-7ed2-4025-a9ee-2364f938dbdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load csv files from into data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8120e5a5-0718-4555-a3a1-d5cf5b047bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input files\n",
    "disappear_df = pd.read_csv(DisappearanceIndicators_filename)\n",
    "indicators_df = pd.read_csv(Rec_indicators_with_V3_filename)\n",
    "\n",
    "rec_indicators_df = pd.merge(\n",
    "    disappear_df,\n",
    "    indicators_df,\n",
    "    on=['cuebiq_id', 'Trip_number'],\n",
    "    how='outer',  # Use 'outer' to keep all rows from both DataFrames\n",
    ")\n",
    "\n",
    "\n",
    "V3_Pings_df = pd.read_csv(Combined_Pings_V3_Before_After_filename)\n",
    "Pings_OurTable_Gulf_df= pd.read_csv(Combined_Pings_OurTable_Gulf_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ebf9bc-1140-4bdb-9155-9e063610360e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Maps of stops and trajectories before and after a trip the points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9549976-ed48-41f4-b1df-5a5ebf1599ed",
   "metadata": {},
   "source": [
    "#### Load indicators and device-table data. Add a unique identifier for each trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d05caa-1f59-4c35-8c40-12331ad6a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_indicators_df = rec_indicators_df.sort_values(by=['cuebiq_id', 'Trip_number']).reset_index(drop=True)\n",
    "rec_indicators_df['Trip_number_full_list'] = rec_indicators_df.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828f228-fcab-449f-8e0b-a29bb44dfb85",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Map the stops for a single trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b201a8-4fdb-4e85-890e-6844ce12b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2000\n",
    "\n",
    "# Load the stations\n",
    "station_points_df = pd.read_csv(station_points_filename)\n",
    "MRIP_station_points_df = pd.read_csv(MRIP_station_points_filename)\n",
    "\n",
    "# Grab the TripToLookAt row from rec_indicators_df\n",
    "trip_row = rec_indicators_df.iloc[index]\n",
    "\n",
    "# EXTRACT THE ROWS FROM THE DEVICE TABLE DATA SET THAT ARE 8 HOURS ON EITHER SIDE OF THE TRIP\n",
    "cuebiqid_value = trip_row['cuebiq_id']\n",
    "trip_start_epochtime = trip_row['timestamp_start_t']\n",
    "trip_end_epochtime = trip_row['timestamp_end_t']\n",
    "\n",
    "# Conversion factors\n",
    "Eight_hours = 8*60*60\n",
    "km_per_min_To_mph = 37.2823\n",
    "mph_To_km_per_min = 1/37.2823\n",
    "\n",
    "########### Create a data frame for points before and after the trip #######\n",
    "# Choose the right cuebiq_id\n",
    "Device_df = V3_Pings_df[(V3_Pings_df['cuebiq_id'] == cuebiqid_value) & \n",
    "                        (V3_Pings_df['event_timestamp'] >= trip_start_epochtime-Eight_hours) &\n",
    "                        (V3_Pings_df['event_timestamp'] <= trip_end_epochtime+Eight_hours)\n",
    "                         ]\n",
    "Device_df = Device_df.sort_values(by='event_timestamp')\n",
    "# Eliminate inaccuate pings\n",
    "Device_df = EliminateErrantPingsSpeed(Device_df, 90)\n",
    "# Filter out pings that occurred during the trip\n",
    "Device_df = Device_df[~((Device_df['event_timestamp'] > trip_start_epochtime) & (Device_df['event_timestamp'] < trip_end_epochtime))]\n",
    "\n",
    "Gulf_Pings_df = Pings_OurTable_Gulf_df[(Pings_OurTable_Gulf_df['cuebiq_id'] == cuebiqid_value) & \n",
    "                                      (Pings_OurTable_Gulf_df['event_timestamp'] >= trip_start_epochtime) &\n",
    "                                      (Pings_OurTable_Gulf_df['event_timestamp'] <= trip_end_epochtime)]\n",
    "Gulf_Pings_df = EliminateErrantPingsSpeed(Gulf_Pings_df, 60)                                                             \n",
    "                                                             \n",
    "# print(cuebiqid_value,\"len(Device_df)\", len(Device_df))\n",
    "\n",
    "##  FIRST PLL THE LINSE FROM THE DEVICE TABLE BEFORE AND AFTER\n",
    "DuringTripLine = Gulf_Pings_df[['lat', 'lng']].values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Prepare before and after strings of points, eliminating errant pings\n",
    "before_df=[]\n",
    "before_df= Device_df[Device_df['event_timestamp'] <= trip_start_epochtime]\n",
    "print(\"before_df\", len(before_df))\n",
    "if len(before_df) > 1:\n",
    "    BeforeTripLine = before_df[['lat', 'lng']].values.tolist()\n",
    "    calculate_speed(before_df)\n",
    "    stops_before_df = before_df[before_df['ping_speed_fwd'] < mph_To_km_per_min].copy()\n",
    "\n",
    "after_df = []\n",
    "after_df = Device_df[Device_df['event_timestamp'] >= trip_end_epochtime]\n",
    "if len(after_df)>1:\n",
    "    AfterTripLine = after_df[['lat', 'lng']].values.tolist()\n",
    "    calculate_speed(after_df)\n",
    "    stops_after_df = after_df[after_df['ping_speed_fwd'] < mph_To_km_per_min].copy()\n",
    "\n",
    "print(index, cuebiqid_value, \"before:\", len(before_df), \"after: \", len(after_df))\n",
    "\n",
    "###  SATELLITE MAP CODE ###\n",
    "# Create a folium map centered at the first point of the trajectory\n",
    "avlat = Device_df['lat'].mean()\n",
    "avlng = Device_df['lng'].mean()\n",
    "map_center = [avlat,avlng]\n",
    "\n",
    "# Create a Folium map with a satellite view\n",
    "my_map = folium.Map(location=[after_df['lat'].iloc[0], after_df['lng'].iloc[0]], zoom_start=13, tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google Satellite')\n",
    "\n",
    "# Add TX & Louisinana  station points\n",
    "for index, row in station_points_df.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lng']],\n",
    "        radius=4,  # Adjust the radius as needed\n",
    "        color='green',  # Color of the dot\n",
    "        fill=True,\n",
    "        fill_color='green',  # Color to fill the dot\n",
    "        fill_opacity=1,  # Opacity of the fill\n",
    "    ).add_to(my_map)\n",
    "    \n",
    "# Add MRIP station points\n",
    "for index, row in MRIP_station_points_df.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['SITE_LAT'], row['SITE_LONG']],\n",
    "        radius=4,  # Adjust the radius as needed\n",
    "        color='yellow',  # Color of the dot\n",
    "        fill=True,\n",
    "        fill_color='yellow',  # Color to fill the dot\n",
    "        fill_opacity=1,  # Opacity of the fill\n",
    "    ).add_to(my_map)\n",
    "    \n",
    "# Add CircleMarker for points in before_df\n",
    "for index, row in stops_before_df.iterrows():\n",
    "    location = [row['lat'], row['lng']]\n",
    "    folium.CircleMarker(\n",
    "        location=location,\n",
    "        radius=5,  # Adjust the radius as needed\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.7,\n",
    "    ).add_to(my_map)\n",
    "\n",
    "# Add CircleMarker for points in after_df\n",
    "for index, row in stops_after_df.iterrows():\n",
    "    location = [row['lat'], row['lng']]\n",
    "    folium.CircleMarker(\n",
    "        location=location,\n",
    "        radius=5,  # Adjust the radius as needed\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.7,\n",
    "    ).add_to(my_map)\n",
    "\n",
    "\n",
    "# Add during trip line    \n",
    "if len(DuringTripLine) >0:\n",
    "    folium.PolyLine(\n",
    "        locations=DuringTripLine,\n",
    "        color='pink',\n",
    "        weight=2,\n",
    "        opacity=1,\n",
    "    ).add_to(my_map)\n",
    "\n",
    "# Add the first PolyLine\n",
    "if len(BeforeTripLine) >0:\n",
    "    folium.PolyLine(\n",
    "        locations=BeforeTripLine,\n",
    "        color='blue',\n",
    "        weight=2,\n",
    "        opacity=1,\n",
    "        popup='Before Trip'\n",
    "    ).add_to(my_map)\n",
    "\n",
    "    # Add marker at First point before trip\n",
    "    time_before = (trip_start_epochtime-Device_df['event_timestamp'].iloc[0])/3600\n",
    "    folium.Marker(\n",
    "        location=BeforeTripLine[0],  # First point of BeforeTripLine\n",
    "        icon=folium.Icon(color='blue'),\n",
    "        popup=f'First point {time_before} hours before trip'\n",
    "    ).add_to(my_map)\n",
    "\n",
    "# Add the second PolyLine\n",
    "if len(AfterTripLine)>0:\n",
    "    folium.PolyLine(\n",
    "        locations=AfterTripLine,\n",
    "        color='red',\n",
    "        weight=2,\n",
    "        opacity=1,\n",
    "        popup='After Trip'\n",
    "    ).add_to(my_map)\n",
    "    \n",
    "    timeafter = round((Device_df['event_timestamp'].iloc[-1] - trip_end_epochtime)/3600,1)\n",
    "    # Add marker at last point before trip\n",
    "    folium.Marker(\n",
    "        location=AfterTripLine[-1],  # Last point of AfterTripLine\n",
    "        icon=folium.Icon(color='red'),\n",
    "        popup=f'Last point {timeafter} hours after trip '\n",
    "    ).add_to(my_map)\n",
    "\n",
    "   \n",
    "    \n",
    "# Iterate over each H3 cell and map that cell and its neigbors\n",
    "for h3_cell in combined_h3_cells:\n",
    "    # Get the polygon vertices of the H3 cell\n",
    "    polygon_vertices = h3.h3_to_geo_boundary(h3_cell)\n",
    "\n",
    "    # Convert the tuple to a list of (latitude, longitude) pairs\n",
    "    polygon_vertices_list = [(lat, lng) for lat, lng in polygon_vertices]\n",
    "\n",
    "    # Plot the polygon on the map\n",
    "    folium.Polygon(\n",
    "        locations=polygon_vertices_list,\n",
    "        color='yellow',\n",
    "        fill=True,\n",
    "        fill_color='yellow',\n",
    "        fill_opacity=0.4\n",
    "    ).add_to(my_map)\n",
    "    \n",
    "# Iterate over each H3 cell in the list and map just that cell\n",
    "for h3_cell in combined_h3_cells_alone:\n",
    "    # Get the polygon vertices of the H3 cell\n",
    "    polygon_vertices = h3.h3_to_geo_boundary(h3_cell)\n",
    "\n",
    "    # Convert the tuple to a list of (latitude, longitude) pairs\n",
    "    polygon_vertices_list = [(lat, lng) for lat, lng in polygon_vertices]\n",
    "\n",
    "    # Plot the polygon on the map\n",
    "    folium.Polygon(\n",
    "        locations=polygon_vertices_list,\n",
    "        color='purple',\n",
    "        fill=True,\n",
    "        fill_color='purple',\n",
    "        fill_opacity=0.2\n",
    "    ).add_to(my_map)\n",
    "    \n",
    "\n",
    "# # END OF MAP CODE ###\n",
    "\n",
    "# Define the filename\n",
    "# folder_name = '../RecMapsBeforeAndAfter'\n",
    "\n",
    "# html_filename = os.path.join(folder_name, f'TESTRecMapOfPathsBeforeAndAfter_{index}.html')\n",
    "\n",
    "# Save and display the map\n",
    "# my_map.save(html_filename)\n",
    "my_map\n",
    "# IFrame(src=html_filename, width=700, height=600)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a111abca-7945-473f-94b6-a1474fa3db97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Loop over multiple trips to look generate\n",
    "#### This includes perturbations so that it can be exported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4009b-a93b-459c-b9a8-9fbb184d7a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stations\n",
    "station_points_df = pd.read_csv(station_points_filename)\n",
    "MRIP_station_points_df = pd.read_csv(MRIP_station_points_filename)\n",
    "\n",
    "for indicator_index in range(803, 850):\n",
    "\n",
    "    # Grab the TripToLookAt row from rec_indicators_df\n",
    "    trip_row = rec_indicators_df.iloc[indicator_index]\n",
    "\n",
    "    # EXTRACT THE ROWS FROM THE DEVICE TABLE DATA SET THAT ARE 8 HOURS ON EITHER SIDE OF THE TRIP\n",
    "    cuebiqid_value = trip_row['cuebiq_id']\n",
    "    trip_start_epochtime = trip_row['timestamp_start_t']\n",
    "    trip_end_epochtime = trip_row['timestamp_end_t']\n",
    "    prob = trip_row['max_prob']\n",
    "    vt  = trip_row['Predicted_Class']\n",
    "    \n",
    "    # Conversion factors\n",
    "    Eight_hours = 8*60*60\n",
    "    km_per_min_To_mph = 37.2823\n",
    "    mph_To_km_per_min = 1/37.2823\n",
    "\n",
    "    ######### First select the relevant pings during the trip\n",
    "    Gulf_Pings_df = Pings_OurTable_Gulf_df[(Pings_OurTable_Gulf_df['cuebiq_id'] == cuebiqid_value) & \n",
    "                                          (Pings_OurTable_Gulf_df['event_timestamp'] >= trip_start_epochtime) &\n",
    "                                          (Pings_OurTable_Gulf_df['event_timestamp'] <= trip_end_epochtime)]\n",
    "    Gulf_Pings_df = EliminateErrantPingsSpeed(Gulf_Pings_df, 60)                                                             \n",
    "\n",
    "    ########### Create a data frame for points before and after the trip #######\n",
    "    # Choose the right cuebiq_id\n",
    "    Device_df = V3_Pings_df[(V3_Pings_df['cuebiq_id'] == cuebiqid_value)]\n",
    "    # Eliminate inaccuate pings\n",
    "    # Filter to only pings within 8 hours of the start or end of the trip\n",
    "    Device_df = Device_df[(Device_df['event_timestamp'] >= (trip_start_epochtime-Eight_hours))] \n",
    "    Device_df = Device_df[(Device_df['event_timestamp'] <= (trip_end_epochtime+Eight_hours))] \n",
    "    # Filter out pings that occurred during the trip\n",
    "    Device_df = Device_df[~((Device_df['event_timestamp'] > trip_start_epochtime) & (Device_df['event_timestamp'] < trip_end_epochtime))]\n",
    "\n",
    "    Device_df = Device_df.sort_values(by='event_timestamp')\n",
    "\n",
    "    #################################### Perturbations ###########################################\n",
    "    # Perturb the latitud and Longitude by approximately +/- 1/4 of the width of a H3 cell\n",
    "    # Side to side distance of an H3 cell longitude: 0.001425451\n",
    "    # Top to bottom distance on an H3 cell in latitude: 0.001355713\n",
    "    lat_perturbation = 0.001355713/4  \n",
    "    lng_perturbation = 0.001425451/4  \n",
    "\n",
    "    Device_df['lat'] += np.random.uniform(-lat_perturbation, lat_perturbation, size=len(Device_df))\n",
    "    Device_df['lng'] += np.random.uniform(-lng_perturbation, lng_perturbation, size=len(Device_df))\n",
    "    Gulf_Pings_df['lat'] += np.random.uniform(-lat_perturbation, lat_perturbation, size=len(Gulf_Pings_df))\n",
    "    Gulf_Pings_df['lng'] += np.random.uniform(-lng_perturbation, lng_perturbation, size=len(Gulf_Pings_df))\n",
    "\n",
    "    \n",
    "    DuringTripLine = Gulf_Pings_df[['lat', 'lng']].values.tolist()\n",
    "\n",
    "    \n",
    "    ##  FIRST PLL THE LINSE FROM THE DEVICE TABLE BEFORE AND AFTER\n",
    "    # Prepare before and after strings of points, eliminating errant pings\n",
    "    before_df=[]\n",
    "    before_df= Device_df[Device_df['event_timestamp'] <= trip_start_epochtime]\n",
    "    before_df = EliminateErrantPingsSpeed(before_df, 90)\n",
    "\n",
    "    if len(before_df) > 1:\n",
    "        BeforeTripLine = before_df[['lat', 'lng']].values.tolist()\n",
    "        calculate_speed(before_df)\n",
    "        stops_before_df = before_df[before_df['ping_speed_fwd'] < mph_To_km_per_min].copy()\n",
    "\n",
    "    after_df = []\n",
    "    after_df = Device_df[Device_df['event_timestamp'] >= trip_end_epochtime]\n",
    "    after_df = EliminateErrantPingsSpeed(after_df, 90)\n",
    "\n",
    "    if len(after_df)>1:\n",
    "        AfterTripLine = after_df[['lat', 'lng']].values.tolist()\n",
    "        calculate_speed(after_df)\n",
    "        stops_after_df = after_df[after_df['ping_speed_fwd'] < mph_To_km_per_min].copy()\n",
    "\n",
    "    print(indicator_index, cuebiqid_value, \"before:\", len(before_df), \"after: \", len(after_df))\n",
    "\n",
    "    # Create map only if there's enough pings to make it interesting\n",
    "    if (len(before_df) > 10) & (len(after_df) > 1): \n",
    "        ###  SATELLITE MAP CODE ###\n",
    "        # Create a folium map centered at the first point of the trajectory\n",
    "        avlat = Device_df['lat'].mean()\n",
    "        avlng = Device_df['lng'].mean()\n",
    "        map_center = [avlat,avlng]\n",
    "\n",
    "        # Create a Folium map with a satellite view\n",
    "        my_map = folium.Map(location=[avlat, avlng], zoom_start=13, tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google Satellite')\n",
    "\n",
    "        # Add TX & Louisinana  station points\n",
    "        for index, row in station_points_df.iterrows():\n",
    "            folium.CircleMarker(\n",
    "                location=[row['lat'], row['lng']],\n",
    "                radius=4,  # Adjust the radius as needed\n",
    "                color='green',  # Color of the dot\n",
    "                fill=True,\n",
    "                fill_color='green',  # Color to fill the dot\n",
    "                fill_opacity=1,  # Opacity of the fill\n",
    "            ).add_to(my_map)\n",
    "\n",
    "        # Add MRIP station points\n",
    "        for index, row in MRIP_station_points_df.iterrows():\n",
    "            folium.CircleMarker(\n",
    "                location=[row['SITE_LAT'], row['SITE_LONG']],\n",
    "                radius=4,  # Adjust the radius as needed\n",
    "                color='yellow',  # Color of the dot\n",
    "                fill=True,\n",
    "                fill_color='yellow',  # Color to fill the dot\n",
    "                fill_opacity=1,  # Opacity of the fill\n",
    "            ).add_to(my_map)\n",
    "\n",
    "        # Add CircleMarker for points in before_df\n",
    "        for index, row in stops_before_df.iterrows():\n",
    "            location = [row['lat'], row['lng']]\n",
    "            folium.CircleMarker(\n",
    "                location=location,\n",
    "                radius=5,  # Adjust the radius as needed\n",
    "                color='blue',\n",
    "                fill=True,\n",
    "                fill_color='blue',\n",
    "                fill_opacity=0.7,\n",
    "            ).add_to(my_map)\n",
    "            \n",
    "        # Add during trip line    \n",
    "        if len(DuringTripLine) >0:\n",
    "            folium.PolyLine(\n",
    "                locations=DuringTripLine,\n",
    "                color='pink',\n",
    "                weight=2,\n",
    "                opacity=1,\n",
    "            ).add_to(my_map)\n",
    "\n",
    "        # Add CircleMarker for points in after_df\n",
    "        for index, row in stops_after_df.iterrows():\n",
    "            location = [row['lat'], row['lng']]\n",
    "            folium.CircleMarker(\n",
    "                location=location,\n",
    "                radius=5,  # Adjust the radius as needed\n",
    "                color='red',\n",
    "                fill=True,\n",
    "                fill_color='red',\n",
    "                fill_opacity=0.7,\n",
    "            ).add_to(my_map)\n",
    "\n",
    "\n",
    "        # Add the first PolyLine\n",
    "        if len(BeforeTripLine) >0:\n",
    "            folium.PolyLine(\n",
    "                locations=BeforeTripLine,\n",
    "                color='blue',\n",
    "                weight=2,\n",
    "                opacity=1,\n",
    "                popup='Before Trip'\n",
    "            ).add_to(my_map)\n",
    "\n",
    "            # Add marker at First point before trip\n",
    "            time_before = round((trip_start_epochtime-Device_df['event_timestamp'].iloc[0])/3600,1)\n",
    "            folium.Marker(\n",
    "                location=BeforeTripLine[0],  # First point of BeforeTripLine\n",
    "                icon=folium.Icon(color='blue'),\n",
    "                popup=f'{time_before} hours before trip. VT {vt} with P( {round(prob,2)}'\n",
    "            ).add_to(my_map)\n",
    "\n",
    "        # Add the second PolyLine\n",
    "        if len(AfterTripLine)>0:\n",
    "            folium.PolyLine(\n",
    "                locations=AfterTripLine,\n",
    "                color='red',\n",
    "                weight=2,\n",
    "                opacity=1,\n",
    "                popup='After Trip'\n",
    "            ).add_to(my_map)\n",
    "\n",
    "            timeafter = round((Device_df['event_timestamp'].iloc[-1] - trip_end_epochtime)/3600,1)\n",
    "            # Add marker at last point before trip\n",
    "            folium.Marker(\n",
    "                location=AfterTripLine[-1],  # Last point of AfterTripLine\n",
    "                icon=folium.Icon(color='red'),\n",
    "                popup=f'{timeafter} hours after trip. VT {vt} with P( {round(prob,2)}'\n",
    "            ).add_to(my_map)\n",
    "\n",
    "\n",
    "\n",
    "        # Iterate over each H3 cell and map that cell and its neigbors\n",
    "        for h3_cell in combined_h3_cells:\n",
    "            # Get the polygon vertices of the H3 cell\n",
    "            polygon_vertices = h3.h3_to_geo_boundary(h3_cell)\n",
    "\n",
    "            # Convert the tuple to a list of (latitude, longitude) pairs\n",
    "            polygon_vertices_list = [(lat, lng) for lat, lng in polygon_vertices]\n",
    "\n",
    "            # Plot the polygon on the map\n",
    "            folium.Polygon(\n",
    "                locations=polygon_vertices_list,\n",
    "                color='yellow',\n",
    "                fill=True,\n",
    "                fill_color='yellow',\n",
    "                fill_opacity=0.4\n",
    "            ).add_to(my_map)\n",
    "\n",
    "        # Iterate over each H3 cell in the list and map just that cell\n",
    "        for h3_cell in combined_h3_cells_alone:\n",
    "            # Get the polygon vertices of the H3 cell\n",
    "            polygon_vertices = h3.h3_to_geo_boundary(h3_cell)\n",
    "\n",
    "            # Convert the tuple to a list of (latitude, longitude) pairs\n",
    "            polygon_vertices_list = [(lat, lng) for lat, lng in polygon_vertices]\n",
    "\n",
    "            # Plot the polygon on the map\n",
    "            folium.Polygon(\n",
    "                locations=polygon_vertices_list,\n",
    "                color='purple',\n",
    "                fill=True,\n",
    "                fill_color='purple',\n",
    "                fill_opacity=0.2\n",
    "            ).add_to(my_map)\n",
    "\n",
    "\n",
    "        # # END OF MAP CODE ###\n",
    "\n",
    "        # Define the filename\n",
    "        html_filename = os.path.join(TrajectoryMaps_directory, f'MapOfPathsBeforeAndAfter_{indicator_index}.html')\n",
    "\n",
    "        # Save and display the map\n",
    "        my_map.save(html_filename)\n",
    "\n",
    "my_map  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b790b66-4501-42ce-81e9-9e1247ba642c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Zip the maps to a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df7be8-9627-4c0f-9935-67e307a5532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "os.chdir(TrajectoryMaps_directory)\n",
    "\n",
    "\n",
    "# Directory containing the files to be zipped\n",
    "# directory = './MapsForExporting'  # Change this to the desired directory\n",
    "# rootdir = '.'\n",
    "\n",
    "# Prefix for the files you want to include in the zip archive\n",
    "# file_prefix = 'SatRecMapOfPathsBeforeAndAfter_'\n",
    "file_prefix = 'MapOfPathsBeforeAndAfter_'\n",
    "\n",
    "# Name of the zip file\n",
    "zip_filename = 'PerturbedMapsBeforeAndAfter.zip'\n",
    "\n",
    "# Get a list of files that match the specified prefix\n",
    "files_to_zip = [filename for filename in os.listdir(TrajectoryMaps_directory) if filename.startswith(file_prefix)]\n",
    "\n",
    "# Create a zip file and add the matching files to it\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zip_file:\n",
    "    for file in files_to_zip:\n",
    "        file_path = os.path.join(TrajectoryMaps_directory, file)\n",
    "        zip_file.write(file_path, arcname=os.path.basename(file_path))\n",
    "\n",
    "print(f'Successfully created {zip_filename} containing {len(files_to_zip)} files.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82880557-8580-4acd-8196-7c270b367ebc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Identify where vessel has \"stopped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c08db-7ae5-49ea-b5bf-a97164cd61ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_df = pd.read_csv(Combined_indicators_with_disappearance_filename)\n",
    "\n",
    "# Variables used for all trips\n",
    "Eight_hours = 8 * 60 * 60\n",
    "One_mph = 0.0268224   # Speeds are calculated in km/min. 1 mph = 0.0268224 km/min\n",
    "\n",
    "# Check whether the output file already exists\n",
    "if os.path.isfile(Station_NonStationAnalysis_filename):\n",
    "    df = pd.read_csv(Station_NonStationAnalysis_filename)\n",
    "    FirstWrite = False\n",
    "    start_row = df['num'].max()+1\n",
    "else:\n",
    "    FirstWrite = True\n",
    "    start_row = 1\n",
    "    \n",
    "# Run through all rows of the Rec_indicators\n",
    "end_row = len(indicators_df)\n",
    "\n",
    "# Loop over the specified range of rows\n",
    "irow = start_row\n",
    "print(\"irow\", irow)\n",
    "# for index, row in tqdm(TC_trips_1_df.iterrows(), total=len(TC_trips_1_df)):\n",
    "\n",
    "for index, row in tqdm(indicators_df.iloc[start_row-1:end_row].iterrows(), total=end_row-start_row):\n",
    "    # print(\"irow\", irow)\n",
    "    intersection_results_df = []\n",
    "    id_value = row['cuebiq_id']\n",
    "    Trip_number = row['Trip_number']\n",
    "    Trip_Start = row['timestamp_start_t']\n",
    "    Trip_End = row['timestamp_end_t']\n",
    "    Interruption_01 = row['Interruption_01']\n",
    "    Avg_Interruption_01 = row['Avg_Interruption_01']\n",
    "\n",
    "    # Get pings before trip\n",
    "    this_trip_before_df = V3_Pings_df[\n",
    "        (V3_Pings_df['cuebiq_id'] == id_value) &\n",
    "        (V3_Pings_df['event_timestamp'] >= (Trip_Start-Eight_hours)) &\n",
    "        (V3_Pings_df['event_timestamp'] <= Trip_Start) ]\n",
    "    this_trip_before_df=EliminateErrantPingsSpeed(this_trip_before_df, 90)\n",
    "    \n",
    "    # Get pings after trip\n",
    "    this_trip_after_df = V3_Pings_df[\n",
    "        (V3_Pings_df['cuebiq_id'] == id_value) &\n",
    "        (V3_Pings_df['event_timestamp'] <= (Trip_End+Eight_hours)) &\n",
    "        (V3_Pings_df['event_timestamp'] >= Trip_End)     ]\n",
    "    this_trip_after_df=EliminateErrantPingsSpeed(this_trip_after_df, 90)\n",
    "    \n",
    "    # Concatenate before and after \n",
    "    before_and_after_df = pd.concat([this_trip_before_df, this_trip_after_df], ignore_index=True)\n",
    "    nPings = len(before_and_after_df)\n",
    "    nStops = 0\n",
    "    # Set key results to zero if no before or after pings are found. This should never happen\n",
    "    if len(before_and_after_df)==0:\n",
    "        intersection_results_df = pd.DataFrame({\n",
    "            'num': [irow],\n",
    "            'cuebiq_id': [id_value],\n",
    "            'Trip_number': [Trip_number],\n",
    "            'Interruption_01': [Interruption_01],\n",
    "            'Avg_Interruption_01': [Avg_Interruption_01],\n",
    "            'nPings': [nPings],\n",
    "            'nStops': [nStops]\n",
    "        })\n",
    "        print(\"Something is wrong because no before or after pings were found\")\n",
    "        continue            \n",
    "\n",
    "    ##################################################################################\n",
    "    # Find out what state we're in and save those indicators\n",
    "    avg_lng = before_and_after_df['lng'].mean()\n",
    "    avg_lat = before_and_after_df['lat'].mean()\n",
    "    avg_point = Point(avg_lng,avg_lat)\n",
    "    # \n",
    "    TX_within_polygon = avg_point.within(TX_polygon)\n",
    "    LA_within_polygon = avg_point.within(LA_polygon)\n",
    "    MS_within_polygon = avg_point.within(MS_polygon)\n",
    "    AL_within_polygon = avg_point.within(AL_polygon)\n",
    "    FL_within_polygon = avg_point.within(FL_polygon)\n",
    "\n",
    "    # # Initialize variables\n",
    "    TX = LA = MS = AL = FL = 0\n",
    "    if TX_within_polygon:\n",
    "        TX = 1\n",
    "    elif LA_within_polygon:\n",
    "        LA = 1\n",
    "    elif MS_within_polygon:\n",
    "        MS = 1\n",
    "    elif AL_within_polygon:\n",
    "        AL = 1\n",
    "    elif FL_within_polygon:\n",
    "        FL = 1\n",
    "\n",
    "\n",
    "    ##################################################################################\n",
    "    # Restrict the results to only points where the device is going less that one mph\n",
    "    before_and_after_df = before_and_after_df[(before_and_after_df['Avg_ping_speed'] <= One_mph)]\n",
    "    nStops = len(before_and_after_df)\n",
    "\n",
    "    # Get h3 cells of all points where the device is \"stopped\"\n",
    "    before_and_after_h3_cells = get_h3_cells_for_dataframe(before_and_after_df, resolution)\n",
    "\n",
    "    # Find all interesections\n",
    "    LA_CREEL_station_points_intersection = list(set(before_and_after_h3_cells).intersection(LA_CREEL_station_points_h3_cells))\n",
    "    FL_MRIP_station_points_intersection = list(set(before_and_after_h3_cells).intersection(FL_MRIP_station_points_h3_cells))\n",
    "    AL_MRIP_station_points_intersection = list(set(before_and_after_h3_cells).intersection(AL_MRIP_station_points_h3_cells))\n",
    "    MS_MRIP_station_points_intersection = list(set(before_and_after_h3_cells).intersection(MS_MRIP_station_points_h3_cells))\n",
    "    TX_station_points_intersection = list(set(before_and_after_h3_cells).intersection(TX_station_points_h3_cells))\n",
    "    LargePorts_Marine_traffic_intersection = list(set(before_and_after_h3_cells).intersection(LargePorts_Marine_traffic_h3_cells))\n",
    "    Medium_Anchorage_Marine_traffic_intersection = list(set(before_and_after_h3_cells).intersection(Medium_Anchorage_Marine_traffic_h3_cells))\n",
    "    Medium_Port_Marine_traffic_intersection = list(set(before_and_after_h3_cells).intersection(Medium_Port_Marine_traffic_h3_cells))\n",
    "    Medium_Port_Marine_traffic_intersection = list(set(before_and_after_h3_cells).intersection(Medium_Port_Marine_traffic_h3_cells))\n",
    "    Small_Marina_Marine_traffic_intersection = list(set(before_and_after_h3_cells).intersection(Small_Marina_Marine_traffic_h3_cells))\n",
    "    Small_Port_Marine_traffic_intersection = list(set(before_and_after_h3_cells).intersection(Small_Port_Marine_traffic_h3_cells))\n",
    "\n",
    "\n",
    "    intersection_results_df = pd.DataFrame({\n",
    "    'num': [irow],\n",
    "    'cuebiq_id': [id_value],\n",
    "    'Trip_number': [Trip_number],\n",
    "    'Interruption_01': [Interruption_01],\n",
    "    'Avg_Interruption_01': [Avg_Interruption_01],\n",
    "    'nPings': [nPings],\n",
    "    'nStops': [nStops],\n",
    "    'TX': [TX],\n",
    "    'LA': [LA],\n",
    "    'MS': [MS],\n",
    "    'AL': [AL],\n",
    "    'FL': [FL],\n",
    "    'LA_CREEL_station_points_0_1': [1*(len(LA_CREEL_station_points_intersection)>0)],\n",
    "    'FL_MRIP_station_points_0_1': [1*(len(FL_MRIP_station_points_intersection)>0)],\n",
    "    'AL_MRIP_station_points_0_1': [1*(len(AL_MRIP_station_points_intersection)>0)],\n",
    "    'MS_MRIP_station_points_0_1': [1*(len(MS_MRIP_station_points_intersection)>0)],\n",
    "    'TX_station_points_0_1': [1*(len(TX_station_points_intersection)>0)],\n",
    "    'LargePorts_Marine_traffic_0_1': [1*(len(LargePorts_Marine_traffic_intersection)>0)],\n",
    "    'Medium_Anchorage_Marine_traffic_0_1': [1*(len(Medium_Anchorage_Marine_traffic_intersection)>0)],\n",
    "    'Medium_Port_Marine_traffic_0_1': [1*(len(Medium_Port_Marine_traffic_intersection)>0)],\n",
    "    'Medium_Port_Marine_traffic_0_1': [1*(len(Medium_Port_Marine_traffic_intersection)>0)],\n",
    "    'Small_Marina_Marine_traffic_0_1': [1*(len(Small_Marina_Marine_traffic_intersection)>0)],\n",
    "    'Small_Port_Marine_traffic_0_1': [1*(len(Small_Port_Marine_traffic_intersection)>0)]\n",
    "    })\n",
    "\n",
    "    intersection_results_df.to_csv(Station_NonStationAnalysis_filename, mode='a', header=not os.path.exists(Station_NonStationAnalysis_filename), index=False) \n",
    "    \n",
    "    current_time_utc = datetime.utcnow()\n",
    "    central_timezone = pytz.timezone('US/Central')\n",
    "    current_time_central = current_time_utc.replace(tzinfo=pytz.utc).astimezone(central_timezone)\n",
    "    formatted_time = current_time_central.strftime(\"%H:%M:%S\")\n",
    "    # print(formatted_time, \"Row\", irow, \"In FL\", FL)\n",
    "    irow = irow +1\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948634b-3839-4a93-8d60-0a6f843e91a7",
   "metadata": {},
   "source": [
    "### Combine the indicators file with the data frame that identifies whether the vessel has stopped near a station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a918bd-ec5e-4f82-b47a-5cddca0389ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_indicators_with_disappearance__df = pd.read_csv(Combined_indicators_with_disappearance_filename)\n",
    "DisappearanceIndicators_df = pd.read_csv(DisappearanceIndicators_filename)\n",
    "print(len(Combined_indicators_with_disappearance__df), len(DisappearanceIndicators_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d56cf0-02bf-4d4c-9f09-8fa8374eee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns of both DataFrames\n",
    "combined_columns = set(Combined_indicators_with_disappearance__df.columns)\n",
    "disappearance_columns = set(DisappearanceIndicators_df.columns)\n",
    "\n",
    "# Find common columns\n",
    "common_columns = combined_columns.intersection(disappearance_columns)\n",
    "print(f\"Common columns: {common_columns}\")\n",
    "\n",
    "# Find columns in Combined_indicators_with_disappearance__df but not in DisappearanceIndicators_df\n",
    "only_in_combined = combined_columns - disappearance_columns\n",
    "print(\" \")\n",
    "print(f\"Columns only in Combined_indicators_with_disappearance__df: {only_in_combined}\")\n",
    "\n",
    "# Find columns in DisappearanceIndicators_df but not in Combined_indicators_with_disappearance__df\n",
    "only_in_disappearance = disappearance_columns - combined_columns\n",
    "print(\" \")\n",
    "print(f\"Columns only in DisappearanceIndicators_df: {only_in_disappearance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d6275-2e60-4651-b650-3741e92262d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file names\n",
    "# indicators_and_stops_filename = 'RecTripIndicators_WithStationStops.csv'  \n",
    "\n",
    "# Load the files\n",
    "intersection_results_df= pd.read_csv(Station_NonStationAnalysis_filename)\n",
    "indicators_df = pd.read_csv(Combined_indicators_with_disappearance_filename)\n",
    "\n",
    "# Merge in the stops\n",
    "indicators_and_stops_df = intersection_results_df.merge(indicators_df, how='left', \n",
    "                                              on=['cuebiq_id', 'Trip_number'], suffixes=('', '_y'))\n",
    "indicators_and_stops_df = indicators_and_stops_df.drop(indicators_and_stops_df.filter(like='_y'), axis=1)\n",
    "\n",
    "\n",
    "indicators_and_stops_df.head(5)\n",
    "\n",
    "# Save to a complete data frame\n",
    "indicators_and_stops_df.to_csv(Station_NonStationAnalysis_full_filename, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085aa9fa-2904-4518-a1c5-19e3379c8197",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Analysis of station and non-station trip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08969797-21f7-453c-864b-4f1e7d252a15",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A. Summary of station and non-station trips by state: launch_sites.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c744c0-7462-4411-9ca4-f56fb94d1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "LaunchSites_filename = Station_NonStationAnalysis_filename  #'launch_sites.csv'\n",
    "# Station_NonStationAnalysis_full_filename  = os.path.join(Results_directory,'Station_NonStationAnalysis_full.csv')\n",
    "count_filename = os.path.join(Results_directory,'SiteCountByState_nStops.csv')\n",
    "percent_filename = os.path.join(Results_directory, 'PercentMonitoredByState_nStops.csv')\n",
    "summary_percent_filename= os.path.join(Results_directory,'SummaryPercentMonitoredByState_nStops.csv')\n",
    "DiffOfMeansResults_filename= os.path.join(Results_directory,'StationVsNon_comparison_of_means.csv')\n",
    "DiffOfMeansResults_2_filename= os.path.join(Results_directory,'StationVsNon_comparison_of_means_3-1-25.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96b41a-5f3a-4596-86aa-876910f98d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# ###### DEBUGGING ########\n",
    "# ###### DEBUGGING ########\n",
    "# ###### DEBUGGING ########\n",
    "# print(LaunchSites_filename)\n",
    "# LaunchSites_filename_df = pd.read_csv(LaunchSites_filename)\n",
    "# print(\" len LaunchSites_filename_df \", len(LaunchSites_filename_df))\n",
    "# Station_NonStationAnalysis_full_filename_df = pd.read_csv(Station_NonStationAnalysis_full_filename)\n",
    "# print(\"len Station_NonStationAnalysis_full_filename_df\", len(Station_NonStationAnalysis_full_filename_df))\n",
    "# print(len(Station_NonStationAnalysis_full_filename_df) - Station_NonStationAnalysis_full_filename_df['Interruption_01'].sum())\n",
    "# print(len(LaunchSites_filename_df))\n",
    "\n",
    "# filter1 = Station_NonStationAnalysis_full_filename_df[Station_NonStationAnalysis_full_filename_df['nStops']>=10]\n",
    "# filter2 = filter1[filter1['Interruption_01'] == 1]\n",
    "# print(len(filter1))\n",
    "# print(filter1['Interruption_01'].sum())\n",
    "\n",
    "# # print(LaunchSites_filename_df.columns)\n",
    "# # for col in Station_NonStationAnalysis_full_filename_df.columns:\n",
    "# #     print(col)\n",
    "\n",
    "# print(LaunchSites_filename_df['TX_station_points_0_1'].sum(),Station_NonStationAnalysis_full_filename_df['TX_station_points_0_1'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e55c6a-d187-426e-b83e-9a927e0e2f37",
   "metadata": {},
   "source": [
    "#### Load the indicators file and create a new variable, StopAtStation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab88a1c6-259b-4d2f-9bdf-81f82d6eb523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicators and stops file name\n",
    "\n",
    "indicators_and_stops_df = pd.read_csv(Station_NonStationAnalysis_full_filename)\n",
    "\n",
    "# List of columns to sum\n",
    "AllStationVariablesColumns = ['LA_CREEL_station_points_0_1', 'FL_MRIP_station_points_0_1', 'AL_MRIP_station_points_0_1', 'MS_MRIP_station_points_0_1', 'TX_station_points_0_1']\n",
    "\n",
    "# Summing the values in the specified columns\n",
    "indicators_and_stops_df['StopAtStation'] = (indicators_and_stops_df[AllStationVariablesColumns].sum(axis=1) > 0).astype(int)\n",
    "\n",
    "indicators_and_stops_df.to_csv(Station_NonStationAnalysis_full_filename, index = False)\n",
    "\n",
    "\n",
    "### ADD THE NEW SPEED VARIABLES #####\n",
    "Combined_indicators_complete_with_New_speeds_filename = os.path.join(OurTable_V3_directory,'Combined_indicators_complete_with_New_speeds_All.csv')\n",
    "# Indicators_with_Disappearance_df = pd.read_csv(Combined_indicators_with_disappearance_filename)\n",
    "newspeed_df = pd.read_csv(Combined_indicators_complete_with_New_speeds_filename)\n",
    "indicators_and_stops_df = Merge_Trip_dfs(newspeed_df, indicators_and_stops_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab8e83-c379-431a-82dd-1573e7bcad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Lists of Variagles\n",
    "station_list = ['LA_CREEL_station_points_0_1','FL_MRIP_station_points_0_1','AL_MRIP_station_points_0_1','MS_MRIP_station_points_0_1','TX_station_points_0_1']\n",
    "states_to_sum = ['TX', 'LA', 'MS', 'AL', 'FL']\n",
    "sites_to_sum = ['LA_CREEL_station_points_0_1', 'FL_MRIP_station_points_0_1',\n",
    "                'AL_MRIP_station_points_0_1', 'MS_MRIP_station_points_0_1',\n",
    "                'TX_station_points_0_1', 'LargePorts_Marine_traffic_0_1',\n",
    "                'Medium_Anchorage_Marine_traffic_0_1', 'Medium_Port_Marine_traffic_0_1',\n",
    "                'Small_Marina_Marine_traffic_0_1', 'Small_Port_Marine_traffic_0_1']\n",
    "\n",
    "# Initialize dfs fro the results\n",
    "all_results_dfs = []\n",
    "all_normalized_dfs = []\n",
    "all_PercentStation_dfs = []\n",
    "\n",
    "\n",
    "# Delete the csv file that stores the difference-of-means results\n",
    "if os.path.exists(DiffOfMeansResults_2_filename):\n",
    "    os.remove(DiffOfMeansResults_2_filename)\n",
    "\n",
    "nStops = 0\n",
    "FirstWrite = True\n",
    "for nStops in range(31):\n",
    "    print(\"nStops:\", nStops)\n",
    "    for iOnlyCompleteTrips in [0, 1]:\n",
    "        # Define name for data frame for this value of nStops\n",
    "        result_df_name = f\"result_{nStops:02d}_df\"\n",
    "        locals()[result_df_name] = pd.DataFrame(index=sites_to_sum, columns=states_to_sum)\n",
    "\n",
    "        normalized_df_name = f\"normalized_{nStops:02d}_df\"\n",
    "        locals()[normalized_df_name] = pd.DataFrame(index=sites_to_sum, columns=states_to_sum)\n",
    "\n",
    "        Sum_PercentStation_name = f\"all_stations_normalized_{nStops:02d}_df\"\n",
    "        locals()[Sum_PercentStation_name] = pd.DataFrame(index=sites_to_sum, columns=states_to_sum)\n",
    "\n",
    "        \n",
    "        # LaunchSites_df = pd.read_csv(LaunchSites_filename)\n",
    "        LaunchSites_df = pd.read_csv(Station_NonStationAnalysis_full_filename)\n",
    "\n",
    "\n",
    "        LaunchSites_df.fillna(0, inplace=True)\n",
    "        # Create Variable indicating if the site is not counted\n",
    "        LaunchSites_df['InsufficientStops'] = (LaunchSites_df['nStops'] < nStops).astype(int)\n",
    "\n",
    "        LaunchSites_df[states_to_sum] = LaunchSites_df[states_to_sum].astype(int)\n",
    "        LaunchSites_df[sites_to_sum] = LaunchSites_df[sites_to_sum].astype(int)\n",
    "\n",
    "\n",
    "        # Replace missing values with zeros\n",
    "        # MissingRow = LaunchSites_df['TX'].isna().sum()\n",
    "\n",
    "        MissingRows = LaunchSites_df['InsufficientStops'].sum()\n",
    "\n",
    "        # Create a data frame for this value of nStops\n",
    "        LaunchSites_df_counted = LaunchSites_df[(LaunchSites_df['nStops'] > nStops) & \n",
    "                                                (LaunchSites_df['Interruption_01'] >= iOnlyCompleteTrips) ]\n",
    "        GrandTotal = len(LaunchSites_df_counted)\n",
    "        # print(nStops, GrandTotal, MissingRows, GrandTotal+MissingRows)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the results\n",
    "        locals()[result_df_name] = pd.DataFrame(index=sites_to_sum, columns=states_to_sum)\n",
    "\n",
    "        # Fill in the DataFrame with the corresponding values\n",
    "        for state_column in states_to_sum:\n",
    "            for site_column in sites_to_sum:\n",
    "                locals()[result_df_name].at[site_column, 'SiteName'] = site_column\n",
    "                locals()[result_df_name].at[site_column, state_column] = np.dot(LaunchSites_df_counted[state_column], LaunchSites_df_counted[site_column])\n",
    "\n",
    "        results_temp_df = locals()[result_df_name]\n",
    "        # Add a final row for the sum of all variables in states_to_sum\n",
    "        locals()[result_df_name].loc['StateTotal'] = LaunchSites_df_counted[states_to_sum].sum()\n",
    "\n",
    "        # Add a final column for the sum of all values in sites_to_sum\n",
    "        locals()[result_df_name]['AllStates'] = LaunchSites_df_counted[sites_to_sum].sum()\n",
    "        locals()[result_df_name].at['StateTotal','AllStates'] = float(GrandTotal)\n",
    "\n",
    "        ########################################\n",
    "        # Create a data frame with the percentages\n",
    "        # Replace 'SiteName' with the actual name of the column you want to move to the first position\n",
    "        column_to_move = 'SiteName'\n",
    "        site_name_column = locals()[result_df_name][column_to_move]\n",
    "        # Normalize the DataFrame\n",
    "        locals()[normalized_df_name] = locals()[result_df_name].div(locals()[result_df_name].loc['StateTotal'])\n",
    "        # Add the 'SiteName' column back to the normalized DataFrame\n",
    "        locals()[normalized_df_name][column_to_move] = site_name_column\n",
    "\n",
    "        # Move the specified column to the first position\n",
    "        columns = list(locals()[normalized_df_name].columns)\n",
    "        columns.insert(0, columns.pop(columns.index(column_to_move)))\n",
    "        locals()[normalized_df_name] = locals()[normalized_df_name][columns]\n",
    "\n",
    "        rows_to_drop = [\n",
    "            'LargePorts_Marine_traffic_0_1',\n",
    "            'Medium_Anchorage_Marine_traffic_0_1',\n",
    "            'Medium_Port_Marine_traffic_0_1',\n",
    "            'Small_Marina_Marine_traffic_0_1',\n",
    "            'Small_Port_Marine_traffic_0_1'\n",
    "        ]\n",
    "        locals()[normalized_df_name].loc['StateTotal'] = locals()[result_df_name].loc['StateTotal']\n",
    "        locals()[normalized_df_name].at['StateTotal','AllStates'] = float(GrandTotal)\n",
    "\n",
    "        # Drop the specified rows\n",
    "        locals()[normalized_df_name] = locals()[normalized_df_name].drop(rows_to_drop)\n",
    "        stations_to_sum = [\n",
    "            'LA_CREEL_station_points_0_1',\n",
    "            'FL_MRIP_station_points_0_1',\n",
    "            'AL_MRIP_station_points_0_1',\n",
    "            'MS_MRIP_station_points_0_1',\n",
    "            'TX_station_points_0_1'\n",
    "        ]\n",
    "\n",
    "        # Create the new \"AllStations\" row by summing the specified rows\n",
    "        locals()[normalized_df_name].loc['AllStations'] = locals()[normalized_df_name].loc[stations_to_sum].sum()\n",
    "        locals()[normalized_df_name].loc['AllStations','SiteName'] = \"Total\"\n",
    "\n",
    "        # Add a row that shows the value of nStops\n",
    "        locals()[result_df_name]['nStops'] = nStops\n",
    "        locals()[normalized_df_name]['nStops'] = nStops\n",
    "        locals()[result_df_name]['Interruption_01'] = iOnlyCompleteTrips\n",
    "        locals()[normalized_df_name]['Interruption_01'] = iOnlyCompleteTrips\n",
    "\n",
    "        # Create a new data frame that only includes the last row of the normalized data frame\n",
    "        locals()[Sum_PercentStation_name] = locals()[normalized_df_name].loc[['AllStations']]\n",
    "        locals()[Sum_PercentStation_name]['TripsAnalyzed'] = GrandTotal\n",
    "\n",
    "        # print(nStops,locals()[normalized_df_name])\n",
    "\n",
    "\n",
    "        # Reorder columns of results\n",
    "        column_to_move = 'SiteName'\n",
    "        columns = list(locals()[result_df_name].columns)\n",
    "        columns.insert(0, columns.pop(columns.index(column_to_move)))\n",
    "        locals()[result_df_name] = locals()[result_df_name][columns]\n",
    "\n",
    "        columns = list(locals()[result_df_name].columns)\n",
    "        columns.insert(0, columns.pop(columns.index(column_to_move)))\n",
    "        locals()[result_df_name] = locals()[result_df_name][columns]\n",
    "\n",
    "        # Append the current DataFrame to the list\n",
    "        all_results_dfs.append(locals()[result_df_name])\n",
    "        all_normalized_dfs.append(locals()[normalized_df_name])\n",
    "        all_PercentStation_dfs.append(locals()[Sum_PercentStation_name])\n",
    "\n",
    "        ###########################################################################\n",
    "        ##### New lines to carry out variable comparisons at the same time\n",
    "        ###########################################################################\n",
    "        # Only implement this for nStops == 10\n",
    "        if nStops == 10:\n",
    "\n",
    "            variables_to_compare = ['moving_mph_new','max_mph_new', 'max_distance_from_coast_mi_t', 'Total_distance_traveled_mi_t', 'Trip_Duration_t', 'move_efficiency_t', 'Weekend_trip_t']\n",
    "            # Initialize an empty DataFrame to store the results\n",
    "            # results_df = pd.DataFrame(columns=['RunTitle', 'Variable', 'StopAtStation=0 Mean', 'StopAtStation=1 Mean', 'Mean Difference', 'P-value'])\n",
    "\n",
    "            LaunchSites_df_counted['AllStates'] = 1\n",
    "\n",
    "            statestocheck = ['AllStates', 'TX', 'LA', 'MS', 'AL', 'FL']\n",
    "            for statename in statestocheck:\n",
    "                # Bring in new speeds\n",
    "                LaunchSites_df_counted2 = LaunchSites_df_counted.merge(newspeed_df, on=['cuebiq_id', 'Trip_number'], how='left')\n",
    "                LaunchSites_df_counted2 = LaunchSites_df_counted2.drop_duplicates()\n",
    "                # Filter only the rows for this state\n",
    "                LaunchSites_df_counted2 = LaunchSites_df_counted2[LaunchSites_df_counted2[statename] == 1]\n",
    "                \n",
    "                LaunchSites_df_counted2['Total_distance_traveled_mi_t']=LaunchSites_df_counted2['Total_distance_traveled_t']*0.621371\n",
    "                LaunchSites_df_counted2['max_distance_from_coast_mi_t']=LaunchSites_df_counted2['max_distance_from_coast_t']*0.621371\n",
    "\n",
    "                # Loop through each variable\n",
    "                station_df = LaunchSites_df_counted2[(LaunchSites_df_counted2['StopAtStation'] == 1)]\n",
    "                nonstation_df = LaunchSites_df_counted2[(LaunchSites_df_counted2['StopAtStation'] == 0)]\n",
    "                for variable in variables_to_compare:\n",
    "                    # Create data frames fro station & nonstation trips\n",
    "                    var_station_df = station_df[(~np.isinf(station_df[variable])) & (~np.isnan(station_df[variable]))]\n",
    "                    var_nonstation_df = nonstation_df[(~np.isinf(nonstation_df[variable])) & (~np.isnan(nonstation_df[variable]))]\n",
    "\n",
    "                    # Calculate stats for this variable\n",
    "                    station_var_mean = var_station_df[variable].mean()\n",
    "                    nonstation_var_mean = var_nonstation_df[variable].mean()\n",
    "                    t_stat, p_value = ttest_ind(var_nonstation_df[variable],var_station_df[variable])\n",
    "\n",
    "                    # Create a dictionary for the row to append\n",
    "                    row_data = {\n",
    "                        \"State\": statename,\n",
    "                        \"variable\": variable,\n",
    "                        \"nStops\": nStops,\n",
    "                        \"iOnlyCompleteTrips\": iOnlyCompleteTrips,\n",
    "                        \"station_var_mean\": station_var_mean,\n",
    "                        \"nonstation_var_mean\": nonstation_var_mean,\n",
    "                        \"t_stat\": t_stat,\n",
    "                        \"p_value\": p_value,\n",
    "                        \"total_var_count\": len(var_station_df) + len(var_nonstation_df),\n",
    "                        \"LaunchSites_count\": len(LaunchSites_df_counted2)\n",
    "                    }\n",
    "\n",
    "                    # Convert to DataFrame\n",
    "                    row_df = pd.DataFrame([row_data])\n",
    "\n",
    "                    # Append to CSV (include header only if the file doesn't exist)\n",
    "                    file_exists = os.path.isfile(DiffOfMeansResults_2_filename)\n",
    "                    row_df.to_csv(DiffOfMeansResults_2_filename, mode='a', header=not file_exists, index=False)\n",
    "\n",
    "            ###########################################################################\n",
    "            ####### End of Stuff for calculating statistics for comparing station & non-station trips\n",
    "            ###########################################################################\n",
    "        \n",
    "        \n",
    "AllResults_df = pd.concat(all_results_dfs, axis=0)\n",
    "AllResults_df.to_csv(count_filename, index=False)\n",
    "\n",
    "AllNormalized_df = pd.concat(all_normalized_dfs, axis=0)\n",
    "AllNormalized_df.to_csv(percent_filename, index=False)\n",
    "\n",
    "All_PercentStation_df = pd.concat(all_PercentStation_dfs, axis=0)\n",
    "All_PercentStation_df.to_csv(summary_percent_filename, index=False)\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275e844-6a78-4240-b6fc-2adc5340e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # cc_df = Merge_Trip_dfs(newspeed_df, LaunchSites_df_counted)\n",
    "# # # LaunchSites_df_counted['Total_distance_traveled_mi_t']\n",
    "# # for col in LaunchSites_df_counted2.columns:\n",
    "# #     print(col)\n",
    "# station_var_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1a5cf5-4f00-4263-89ae-554adf7d9478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AllResults_df[AllResults_df['nStops'] == 10].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b6781-e78c-420a-a4e4-89fb40e970f8",
   "metadata": {},
   "source": [
    "## Create counts of all the trips that are used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b894e9a-7ac4-4ebd-981c-d29552d9587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Complete_Indicators_df = pd.read_csv(Indicators_Classified_filename)\n",
    "print(\"with dups\", len(Complete_Indicators_df))\n",
    "Complete_Indicators_df = Complete_Indicators_df.sort_values(by=['cuebiq_id', 'Trip_number'])\n",
    "Complete_Indicators_df = Complete_Indicators_df.drop_duplicates(subset=['cuebiq_id', 'Trip_number'], keep='first')\n",
    "print(\"without dups\", len(Complete_Indicators_df))\n",
    "\n",
    "# Count the rows where Predicted_Class == 371\n",
    "count_371 = Complete_Indicators_df[Complete_Indicators_df['Predicted_Class'] == 371].shape[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of rows with Predicted_Class == 371: {count_371}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774068e0-c0a7-4c2c-ada2-0e71f16da3fa",
   "metadata": {},
   "source": [
    "### Create graph of station as a function of nStops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b5a710-c717-4178-8b4b-460984a98f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "# Note that when Interruption_01 = 0, this includes all trips\n",
    "for iNotDisappear in [0, 1]:\n",
    "    df = pd.read_csv(summary_percent_filename)\n",
    "    df = df[df['Interruption_01'] == iNotDisappear]\n",
    "\n",
    "    # Extract data for each state and 'AllStates'\n",
    "    tx_data = df['TX']\n",
    "    la_data = df['LA']\n",
    "    ms_data = df['MS']\n",
    "    al_data = df['AL']\n",
    "    fl_data = df['FL']\n",
    "    all_states_data = df['AllStates']\n",
    "    trips_analyzed_data = df['TripsAnalyzed']\n",
    "    n_stops_data = df['nStops']\n",
    "\n",
    "    # # Plot the data for each state with different line styles and thickness\n",
    "    # plt.plot(n_stops_data, tx_data, label='TX', linestyle='-', linewidth=0.8)\n",
    "    # plt.plot(n_stops_data, la_data, label='LA', linestyle='-', linewidth=0.8)\n",
    "    # plt.plot(n_stops_data, ms_data, label='MS', linestyle='-', linewidth=0.8)\n",
    "    # plt.plot(n_stops_data, al_data, label='AL', linestyle='-', linewidth=0.8)\n",
    "    # # plt.plot(n_stops_data, fl_data, label='FL', linestyle='-', linewidth=0.8)\n",
    "    # plt.plot(n_stops_data, all_states_data, label='All States', linestyle='-', linewidth=2.0)\n",
    "\n",
    "    linewidth = 0.8\n",
    "    marksize = 3\n",
    "    plt.plot(n_stops_data, tx_data, label='TX', linestyle='-',  linewidth=linewidth, marker='o', markersize=marksize, color='#1b9e77')   # green\n",
    "    plt.plot(n_stops_data, la_data, label='LA', linestyle='--', linewidth=linewidth, marker='s', markersize=marksize, color='#d95f02')   # orange\n",
    "    plt.plot(n_stops_data, ms_data, label='MS', linestyle=':',  linewidth=linewidth, marker='^', markersize=marksize, color='#7570b3')   # purple\n",
    "    plt.plot(n_stops_data, al_data, label='AL', linestyle='-.', linewidth=linewidth, marker='D', markersize=marksize, color='#e7298a')   # pink\n",
    "    plt.plot(n_stops_data, all_states_data, label='All States', linestyle='-', linewidth=3.0, color='black')\n",
    "\n",
    "    # Add a legend to the plot\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    # Set labels for the axes\n",
    "    plt.xlabel('Stops required for inclusion')\n",
    "    plt.ylabel('Station trips as a percent of all trips analyzed')\n",
    "\n",
    "    # Set the y-axis range for the primary y-axis (left axis)\n",
    "    plt.ylim(0.5, 1.0)\n",
    "\n",
    "    # Format the left y-axis as a percentage\n",
    "    percent_formatter = FuncFormatter(lambda x, _: '{:.0%}'.format(x))\n",
    "    plt.gca().yaxis.set_major_formatter(percent_formatter)\n",
    "\n",
    "    ################# Second Axis Stuff ###################\n",
    "    # Create a secondary y-axis for 'TripsAnalyzed'\n",
    "    # ax2 = plt.gca().twinx()\n",
    "    # ax2.plot(n_stops_data, trips_analyzed_data, label='Trips Analyzed', linestyle='--', linewidth=1.5, color='orange')\n",
    "\n",
    "    # Set labels for the secondary y-axis\n",
    "    # ax2.set_ylabel('TripsAnalyzed') #, color='orange')\n",
    "\n",
    "    # Format the right y-axis with commas\n",
    "    comma_formatter = FuncFormatter(lambda x, _: '{:,}'.format(int(x)))\n",
    "    # ax2.yaxis.set_major_formatter(comma_formatter)\n",
    "\n",
    "    # Add a legend to the plot\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Set the title of the plot\n",
    "    titletext = f\"Percentage of all trips that stop near a station\\n(All Trips)\"\n",
    "    fig_title = os.path.join(Figures_directory, f'AnalysisOfTripsNearStations_AllTrips.jpg')\n",
    "\n",
    "    if iNotDisappear >0:\n",
    "        titletext = f\"Percentage of all trips that stop near a station\\n(Fully-tracked trips)\"\n",
    "        fig_title = os.path.join(Figures_directory, f'AnalysisOfTripsNearStations_CompleteTrips.jpg')\n",
    "        \n",
    "    # plt.title(titletext)\n",
    "\n",
    "    # Adjust layout to make room for the buffer\n",
    "    plt.tight_layout(pad=0.5)\n",
    "\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.savefig(fig_title, bbox_inches='tight', pad_inches=0, facecolor='white')\n",
    "    plt.show()\n",
    "    print(trips_analyzed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4357cc96-bd2c-4372-bd4a-ad762f448e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import h3\n",
    "import numpy as np\n",
    "\n",
    "percentiles = np.arange(10, 100, 10)\n",
    "\n",
    "# Create a map centered around the Gulf of Mexico\n",
    "def MakeH3Map(TripsByH3_df, ColName, ThisMapName):\n",
    "    # Create a folium map centered at the specified location\n",
    "    m = folium.Map(location=[27.0, -90.0], zoom_start=6)\n",
    "\n",
    "    # Compute percentiles for the color scale\n",
    "    percentiles = np.arange(10, 100, 10)\n",
    "    percentile_values = np.percentile(TripsByH3_df[ColName], percentiles)\n",
    "\n",
    "    # Add the boundaries of each H3 cell to the map with different colors based on total_time\n",
    "    for index, row in TripsByH3_df.iterrows():\n",
    "        # Determine the color based on the value in ColName\n",
    "        if row[ColName] == 0:\n",
    "            fillColor = None\n",
    "            fillOpacity = 0\n",
    "        else:\n",
    "            fillColor = get_color(row[ColName], percentile_values)\n",
    "            fillOpacity = 0.8\n",
    "\n",
    "        geojson = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"geometry\": {\n",
    "                \"type\": \"Polygon\",\n",
    "                \"coordinates\": [h3.h3_to_geo_boundary(row['h3_cell'], geo_json=True)]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Add the GeoJSON layer to the map with custom style\n",
    "        folium.GeoJson(\n",
    "            geojson,\n",
    "            style_function=lambda feature, fillColor=fillColor, fillOpacity=fillOpacity: {\n",
    "                'fillColor': fillColor,\n",
    "                'color': 'black',\n",
    "                'weight': 1,\n",
    "                'fillOpacity': fillOpacity\n",
    "            }\n",
    "        ).add_child(\n",
    "            folium.Popup(f\"{ColName}: {row[ColName]}\")\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Save the map to an HTML file\n",
    "    map_filename = os.path.join(Figures_directory, ThisMapName)\n",
    "    m.save(map_filename)\n",
    "    \n",
    "    # Return the map\n",
    "    return m\n",
    "\n",
    "m1 = MakeH3Map(TripsByH3, 'trip_count', \"MapOfAllTrips.html\")\n",
    "m2 = MakeH3Map(TripsByH3_station, 'trip_count', \"MapOfStationTrips.html\")\n",
    "m3 = MakeH3Map(TripsByH3_nonstation, 'trip_count', \"MapOfNonStationTrips.html\")\n",
    "\n",
    "m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e339a43e-e0d3-40c9-abef-d0c615b40ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c8dc0-8ada-4d76-9f1c-efaa3086c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeH3MapTwoColors(TripsByH3_df, ColName, ThisMapName):\n",
    "    m = folium.Map(location=[27.0, -90.0], zoom_start=6)\n",
    "\n",
    "    # Add the boundaries of each H3 cell to the map with different colors based on the value in ColName\n",
    "    for index, row in TripsByH3_df.iterrows():\n",
    "        # Determine the color based on whether the value is positive or negative\n",
    "        color = 'green' if row[ColName] > 0 else 'red'\n",
    "\n",
    "        # Define the GeoJSON for the H3 cell\n",
    "        geojson = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"geometry\": {\n",
    "                \"type\": \"Polygon\",\n",
    "                \"coordinates\": [h3.h3_to_geo_boundary(row['h3_cell'], geo_json=True)]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Add the GeoJSON layer to the map with custom style\n",
    "        folium.GeoJson(\n",
    "            geojson,\n",
    "            style_function=lambda feature, color=color: {\n",
    "                'fillColor': color,\n",
    "                'color': 'black',\n",
    "                'weight': 1,\n",
    "                'fillOpacity': 0.8\n",
    "            }\n",
    "        ).add_child(\n",
    "            folium.Popup(f\"{ColName}: {row[ColName]}\")\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Save the map to an HTML file\n",
    "    map_filename = os.path.join(Figures_directory, ThisMapName)\n",
    "    m.save(map_filename)\n",
    "    \n",
    "    # Display the map in the notebook (if in a Jupyter notebook environment)\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89927d-b667-4874-b319-cb11d3746469",
   "metadata": {},
   "source": [
    "### Map comparing destinations for more non-station "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5c092-6f6e-4879-83de-2d2026a9cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TripsByH3_nonstation['percent'] = TripsByH3_nonstation['trip_count']/ TripsByH3_nonstation['trip_count'].sum()\n",
    "TripsByH3_station['percent'] = TripsByH3_station['trip_count']/ TripsByH3_station['trip_count'].sum()\n",
    "\n",
    "merged_df = pd.merge(TripsByH3_nonstation, TripsByH3_station, on='h3_cell', how='outer', suffixes=('_nonstation', '_station'))\n",
    "\n",
    "# Fill NaN values with 0 (or another appropriate value)\n",
    "merged_df.fillna(0, inplace=True)\n",
    "\n",
    "# Calculate the difference\n",
    "merged_df['percent_diff'] = merged_df['percent_nonstation'] - merged_df['percent_station']\n",
    "\n",
    "# Create the final DataFrame with 'h3_cell' and 'percent_diff'\n",
    "H3_nonstation_less_station = merged_df[['h3_cell', 'percent_diff']]\n",
    "\n",
    "mcomparison = MakeH3Map(H3_nonstation_less_station, 'percent_diff', \"ComparionsMap.html\")\n",
    "per_min = 0.01\n",
    "H3_nonstation_less_station_big = H3_nonstation_less_station[(H3_nonstation_less_station['percent_diff'] > per_min) |\n",
    "                                                            (H3_nonstation_less_station['percent_diff'] < -per_min) ]\n",
    "H3_nonstation_less_station_big['percent_diff'] = round(H3_nonstation_less_station_big['percent_diff']*100,1)\n",
    "mcomparison2 = MakeH3MapTwoColors(H3_nonstation_less_station_big, 'percent_diff', \"ComparionsMapBigDiff.html\")\n",
    "\n",
    "mcomparison2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e123fb1-6dfd-45fa-8ea2-dbad49aa30f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "H3_nonstation_less_station.to_csv('Debug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc780e2-641e-43ea-94a9-3193e6fcfdf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
