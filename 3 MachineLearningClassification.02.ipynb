{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f60c3c41-d0b6-4b9c-9acb-fdfc69533b3c",
   "metadata": {},
   "source": [
    "# This notebook carries out the machine learning algorithm on the Cuebiq data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cfbdee-8f86-4db9-a03e-8c7ff332b8ba",
   "metadata": {},
   "source": [
    "## Key inputs: \n",
    "* **Indicators_OurTable.csv** - features for each trip, that will be used for ML classification. \n",
    "* Random Forest Classifier trained on AIS data: **rf_model_AIS_2019.pkl**\n",
    "\n",
    "## Key outputs:\n",
    "* Indicators file with columns indicating the predicted vessel class: **Indicators_OurTable.Predictions.csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d983e-17b0-440b-ad7e-aef0bbdb4fde",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4513f3-ed33-4ab3-a855-a8ea15cc1188",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b55ba2a-5b07-4917-9fc2-92b63fe254f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime\n",
    "import pytz  \n",
    "import warnings\n",
    "import numpy as np\n",
    "import math\n",
    "local_timezone = pytz.timezone('US/Central')\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "import geopandas as gpd\n",
    "import heapq\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "import statsmodels\n",
    "\n",
    "from scipy import interpolate\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "from zipfile import ZipFile\n",
    "from shapely.geometry import Point  # Import the Point class from shapely.geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41acce2e-4c2a-4b3b-b441-943e9b0fab74",
   "metadata": {},
   "source": [
    "## Set Directories to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc5154-f12f-4057-85a8-5f69ecc7b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Directory for this output\n",
    "OurTable_V3_directory = '~/RecFishing/Analysis with Our Tables and V3/Data Files'\n",
    "# Expand the tilde to the user's home directory\n",
    "OurTable_V3_directory = os.path.expanduser(OurTable_V3_directory)\n",
    "# Check to make sure the directory exist\n",
    "DirExist = os.path.exists(OurTable_V3_directory)\n",
    "print(OurTable_V3_directory, \"exists = \" ,DirExist)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory for output from the first draw\n",
    "Batch01_directory = '~/RecFishing/Analysis with Our Tables and V3/Data Files/Batch01'\n",
    "# Expand the tilde to the user's home directory\n",
    "Batch01_directory = os.path.expanduser(Batch01_directory)\n",
    "# Check to make sure the directory exist\n",
    "DirExist = os.path.exists(Batch01_directory)\n",
    "print(Batch01_directory, \"exists = \" ,DirExist)\n",
    "\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory for Groups of V3 Pings\n",
    "V3_Pings_Groups_directory = '~/RecFishing/Analysis with Our Tables and V3/Data Files/V3_Ping_Groups'\n",
    "V3_Pings_Groups_directory = os.path.expanduser(V3_Pings_Groups_directory)\n",
    "print(V3_Pings_Groups_directory, \"exists = \" ,os.path.exists(V3_Pings_Groups_directory))\n",
    "\n",
    "#################################################################\n",
    "# Directory some core data fro analysis\n",
    "CoreData_Directory = '~/RecFishing/CoreData'\n",
    "CoreData_Directory = os.path.expanduser(CoreData_Directory)\n",
    "print(CoreData_Directory, \"exists = \", os.path.exists(CoreData_Directory))\n",
    "\n",
    "#################################################################\n",
    "# Directory  with Original_directory material\n",
    "Original_directory = '~/RecFishing/DataflowStudioJobs'\n",
    "Original_directory = os.path.expanduser(Original_directory)\n",
    "DirExist = os.path.exists(Original_directory)\n",
    "print(Original_directory, \"exists = \", DirExist)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory  with Original_directory material\n",
    "Previously_Processed_directory = '~/RecFishing/DataflowStudioJobs/FinalCode - Rec Fishing Identification'\n",
    "Previously_Processed_directory = os.path.expanduser(Previously_Processed_directory)\n",
    "DirExist = os.path.exists(Previously_Processed_directory)\n",
    "print(Previously_Processed_directory, \"exists = \", DirExist)\n",
    "\n",
    "#################################################################\n",
    "# Directory with Travel Cost files\n",
    "Travel_Cost_directory = '~/RecFishing/Travel Costs with Dedicated Table/CSV Files'\n",
    "# Expand the tilde to the user's home directory\n",
    "Travel_Cost_directory = os.path.expanduser(Travel_Cost_directory)\n",
    "DirExist = os.path.exists(Travel_Cost_directory)\n",
    "print(Travel_Cost_directory, \"exists = \", DirExist)\n",
    "\n",
    "#################################################################\n",
    "# Directory with Weather data and related files\n",
    "Weather_Data_directory = '~/RecFishing/uploaded_files/Weather Data'\n",
    "# Expand the tilde to the user's home directory\n",
    "Weather_Data_directory = os.path.expanduser(Weather_Data_directory)\n",
    "print(Weather_Data_directory, \"exists = \", DirExist)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Directory with other Uploaded data \n",
    "Uploaded_Data_directory = '~/RecFishing/uploaded_files'\n",
    "# Expand the tilde to the user's home directory\n",
    "Uploaded_Data_directory = os.path.expanduser(Uploaded_Data_directory)\n",
    "\n",
    "#################################################################\n",
    "####################   Results and Analysis #################################################\n",
    "Results_directory = '~/RecFishing/Analysis with Our Tables and V3/Results'\n",
    "# Expand the tilde to the user's home directory\n",
    "Results_directory = os.path.expanduser(Results_directory)\n",
    "\n",
    "####################################################################################\n",
    "####################  AIS Directory #################################################\n",
    "AIS_Directory = '~/RecFishing/AIS Files/Data'\n",
    "AIS_Directory = os.path.expanduser(AIS_Directory)\n",
    "DirExist = os.path.exists(AIS_Directory)\n",
    "print(AIS_Directory, \"exists = \", DirExist)\n",
    "\n",
    "# ID_list_RandomSample from ScheduledExecution5.pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac88be0-2199-4d5b-a65f-6abcc63b08e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Files to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25567483-eca1-4043-8b1d-aa619bbba018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_existence(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"{file_path} Does NOT Exist\")\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "#########################  Log File  ################\n",
    "Log_filename  =  os.path.join(OurTable_V3_directory, 'Log.txt')\n",
    "\n",
    "######################################################################################################################\n",
    "########################## Complete list of randomized IDs- without bernouli sampling 740k #########################\n",
    "# PKL_File_With_Random_IDs_Filename  =  os.path.join(Original_directory, 'cuebiq_id_list_wo_sampling_740k.pkl')\n",
    "PKL_File_With_Random_IDs_Filename  =  os.path.join(CoreData_Directory, 'cuebiq_id_list_wo_sampling_740k.pkl')\n",
    "check_file_existence(PKL_File_With_Random_IDs_Filename)\n",
    "    \n",
    "# Data gathered and used prior to the NOAA Webinar in February 2024\n",
    "IDs_Used_in_NOAA_Webinar_filename = os.path.join(CoreData_Directory, 'IDs_From_Random_Draw_Prior_to_NOAA_Webinar.csv')\n",
    "check_file_existence(IDs_Used_in_NOAA_Webinar_filename)\n",
    "Ping_Used_in_NOAA_Webinar_filename = os.path.join(CoreData_Directory, 'Pings_From_Random_Draw_Prior_to_NOAA_Webinar.csv')\n",
    "check_file_existence(Ping_Used_in_NOAA_Webinar_filename)\n",
    "                                                     \n",
    "# List of IDs that have been processed for Indicators\n",
    "AlreadyFullyProcessedIDs_Filename  =  os.path.join(OurTable_V3_directory, 'RandomlyChosenCuebiq_ids.List_of_Processed_ids.csv')\n",
    "check_file_existence(AlreadyFullyProcessedIDs_Filename)\n",
    "    \n",
    "######################################################################################################################\n",
    "#########################  ID Checklist with columns for ID, Pings, Indicators Created (TF) & Trips  ################\n",
    "IDs_Pulled_from_Dedicated_Table_filename  =  os.path.join(OurTable_V3_directory, 'IDs_Pulled_From_Dedicated_Table.csv')\n",
    "check_file_existence(IDs_Pulled_from_Dedicated_Table_filename)\n",
    "    \n",
    "ID_For_V3_Queries_filename  =  os.path.join(OurTable_V3_directory, 'IDs_from_V3.csv')\n",
    "check_file_existence(ID_For_V3_Queries_filename)\n",
    "    \n",
    "RecTripRating_filename =  os.path.join(OurTable_V3_directory, 'RecTripRating.csv')\n",
    "check_file_existence(RecTripRating_filename)\n",
    "    \n",
    "# This file contains information about the rows of Pings_V3_temp_filename that can be used to avoid loading the entire file into a data frame\n",
    "V3_Pings_Index_filename =  os.path.join(OurTable_V3_directory, 'V3_Pings_File_Index.csv')\n",
    "check_file_existence(V3_Pings_Index_filename)\n",
    "    \n",
    "ID_Groups_filename = os.path.join(OurTable_V3_directory,'Cuebiq_ID_Groups.csv')\n",
    "check_file_existence(ID_Groups_filename)\n",
    "\n",
    "######################################################\n",
    "# Pings in the OurTable for a single large draw of IDs TEMPORARY FILE\n",
    "Pings_OurTable_temp_filename = os.path.join(OurTable_V3_directory,'Pings_OurTable_temp.csv')\n",
    "check_file_existence(Pings_OurTable_temp_filename)\n",
    "    \n",
    "# Pings from V3 corresponding with the IDs found in the OurTable \n",
    "Pings_V3_temp_filename = os.path.join(OurTable_V3_directory,'Pings_V3_temp.csv')\n",
    "check_file_existence(Pings_V3_temp_filename)\n",
    "    \n",
    "# Set output file names\n",
    "Indicators_IDs_checked_filename = os.path.join(OurTable_V3_directory, 'IDs_Checked_Indicators_OurTable.csv')\n",
    "check_file_existence(Indicators_IDs_checked_filename)\n",
    "\n",
    "cuebiq_id_list_and_count_filename= os.path.join(OurTable_V3_directory,'cuebiq_id_list_and_count.csv')\n",
    "check_file_existence(cuebiq_id_list_and_count_filename)\n",
    "\n",
    "# List of IDs and dates for V3 query Pings in the OurTable for a single large draw of IDs TEMPORARY FILE\n",
    "OurTable_IDs_and_Dates_filename = os.path.join(OurTable_V3_directory,'OurTable_IDs_and_Dates.csv')\n",
    "check_file_existence(OurTable_IDs_and_Dates_filename)\n",
    "    \n",
    "##########################################################################################\n",
    "############################### PINGS FILES   ##############################################\n",
    "Pings_OurTable_Gulf_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Gulf_ALL.csv')\n",
    "check_file_existence(Pings_OurTable_Gulf_filename)\n",
    "\n",
    "Combined_Pings_OurTable_Gulf_filename= os.path.join(OurTable_V3_directory,'Combined_Pings_OurTable_Gulf_ALL.csv')\n",
    "check_file_existence(Combined_Pings_OurTable_Gulf_filename)\n",
    "\n",
    "Pings_V3_Before_After_filename= os.path.join(OurTable_V3_directory,'Pings_V3_Before_After.csv')\n",
    "check_file_existence(Pings_V3_Before_After_filename)\n",
    "\n",
    "Combined_Pings_V3_Before_After_filename= os.path.join(OurTable_V3_directory,'Combined_Pings_V3_Before_After.csv')\n",
    "check_file_existence(Combined_Pings_V3_Before_After_filename)\n",
    "\n",
    "Pings_OurTable_Gulf_MT19_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Gulf_MT19.csv')\n",
    "check_file_existence(Pings_OurTable_Gulf_MT19_filename)\n",
    "\n",
    "Pings_OurTable_Coast_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Coast.csv')\n",
    "check_file_existence(Pings_OurTable_Coast_filename)\n",
    "\n",
    "Pings_OurTable_Outside_our_wkts_filename= os.path.join(OurTable_V3_directory,'Pings_OurTable_Outside_our_wkts.csv')\n",
    "check_file_existence(Pings_OurTable_Outside_our_wkts_filename)\n",
    "    \n",
    "##########################################################################################\n",
    "############################### INDICATORS  ##############################################\n",
    "Indicators_filename = os.path.join(OurTable_V3_directory,'Indicators_OurTable.csv')\n",
    "check_file_existence(Indicators_filename)\n",
    "    \n",
    "# cuebiq_id_count_filename= os.path.join(EEZ_V3_directory,'cuebiq_id_count_distribution_EEZ_V3.csv')\n",
    "Indicators_Classified_filename = os.path.join(OurTable_V3_directory,'Indicators_OurTable.Predictions.csv')\n",
    "check_file_existence(Indicators_Classified_filename)\n",
    "\n",
    "Rec_Indicators_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable.csv')\n",
    "check_file_existence(Rec_Indicators_filename)\n",
    "\n",
    "Rec_Indicators_Step1_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable.Step1.csv')\n",
    "check_file_existence(Rec_Indicators_filename)\n",
    "\n",
    "V3_Indicators_filename =  os.path.join(OurTable_V3_directory,'V3_indicators.csv')\n",
    "check_file_existence(V3_Indicators_filename)\n",
    "\n",
    "Rec_indicators_with_V3_filename = os.path.join(OurTable_V3_directory,'rec_indicators_with_V3.csv')\n",
    "check_file_existence(Rec_indicators_with_V3_filename)\n",
    "\n",
    "Indicators_with_V3_indicators_filename= os.path.join(OurTable_V3_directory,'Indicators_with_V3_indicators_indicators.csv')\n",
    "check_file_existence(Indicators_with_V3_indicators_filename)\n",
    "\n",
    "# Rec_Indicators_Selected_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable_Selected.csv')\n",
    "Rec_Indicators_Selected_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_OurTable_Selected_May2024.csv')\n",
    "check_file_existence(Rec_Indicators_Selected_filename)\n",
    "\n",
    "Rec_Indicators_Final_All_Exclusions_And_Disappearance_filename = os.path.join(OurTable_V3_directory,'Rec_Indicators_Final_All_Exclusions_And_Disappearance.csv')\n",
    "check_file_existence(Rec_Indicators_Final_All_Exclusions_And_Disappearance_filename)\n",
    "\n",
    "# Sorted_Results_file_path = os.path.join(OurTable_V3_directory,'Indicators_EEZ_and_V3.Predictions.sorted.csv')\n",
    "# RecFishing_Results_file_path =  os.path.join(OurTable_V3_directory,'RecFishingBoat Predictions.sorted.csv')\n",
    "\n",
    "DisappearanceIndicators_filename = os.path.join(OurTable_V3_directory,'DisappearanceIndicators.csv')\n",
    "check_file_existence(DisappearanceIndicators_filename)\n",
    "\n",
    "DisappearanceAnalysis_filename = os.path.join(OurTable_V3_directory,'DisappearanceAnalysis.csv')\n",
    "check_file_existence(DisappearanceAnalysis_filename)\n",
    "\n",
    "Stops_Indicators_filename = os.path.join(OurTable_V3_directory,'Stops_Indicators.csv')\n",
    "check_file_existence(Stops_Indicators_filename)\n",
    "\n",
    "Trawls_Indicators_filename = os.path.join(OurTable_V3_directory,'Trawls_Indicators.csv')\n",
    "check_file_existence(Trawls_Indicators_filename)\n",
    "\n",
    "Stop_Trawls_Indicators_filename = os.path.join(OurTable_V3_directory,'Stop_Trawls_Indicators.csv')\n",
    "check_file_existence(Stop_Trawls_Indicators_filename)\n",
    "\n",
    "##########################################################################################\n",
    "############################### Files that COMBINE BATCH01 and newer data ################\n",
    "Combined_indicators_with_disappearance_filename = os.path.join(OurTable_V3_directory,'Combined_indicators_with_disappearance.csv')\n",
    "check_file_existence(Combined_indicators_with_disappearance_filename)\n",
    "\n",
    "Combined_Raw_indicators_filename = os.path.join(OurTable_V3_directory,'Combined_Raw_indicators.csv')\n",
    "check_file_existence(Combined_Raw_indicators_filename)\n",
    "\n",
    "\n",
    "## Combined file that includes the Batch01 files\n",
    "Combined_Stops_Indicators_filename = os.path.join(OurTable_V3_directory,'Combined_Stops_Indicators.csv')\n",
    "check_file_existence(Combined_Stops_Indicators_filename)\n",
    "\n",
    "Combined_Trawls_Indicators_filename = os.path.join(OurTable_V3_directory,'Combined_Trawls_Indicators.csv')\n",
    "check_file_existence(Combined_Trawls_Indicators_filename)\n",
    "\n",
    "Combined_Stop_Trawls_Indicators_filename = os.path.join(OurTable_V3_directory,'Combined_Stop_Trawls_Indicators.csv')\n",
    "check_file_existence(Combined_Stop_Trawls_Indicators_filename)\n",
    "\n",
    "\n",
    "\n",
    "#################################################################\n",
    "####################   Results and Analysis #################################################\n",
    "Station_NonStationAnalysis_filename  = os.path.join(Results_directory,'Station_NonStationAnalysis.csv')\n",
    "check_file_existence(Station_NonStationAnalysis_filename)\n",
    "\n",
    "Station_NonStationAnalysis_full_filename  = os.path.join(Results_directory,'Station_NonStationAnalysis_full.csv')\n",
    "check_file_existence(Station_NonStationAnalysis_full_filename)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "############################### WEATHER data files ####################################\n",
    "Buoys_file_path  = os.path.join(Weather_Data_directory,'Buoys.csv')\n",
    "check_file_existence(Buoys_file_path)\n",
    "\n",
    "Weather_file_path  = os.path.join(Weather_Data_directory,'DailyWeatherData.csv')\n",
    "check_file_existence(Weather_file_path)\n",
    "\n",
    "##########################################################################################\n",
    "############################### SUPPLEMENTARY MAP DATA  ############################\n",
    "Industrial_polygons_filename  = os.path.join(Uploaded_Data_directory,'Polygons Around Industrial Sites.wkt')\n",
    "check_file_existence(Industrial_polygons_filename)\n",
    "\n",
    "\n",
    "station_points_filename = os.path.join(Uploaded_Data_directory,'LA_TX_Union_Station_WGS84.csv')\n",
    "check_file_existence(station_points_filename)\n",
    "\n",
    "MRIP_station_points_filename =os.path.join(Uploaded_Data_directory,'MRIP_stations.csv')\n",
    "check_file_existence(MRIP_station_points_filename)\n",
    "\n",
    "##########################################################################################\n",
    "############################### AIS Files INCLUDING CLASSIFIER ############################\n",
    "RF_Classfier_filename = os.path.join(AIS_Directory, 'rf_model_AIS_2019.pkl')\n",
    "check_file_existence(RF_Classfier_filename)\n",
    "\n",
    "RF_Importance_Factors_filename = os.path.join(AIS_Directory, 'rf_classifier_importance_factors.csv')\n",
    "check_file_existence(RF_Importance_Factors_filename)\n",
    "\n",
    "AIS_Predictions_filename = os.path.join(AIS_Directory, 'RandomForest_Predictions2019AISData.csv')\n",
    "check_file_existence(AIS_Predictions_filename)\n",
    "    \n",
    "AIS_indicators_file_path = os.path.join(AIS_Directory,'Indicators2019_All.C.csv')\n",
    "check_file_existence(AIS_indicators_file_path)\n",
    "  \n",
    "\n",
    "### Dedicate Table Names for reference\n",
    "# Dedicated table with all Pings within the Gulf WKT for 1/12019 - 4/22/2022\n",
    "#  Table Name:  dedicated.ScheduledExecution5.DeviceTable   \n",
    "#  Code used for call:  RecFishing/DataflowStudioJobs/ScheduledEx5-updated.ipynb\n",
    "\n",
    "# Dedicated table with all Pings within the Gulf WKT AND Origin for 1/12019 - 4/22/2022\n",
    "#  Table Name:  dedicated.ScheduledExecution5_parallel_origin.DeviceTable\n",
    "#  Code used for call:  RecFishing/DataflowStudioJobs/ScheduledEx5-origin.ipynb\n",
    "\n",
    "##########################################################################################\n",
    "############################### Results & Analysis ############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e02451-e714-4aaf-b887-eac63129e1cf",
   "metadata": {},
   "source": [
    "# Correct the Indicators for the problem in the calculation of trip duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09393211-23ef-4900-ab0c-0331d267574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### This Function works on an entire data frame ##################\n",
    "##  Use\n",
    "## df = df_duration_correction(df)\n",
    "def df_duration_correction(df):\n",
    "    # Calculate corrected trip duration\n",
    "    df['Trip_Duration_t_Corrected'] = (df['timestamp_end_t'] - df['timestamp_start_t']) / 60\n",
    "    \n",
    "    # Calculate duration correction factor and apply to other columns\n",
    "    df['Duration_Correction'] = df['Trip_Duration_t'] / df['Trip_Duration_t_Corrected']\n",
    "    df['Pings_per_minute_t'] = df['Pings_per_minute_t'] * df['Duration_Correction']\n",
    "    df['pct_time_stopped'] = df['pct_time_stopped'] * df['Duration_Correction']\n",
    "    df['pct_time_trawling'] = df['pct_time_trawling'] * df['Duration_Correction']\n",
    "    df['pct_time_moving'] = df['pct_time_moving'] * df['Duration_Correction']\n",
    "    \n",
    "    # If Trip_Duration_t_Corrected is zero, set all related columns to zero\n",
    "    zero_duration_mask = df['Trip_Duration_t_Corrected'] == 0\n",
    "    df.loc[zero_duration_mask, ['Pings_per_minute_t', 'pct_time_stopped', 'pct_time_trawling', 'pct_time_moving']] = 0\n",
    "    \n",
    "    # Update Trip_Duration_t to corrected value\n",
    "    df['Trip_Duration_t'] = df['Trip_Duration_t_Corrected']\n",
    "    \n",
    "    # Drop intermediate columns\n",
    "    df.drop(columns=['Trip_Duration_t_Corrected', 'Duration_Correction'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "########### This Function only works on one value at a time ##################\n",
    "def duration_correction(timestamp_end_t, timestamp_start_t, Trip_Duration_t, \n",
    "                        Pings_per_minute_t, pct_time_stopped, pct_time_trawling, \n",
    "                        pct_time_moving):\n",
    "\n",
    "    Trip_Duration_t_Corrected = (timestamp_end_t - timestamp_start_t) / 60\n",
    "    \n",
    "    print(\"Trip_Duration_t, Trip_Duration_t_Corrected\", Trip_Duration_t, Trip_Duration_t_Corrected)\n",
    "    if Trip_Duration_t_Corrected > 0:\n",
    "        Duration_Correction = Trip_Duration_t / (Trip_Duration_t_Corrected)\n",
    "\n",
    "        Pings_per_minute_t = Pings_per_minute_t * Duration_Correction\n",
    "        pct_time_stopped = pct_time_stopped * Duration_Correction\n",
    "        pct_time_trawling = pct_time_trawling * Duration_Correction\n",
    "        pct_time_moving = pct_time_moving * Duration_Correction\n",
    "    else:\n",
    "        Duration_Correction = 0\n",
    "\n",
    "        Pings_per_minute_t = 0\n",
    "        pct_time_stopped = 0\n",
    "        pct_time_trawling = 0\n",
    "        pct_time_moving = 0\n",
    "\n",
    "    Trip_Duration_t =Trip_Duration_t_Corrected\n",
    "    return Trip_Duration_t, Pings_per_minute_t, pct_time_stopped, pct_time_trawling, pct_time_moving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aabe541-4216-415b-bea7-eaf700db6afd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Use the RF classifier to categorize the observations from the Spectus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a1afcb-d0ce-481a-86fd-01740ea9b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Load the trained Random Forest classifier from the saved file\n",
    "rf_classifier = joblib.load(RF_Classfier_filename)\n",
    "\n",
    "# Load the new dataset\n",
    "Complete_Indicators_df = pd.read_csv(Combined_Raw_indicators_filename)\n",
    "\n",
    "# # Correct the durations, which had problems in the original indicators file\n",
    "# Complete_Indicators_df = df_duration_correction(Complete_Indicators_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a77d6-3818-4baa-b978-ab0639548ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_Raw_indicators_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac3ab0-3813-478d-9979-48f5b05955c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import datetime\n",
    "\n",
    "# The set of variables used to predict must match those used in the original random forest classification \n",
    "X = Complete_Indicators_df[[\n",
    "                        'trips_per_day', \n",
    "                        'break_duration_t',\n",
    "                        'Weekday_trips', \n",
    "                        'Trip_Duration_t',\n",
    "                        'Total_distance_traveled_t', \n",
    "                        'Distance_from_origin_t', \n",
    "                        'first_distance_from_coast_t',\n",
    "                        'last_distance_from_coast_t', \n",
    "                        'Max_distance_traveled_origin_t',\n",
    "                        'Weekend_trip_t', \n",
    "                        'time_stopped_t', \n",
    "                        'pct_time_stopped',\n",
    "                        'number_of_stops_t', \n",
    "                        'longest_stop_t',\n",
    "                        'shortest_stop_t',\n",
    "                        'time_trawling_t',\n",
    "                        'pct_time_trawling',\n",
    "                        'number_of_trawl_t',\n",
    "                        'longest_trawl_t', \n",
    "                        'shortest_trawl_t', \n",
    "                        'distance_trawling_t',\n",
    "                        'time_moving_t', \n",
    "                        'pct_time_moving', \n",
    "                        'number_of_moves_t',\n",
    "                        'longest_move_t', \n",
    "                        'shortest_move_t',\n",
    "                        'distance_moving_t',\n",
    "                        'dist_from_origin_during_stops', \n",
    "                        'Trip_pings_mov_traw_per_time_t',\n",
    "                        'move_efficiency_t', \n",
    "                        'move_speed_t',\n",
    "                        'maxspeed_t', \n",
    "                        'max_distance_from_coast_t',\n",
    "                        'WSPD', \n",
    "                        'GST',\n",
    "                        'WVHT', \n",
    "                        'ATMP'   \n",
    "                        ]]\n",
    "\n",
    "\n",
    "# Assuming you have preprocessed your training data, preprocess the new dataset similarly\n",
    "# Use the trained rf_classifier to make predictions on the new dataset\n",
    "predictions = rf_classifier.predict(X)\n",
    "\n",
    "# Use the trained rf_classifier to predict class probabilities\n",
    "class_probabilities = rf_classifier.predict_proba(X)\n",
    "\n",
    "# Get the class labels\n",
    "class_labels = rf_classifier.classes_\n",
    "\n",
    "# Combine the predictions, class probabilities, and original features\n",
    "Indicators_with_RF_Predictions_df = pd.DataFrame({\n",
    "    'Predicted_Class': rf_classifier.predict(X),\n",
    "    **{f'Probability_{class_labels[i]}': class_probabilities[:, i] for i in range(len(class_labels))},\n",
    "    **Complete_Indicators_df  # Include original features from the new dataset\n",
    "})\n",
    "\n",
    "####################### Clean up and add max_prob column #############################\n",
    "# Add a new column 'max_prob' with the maximum probability from specified columns\n",
    "Indicators_with_RF_Predictions_df['max_prob'] = Indicators_with_RF_Predictions_df[['Probability_30', 'Probability_31', 'Probability_60',\n",
    "                                             'Probability_70', 'Probability_80', 'Probability_90',\n",
    "                                             'Probability_371', 'Probability_372']].max(axis=1)\n",
    "# Sort the DataFrame based on 'max_prob' in descending order\n",
    "Indicators_with_RF_Predictions_df = Indicators_with_RF_Predictions_df.sort_values(by='max_prob', ascending=False)\n",
    "\n",
    "# Reorder the columns as specified\n",
    "column_order = ['cuebiq_id', 'Trip_number','Predicted_Class', 'max_prob','Probability_30', 'Probability_31', 'Probability_60',\n",
    "                                             'Probability_70', 'Probability_80', 'Probability_90',\n",
    "                                             'Probability_371', 'Probability_372'] # sorted_feature_names  # 'feature_names' contains the ordered feature names\n",
    "other_columns = [col for col in Indicators_with_RF_Predictions_df.columns if col not in column_order]\n",
    "new_column_order = column_order + other_columns\n",
    "Indicators_with_RF_Predictions_df = Indicators_with_RF_Predictions_df[new_column_order]\n",
    "\n",
    "\n",
    "######################## Save the combined data to a CSV file  #######################\n",
    "Indicators_with_RF_Predictions_df.to_csv(Indicators_Classified_filename, index=False)\n",
    "\n",
    "classes_to_count = [371, 30, 31, 60, 70, 80, 90, 372]\n",
    "\n",
    "# Filter the DataFrame for each class and count the number of rows\n",
    "for class_num in classes_to_count:\n",
    "    count = Indicators_with_RF_Predictions_df[Indicators_with_RF_Predictions_df['Predicted_Class'] == class_num].shape[0]\n",
    "    percent = round(100*count/len(Indicators_with_RF_Predictions_df),0)\n",
    "    print(f\"Predictions of {class_num}: {count}  {percent}%\")\n",
    "\n",
    "print(f\"Total:         {len(Indicators_with_RF_Predictions_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4d9ae-29fd-48cc-8fc4-34e4e19ebe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store sorted results into a csv file\n",
    "\n",
    "RecFishing_Predictions = Indicators_with_RF_Predictions_df[(Indicators_with_RF_Predictions_df['Predicted_Class'] == 371) & (Indicators_with_RF_Predictions_df['max_prob'] >= 0.5)].copy()\n",
    "Indicators_with_RF_Predictions_df.to_csv(Sorted_Results_file_path, index=False)\n",
    "RecFishing_Predictions.to_csv(REcFishing_Results_file_path, index=False)\n",
    "\n",
    "\n",
    "current_time = datetime.now()\n",
    "formatted_time = current_time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "print(Sorted_Results_file_path, \" and \", REcFishing_Results_file_path, \"created at  \",  formatted_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51059af0-e5b2-4daa-96b7-d002c649853c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Analysis with categorized Spectus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c3934e-1e5d-4e2f-99f0-5cb604a59132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# File path and DataFrame loading\n",
    "RF_predictions_sorted = pd.read_csv(Sorted_Results_file_path)\n",
    "\n",
    "# Filter the DataFrame based on the conditions\n",
    "RFB_df = RF_predictions_sorted[(RF_predictions_sorted['Predicted_Class'] == 371) & (RF_predictions_sorted['max_prob'] >= 0.1)]\n",
    "\n",
    "# Initialize lists to store results\n",
    "thresholds = []\n",
    "count_rows_list = []\n",
    "unique_ids_list = []\n",
    "\n",
    "# Iterate over the probability thresholds (0.1, 0.2, ..., 0.9)\n",
    "for threshold in range(1, 10):\n",
    "    threshold /= 10.0  # Convert to decimal form\n",
    "    threshold_condition = RFB_df['max_prob'] >= threshold\n",
    "    count_rows = len(RFB_df[threshold_condition])\n",
    "    unique_ids = RFB_df[threshold_condition]['id'].nunique()\n",
    "    thresholds.append(threshold)\n",
    "    count_rows_list.append(count_rows)\n",
    "    unique_ids_list.append(unique_ids)\n",
    "\n",
    "# Display the results\n",
    "print(\"Recreational fishing boats\")\n",
    "for threshold, count_rows, unique_ids in zip(thresholds, count_rows_list, unique_ids_list):\n",
    "    print(f\"For max_prob >= {threshold:.1f}:\", f\"  Number of trips: {count_rows}\", f\"  Number of ids: {unique_ids}\")\n",
    "\n",
    "# Create a line graph\n",
    "plt.plot(thresholds, count_rows_list, label='Number of trips')\n",
    "plt.plot(thresholds, unique_ids_list, label='Number of devices')\n",
    "plt.xlabel('Minimum probability for inclusion')\n",
    "plt.ylabel('Count')\n",
    "# plt.title('Number of trips and unique ids by max_prob threshold')\n",
    "\n",
    "# Format the horizontal axis as percentages\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: '{:.0%}'.format(x)))\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
